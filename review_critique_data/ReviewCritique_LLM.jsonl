{"decision": "rejected", "title": "UNDERSTANDING HTML WITH LARGE LANGUAGE MODELS", "body_text": "\n\nU NDERSTANDING HTML WITH L ARGE L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Large language models (LLM) have shown exceptional performance on a vari- ety of natural language tasks. Yet, their capabilities for HTML understanding \u2013 i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval \u2013 have not been fully explored. We contribute HTML understanding models (\ufb01ne-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classi\ufb01ca- tion of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has devel- oped dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer re- markably well to HTML understanding tasks. For instance, \ufb01ne-tuned LLMs are 12% more accurate at semantic classi\ufb01cation compared to models trained exclu- sively on the task dataset. Moreover, when \ufb01ne-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl. We show evidence that T5-based models due to the bidirectional encoder-decoder architecture are the best choice and that for practitioners larger models are not necessarily better.\n\n1 I NTRODUCTION\n\n Web crawling (Olston et al., 2010), form-\ufb01lling (Diaz et al., 2013; Gur et al., 2021), or information retrieving web agents (Nogueira & Cho, 2016) are important for both automating and assisting users in web-based tasks. These and similar applications rely on models that can search for speci\ufb01c content or controls on a web page as well as navigate a website autonomously. Since a web page in its raw form is represented as an HTML-based text sequence, the success of models for web-based tasks relies on their ability to understand HTML semantics, structure and embedded interactions. The predominant approach to web automation and HTML understanding is to train specialized mod- els, i.e., gathering application-speci\ufb01c datasets and designing neural network (NN) architectures to leverage inductive biases of the HTML\u2019s structure; see, e.g., Liu et al. (2018); Toyama et al. (2021); Gur et al. (2021); Humphreys et al. (2022). However, both dataset collection and neural architecture design are expensive, time-consuming, and require highly-specialized, domain-speci\ufb01c knowledge. Meanwhile, in the natural language processing (NLP) literature, large language models (LLMs) have emerged as a solution to the dif\ufb01culties of dataset collection and specialized NN design (Kaplan et al., 2020; Bommasani et al., 2021). A popular paradigm in NLP is to take an off-the-shelf LLM \u2013 pretrained on a large text corpus via an unsupervised and task-agnostic learning objective \u2013 and either \ufb01ne-tune or prompt the LLM on a small task-speci\ufb01c dataset. This paradigm has shown exceptional performance on a variety of NLP tasks (Xue et al., 2020; Brown et al., 2020; Austin et al., 2021). Whether LLMs can be applied to HTML understanding \u2013 especially given the much larger context and sequence lengths \u2013 remains an under-explored question. In this paper, we investigate whether LLMs can be applied to HTML understanding to produce better-performing, more sample-ef\ufb01cient HTML understanding models and without the need for custom NN architecture design. To that end, we present a suite of three benchmarking tasks for HTML understanding that capture the essence of these applications and require understanding both 1 structure and content. First, we devise Semantic Classi\ufb01cation as a task that requires a model to classify a given HTML element into one of the set of categories, such as address, email, password etc., with application to automated form-\ufb01lling. Second, we present Description Generation , a label-extraction task where a model is given an HTML snippet and is asked to produce a natural language description. For instance for an email \ufb01eld, the description might be \u201cPlease enter your email address.\u201d Note that in the majority of web pages, this connection between input elements and description content is only implicit in the raw HTML code and inferring such links is a prerequisite for higher-level navigation objectives. The third task is Autonomous Web Navigation (Shi et al., 2017). A model is presented with an HTML page paired with a natural language command and must apply appropriate actions on a sequence of HTML pages to satisfy the command. See Figure 1a for a simpli\ufb01ed example of these tasks. With these benchmark tasks in hand, we evaluate the transfer capabilities of a variety of pretrained LLMs (Table 1), varying in architecture (encoder-only, encoder-decoder, or decoder-only), model size (from 24.6M to 62B parameters), and training data corpora (both including and excluding pre- training NLP and HTML corpus). While prior work universally pre-parses the HTML as input to the model (Gur et al., 2021; Liu et al., 2018; Nakano et al., 2021), ours \u2013 to the best of our knowledge \u2013 is the \ufb01rst work that uses raw, unprocessed HTML. Our results show that LLMs demonstrate a remark- able level of HTML understanding across all tasks, with up to 192 \u00d7 more sample-ef\ufb01ciency than models trained from scratch, and achieving a new SoTA for supervised learning on the MiniWoB benchmark suite (Shi et al., 2017). The encoder-decoder architectures with bi-directional attention show the best performance across the board even when the pre-training does not include HTML. In addition, we show that the performance scales sub-linearly with the model size. In summary, our paper provides a new perspective on machine learning for HTML understanding and web automation, showing that pretrained LLMs can achieve signi\ufb01cant performance on such tasks, eschewing the need for specialized architectures and training protocols. To encourage further research in this direction, we plan to open-source our dataset for description generation. The further details are available at https://sites.google.com/view/llm4html/home\n\n2 R ELATED W ORK\n\n HTML Understanding Autonomous web navigation has been a popular application for neural net- work models, and a variety of works propose simulated websites for training web-based agents, with application to task ful\ufb01llment (Yao et al., 2022; Gur et al., 2021; Burns et al., 2022; Mazumder & 2 Riva, 2020; Shi et al., 2017; Liu et al., 2018) as well as information retrieval or question-answering (Adolphs et al., 2021; Nogueira & Cho, 2016). Simulated websites provide an easy way to evaluate models online, and for this reason we use the existing MiniWoB benchmark (Shi et al., 2017) for our web navigation setting. However, it is still important to have a mechanism for evaluating models on a wide variety of real-world websites. This was the key motivation for generating our own dataset for the description generation task, which distilled and auto-labeled from CommonCrawl and is a key contribution of our paper. Alongside these benchmarks, many works have developed models for web navigation and related subtasks (Pasupat et al., 2018; Bommasani et al., 2021; He et al., 2021; Gur et al., 2021; Humphreys et al., 2022; Liu et al., 2018; Jia et al., 2019). These works often rely on specialized neural network architectures that leverage inductive biases of HTML structure, or on preprocessing of HTML to make it easier to input to a model. In contrast, our work takes a minimalist approach, providing HTML in raw text form and using widely-adopted transformer networks. LLMs and HTML Works that explore the intersection of LLMs and HTML generally fall into two categories. First category uses LLMs to assist web navigation (Nakano et al., 2021; Yao et al., 2022), and typically relies on a custom preprocessing to map the context and structure of a web page to natural language, thus severely restricting what HTML pages the model can parse. The second category pretrains LLMs on a large corpora of HTML text (Aghajanyan et al., 2021). However, these works typically restrict the model evaluation to standard NLP tasks, e.g., summarization and question/answering as opposed to tasks more relevant to HTML understanding and web automation. Our work can be thought of as the reverse: We keep the pretraining of LLMs unchanged and focus on the mechanisms for transferring the pretrained LLMs on HTML-relevant tasks.\n\n3 B RIEF B ACKGROUND ON HTML AS S EMI - STRUCTURED T EXT D ATA\n\n HTML is a markup language, used to organize web page structure and content . Consider the example HTML page in Figure 1a. This web page includes two adjacent input elements, one for e-mail and another for password, with their corresponding label s on a separate branch of the page. These input s and label s are one of many possible elements that serve as HTML building blocks. Each element has a set of attributes \u2013 key and value pair \u2013 that describe the element\u2019s content, such as style and human-readable text. When rendered in a browser, these attributes will be responsible for how the element is shown and where it is positioned. In the example in Figure 1a, the \ufb01rst input has three attributes, tag=\"input\" , type=\"email\" , and id=\"uName\" , that identify the element as an email input with an identi\ufb01er (\u201cuName\u201d) that can be accessed programmatically.\n\n4 C ANONICAL T ASKS FOR HTML U NDERSTANDING\n\n Autonomous Web Navigation task evaluates how well a model navigates multi-page websites, as a sequential decision-making problem Shi et al. (2017); Liu et al. (2018). At the beginning of an episode, the agent is given a natural language instruction, e.g. Enter the username \u201clyda\u201d and the password \u201cN22t\u201d into the text \ufb01elds and press login . The agent applies actions to a sequence of HTML pages. The action have the form: function(selector, text) . function is one of click or type , selector is an integer pointer that uniquely identi\ufb01es an element, and text is a text to input if the type functionality is activated. An episode terminates when either the page reaches a terminal state (e.g., the \u2018sign in\u2019 button is clicked) or the maximum number of steps is reached. Semantic Classi\ufb01cation . Many HTML understanding applications require a model that can classify HTML elements into standardized categories. In automated form-\ufb01lling (Diaz et al., 2013; Gur et al., 2021), it is useful to identify a \u2018submit button\u2019 across many websites (e.g., shopping, \ufb02ight booking, utility application) with various button representations (e.g., position, color, or text). Thus, we formulate Semantic Classi\ufb01cation as classifying elements into role categories. Take the example HTML in Figure 1a which includes two input elements and a submit button . Let\u2019s pick the \ufb01rst input as an element of interest to be classi\ufb01ed by the system, also called a salient element . The system should classify this element as username , since it appears on a login page and it has a label with Email Address which is typically associated with the username in form-\ufb01lling applications. To solve this, the system can aggregate information from multiple sources in the page \u2013 the label that says Enter Email Address , the input attributes ( type=\u201cemail\u201d and id=\u201cuName\u201d ), or even the 3 Dataset Size Input Architecture Output Task Output MiniWoB . 12K Page Enc Dec Text Dictionary Dec ) 38K Snippet All Text Category Snippet ordering of other elements in the page such as \u2018password\u2019 and \u2018sign in\u2019. To properly solve this task, a model must build an understanding of the structure and content of the page and use this understanding to make an accurate prediction. Description Generation . Motivated by applications in accessibility-minded web browser con- trol (Jorgensen & Binsted, 2005), we formulate description generation as an extractive problem where the goal is to locate the textual description of an element in the HTML and generate it as output. For instance, the description of the salient element in Figure 1a is Enter Email Address ; when rendered, this label will appear above the \u2018email\u2019 input \ufb01eld. HTML provides a large amount of \ufb02exibility, and so in general a descriptive text that appears alongside a speci\ufb01c element when rendered can be very far from that element when looking at the HTML plaintext. Thus, this task evaluates a model\u2019s ability to understand the structure of HTML as it would appear to a user, despite not having access to the rendered web page directly.\n\n5 D ATASETS\n\n Autonomous Web Navigation . We use the 12K demonstrations included in the publicly available MiniWoB benchmark (Shi et al., 2017), which encompass 62 website applications ranging from email forwarding to social media interactions. Every element in a MiniWoB demonstration is ac- companied by a reference number unique within its respective pages. This number can be used as an element selector, making the action space uni\ufb01ed across all tasks and time steps. Semantic Classi\ufb01cation . We use a dataset of 28K labelled examples, containing 66 different cat- egories, of the form (HTML, element, category) , previously used in the context of environment generation (Gur et al., 2021). The dataset consists of URLs from the real shopping websites. Description Generation . We derive a dataset from CommonCrawl. 1 CommonCrawl does not in- clude renderings or annotations that would reveal what text in the HTML is associated with the elements. Instead, we infer descriptions of various elements by exploiting a special attribute in the HTML schema known as for .As an example in Figure 1a, the \ufb01rst label in the HTML has a for attribute with value uName , which is the id of the element described by label ; in this case, it is the \ufb01rst input in the page. While this annotation procedure is typically used for accessibility purposes, we utilize it to create a large-scale dataset to study description generation. A small sample is available in the supplemental material, while the entire dataset will be available upon publication. We randomly collected 100 WARC (from April 2019) \ufb01les from the CommonCrawl project and extracted all HTML label s that have a for attribute. Removing non-Unicode and alphanumeric text in HTML label s, results in 400K example datset. Balancing the labels downsamples the dataset to 85 K samples. Each example is represented as (HTML, element, description) , where HTML is the HTML plaintext of the page, element is the element whose id attribute matches that appearing in the label \u2019s for attribute, and description is the text inside the label element (see example in Figure 1a). More details of the dataset can be found in Appendix A.1.\n\n6 P RE -P ROCESSING\n\n In treating HTML as token sequences, we minimize any HTML tree pre-processing prior to model input. We thus provide HTML as raw text (i.e., sequences of text tokens) and only apply a snippet extraction pre-processing for pages which are too large to \ufb01t into the typical LLMs context windows. 1 http://commoncrawl.org 4 Snippet Extraction. Real HTML pages can grow extremely large, reaching thousands of elements, far beyond the context window of the largest LLM that we studied (1920 tokens in PaLM (Chowdh- ery et al., 2022)). LLMs typically truncate such long sequences, which can be detrimental to HTML understanding as HTMLs are not linearly structured. We take an element-centric approach and ex- tract HTML snippets (a small portion of HTML code) surrounding a salient element (Figure 5). A simple heuristic, which controls the tree\u2019s width and depth, guides the process \u2013 start with a salient element and traverse its ancestors in the HTML tree until a stopping condition is satis\ufb01ed. As we traverse up, we estimate the height of the tree and the increased number of descendants of the new root. We stop when either violates a pre-de\ufb01ned limit and take the current sub-tree as the snippet. We mark the salient element using a special attribute, called target , to distinguish it from other ele- ments .We perform the snippet extraction for the semantic classi\ufb01cation and description generation datasets, and keep the full HTML pages in MiniWoB because these pages are typically much smaller than real-world HTML. HTML un-Parsing. We provide the models with the unparsed plaintext HTML in the form of a sequence of tokens. This canonical representation does not require speci\ufb01c model architectures such as hierarchical networks (Liu et al., 2018; Gur et al., 2021) and can be fed into any LLM. We transform all datasets by converting every HTML page or snippet into a sequence. For MiniWoB, we additionally concatenate (action history, instruction, HTML) tuples into a single sequence.\n\n7 M ODEL T RAINING\n\n We study a variety of Transformer-based LLMs with different sizes and architectures for HTML understanding tasks (Table 1). In the rest of the text, we pre\ufb01x models \ufb01ne-tuned for Autonomous Web Navigation , Description Generation , and Semantic Classi\ufb01cation with WebN-, WebD-, WebC- respectively. For instance, WebD\u2013T5-3B is the three billion parameter T5 model (Raffel et al., 2020) \ufb01ne-tuned for the Description Generation task. The rest of this section introduces training details. Encoder-Decoder and Decoder-only Models. We train encoder-decoder, T5 (Raffel et al., 2020) and decoder-only models, LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022), with text input and text output (Figure 1b). Inputs are raw HTML pages or snippet texts; simi- larly, outputs are categories, natural language descriptions, or actions represented as text. While descriptions are already represented in natural language, we use names of categories for Semantic Classi\ufb01cation similar to previous classi\ufb01cation problems in NLP (Raffel et al., 2020). Actions are converted into text by \ufb01rst converting them into key and value pairs and then concatenating the pairs. Many websites in MiniWoB require multiple interaction, such as click-button-sequence or click- checkboxes , where each interaction might cause a subtle change in the website state. For instance, after clicking on a checkbox in the click-checkboxes website, its value \ufb02ips from positive to negative or the other way around, which is not always re\ufb02ected in LLMs\u2019 predictions and leads to action repetitions. We solve this issue by augmenting tuples in the dataset with a sequence of past actions, (action history, instruction, HTML, action) , and allowing LLMs to learn from past experience. Encoder-only Models. We train encoder-only models, BERT style (Devlin et al., 2018), with text input and categorical output. We keep semantic categories as discrete one-hot classes. To train encoder-only models, we add a new classi\ufb01cation layer after the \ufb01nal encoder layer to produce a distribution over semantic categories. In addition to the typical BERT models, we study Mobile- BERT (Sun et al., 2020), distilled from BERT-large with inverted bottlenecks, and Albert-XL (Lan et al., 2020), with parameter sharing and embedding split.\n\n8 R ESULTS\n\n This Section evaluates the performance of the HTML pre-trained encoder-decoder architectures (Web-T5 class of LLMs) on the three HTML Understanding tasks. (Web-T5 class of LLMs). We compare the models\u2019 performance with the existing baselines where possible (N), and against other LLM architectures and training regimes (all tasks). Subsections 8.1, 8.2, and 8.3 evaluate task- speci\ufb01c performance, while the Section 8.4 assesses the performance across all the tasks. Metrics: Autonomous Web Navigation Success Rate evaluates the model performance, averaged over 100 episodes per task. Episodes end with either 1.0 for success and 0.0 for failure. Accuracy 5 measures exact match between prediction and ground truth in semantic classi\ufb01cation and description generation tasks. In the description generation task, we additionally use typical text evaluation metrics, BLEU and ROUGE-1 , to understand if a model fails to locate or to generate the description.\n\n8.1 A UTONOMOUS W EB N AVIGATION R ESULTS\n\n For Autonomous Web Navigation we \ufb01ne-tune two WebN- encoder-decoder architectures (WebN- T5-large and WebN-T5-3B) on 12k demonstrations from human-annotated real websites. We eval- uate the models on MiniWob (Liu et al., 2018) benchmark, and compare with specialized archi- tectures trained using supervised learning (SL) on 2.4 million human expert demonstrations CC-Net (SL) (Humphreys et al., 2022), and two RL models bootstrapped with SL, CC-Net (SL) (CC-Net (SL & RL) (Humphreys et al., 2022), and WGE (SL & RL) (Liu et al., 2018)). We provide human expert data as well. Additionally, we compare with the decoder-only architecture (WebN-Lambda-1B) and perform an ablation study on the impact of including the action history in the input. Comparison to SoTA. Since previous works report success on only a subset of websites in Mini- WoB, we evaluate on 48 out of 62 websites that are common across all models (Figure 2a). Table 8 in the Appendix reports all results. Compared to CC-Net (SL) which is trained on all 2.4M demon- strations, WebN-T5-3B improves the success 16% while signi\ufb01cantly sample ef\ufb01cient. It yields 192x training data reduction, being trained only on the 12K publicly-available demonstrations. All LLMs outperform SL models. Speci\ufb01cally, WebN-T5-3B signi\ufb01cantly improves on websites requir- ing multiple-action sequences such as click checkboxes or websites requiring entering text such as login user (Table 8). Action history ablation. Across all LLMs we consistently observe a decrease in success, on av- erage 6.4%, when past actions are excluded from the inputs (Figure 2a). Action history helps with websites that require entering multiple texts, as well as understanding minor changes that could be dif\ufb01cult to detect (e.g. click checkboxes and multi layout ). multi layout requires entering 3 different texts in the website where the layout is randomized at each episode, yet, surprisingly, even the (rel- atively smaller) WebN-T5-large model without action history outperforms the CC-Net (SL) model; illustrating that incorporating action history is not the only contributing factor for the better success.\n\n8.2 S EMANTIC C LASSIFICATION T ASK R ESULTS\n\n To evaluate the Semantic Classi\ufb01cation we compare the T5 encoder-decoder architecture\u2019s three size variants (WebC-T5-base, WebC-T5-large, and WebC-T5-3B) \ufb01ne-tuned on 38K real, human- labeled websites. We compare with a \ufb01ne-tuned encoder only architectures (WebC-*BERT*), three \ufb01ne-tuned decoder-only architectures (WebC-LaMDA and PaLM), and both encoder-decoder and decoder-only models trained on 38K human labeled websites from scratch (Table-2) All WebC- LLMs perform well, having accuracy over 75%. Accuracy per category. The \ufb01ne-tuned encoder-decoder model performs strongly on majority of the categories (Figure 3), even on those with very few samples. For instance, the model is 100% 6 7 accurate on password new which has only 56 training examples, because the class is unambiguous. On the other hand, unsurprisingly, the performance drops when the category is ambiguous, such as in the email category which is frequently mistaken as username . Snippet generation ablation. Two hyper-parameters govern snippet generation: percentage of new descendants and height of the new root. While small variations of both parameters don\u2019t change the performance, increasing both degrades the performance signi\ufb01cantly (Table 4a). With new descen- dants up to 500% and height up to 7, the performance drops by more than 15%. Note that snippet generation returns the full-page HTML when both parameters increase inde\ufb01nitely. Data size impact. When varying the \ufb01ne-tuning training data sizes (1, 5, 10, 20, or 50 samples per class) in Figure 4b, WebC-T5-3B slightly outperforms WebC-PaLM-8B which is an order of magnitude larger. Compared to T5-3B that is trained on all available HTML data without pretraining, WebC-T5-3B achieves better performance while using only 3.4% of labeled data (1000 samples); showing outstanding sample ef\ufb01ciency.\n\n8.3 D ESCRIPTION G ENERATION T ASK R ESULTS\n\n For Description Generation we split the CommonCrawl dataset based on URL top-level domains to test LLMs\u2019 capabilities to generalize to unseen HTML. We \ufb01ne-tune encoder-decoder architectures (WebD\u2013T5*), decoder-only models (Table 3). We also present a strong intuitive baseline, \ufb01nding the description closest to the salient element in the HTML text (Closest Description). Accuracy and Similarity Performance All pre-trained models achieve scores across all metrics ( 90% on the accuracy) and higher BLEU and ROUGE-1 scores (Table 3). The difference indicates that the models are capable of locating the descriptions, but not generating the exact output. It is interesting that WebD\u2013T5-large and WebD\u2013LaMDA-1B with similar accuracy and ROUGE-1 scores, WebD\u2013T5-large has substantially higher (3%) BLEU score.\n\n8.4 HTML U NDERSTANDING LLM S P ERFORMANCE A NALYSIS A CROSS T ASKS 8.4.1 P RE - TRAINING EFFECT : P RE - TRAINING ON LARGE TEXT CORPORA MATTERS .\n\n Fine-tuned pretrained LLMs outperform LLMs trained on HTML-only data, improving the perfor- mance by more than 34.1% on the Autonomous Web Navigation (Table 2b), and 10% to 12.7% on the Semantic Classi\ufb01cation task (Table 2). Since Autonomous Web Navigation is the most dif\ufb01cult task, the improved performance is an en- couraging evidence of the value of LLMs in HTML understanding tasks. Speci\ufb01cally, we observe that LLMs without pretraining were comparable to \ufb01ne-tuned pretrained models only on websites that require simple text matching. However, in websites such as click checkboxes , text matching is harder and pretraining helps. We also found that without pretraining, model outputs were frequently in an incorrect format such as invalid dictionaries or invalid refs with non-integer values. This sug- gests the large corpora used for pretraining helps models to learn the general HTML structure.\n\n8.4.2 A RCHITECTURE E FFECT : T5 BASED MODELS PERFORM BEST ACROSS ALL TASKS\n\n Encoder-decoder T5 based models perform better across all three tasks. On the Autonomous Web Navigation task, encoder-decoder (WebN-T5) architectures are better or comparable to WebN- LaMDA-1B (Figure 2a). On the Semantic Classi\ufb01cation , the smallest encoder-decoder model (WebC-T5-base) performs comparably to much larger decoder-only models (WebC-LaMDA-1B or 8 WebC-PaLM-8B) and the largest encoder-only model (WebC-BERT-large) which has 85M more pa- rameters (Table 2). We also observe that decoder-only PaLM-8B performs worse than much-smaller encoder-decoder T5-large when trained only on HTML data. Finally, on the Description Generation encoder-decoder architecture has higher BLEU score which approximately measures precision. One particular explanation is that encoder-decoder T5 models utilize an encoder with a bidirectional attention mechanism, not present in the LaMDA and PaLM decoders. The bidirectional attention mechanism can process HTML pages from both ends, potentially overcoming the loss of information when tree-structured HTML pages are converted into a \ufb01xed linear text sequences. This hypothesis would explain the strong performance of T5 models on all three tasks.\n\n8\n\n Across the tasks it appears that the architecture plays an important role in the model performance. Model size and performance are also positively correlated, although they reach diminishing returns. For instance, the model performance is O (log log n ) if n is model size on the Semantic Classi\ufb01ca- tion (Figure 4b in Appendix). On the Autonomous Web Navigation task performance grows slowly with the model size (Table 8, while on the Description Generation it plateaus 3.\n\n8.5 D ISCUSSION\n\n Bi-directional attention vs training corpora: Pretraining on large corpora matters, yielding \u2264 4.5x performance improvements. Larger models tend to be better and we credit the bi-directional atten- tion for T5\u2019s best overall performance across the tasks. PaLM And LaMDA included HTML and other code in the pre-training corpora, but BERT and T5 architectures did not. This strengthens the hypothesis behind the role of the bi-directional attention, and opens up a possibility to improving the performance on T5 architectures by training them on the corpora with HTML. Practical impact on labeling: When available, the pretrained LLMs need very little new expert data (200x and 30x reduction on the web navigation and classi\ufb01cation tasks). This has a big potential impact on practical applications, reducing the data collection time and cost by orders of magnitude. Bigger is not always better: When choosing the model size, the expected performance gains (sub- linear at best and asymptotic at worst) should be considered alongside the model\u2019s training and inference time and cost. For instance, on the classi\ufb01cation task, the largest model WebC-PaLM-62B takes several days to \ufb01ne-tune, and infers at 30 Hz, while WebC-T5-large \ufb01ne-tunes in several hours and infers at 700 Hz \u2013 an order of magnitude more expensive for a single percent uplift in accuracy. BERT models on the other hand train in minutes. If the application does not require high precision, they might be a good choice. Context window is a bottleneck: The major bottleneck for the HTML understanding tasks seems to be the context window length that the current LLMs support, even with models that accept 1000+ tokens. It remains prohibitive to evaluate web navigation tasks on the real websites that are or- ders of magnitude larger than pages in MiniWob. Similarly, we observed that increasing the snip- pet size leads to major performance degradation. This makes HTML Understanding an interesting benchmark for future LLM development. For instance, new methods may need to be developed to compress the state representation of web content for use in LLM context windows.\n\n9 C ONCLUSION\n\n We presented canonical tasks and \ufb01ne-tuned LLMs for HTML understanding. The comprehensive evaluations and analyses over a range of architectures, dataset sizes, and baselines yields practical \ufb01ndings and highlights current limitations of these models. We \ufb01nd that a) pretraining is critical for the performance and can reduce labeled data requirements / improve sample ef\ufb01ciency up to 200x; b) model architecture is the second most important factor and T5 models with bi-directional attention and encoder-decoder architecture performs the best across the board; c) given a choice, model size should be evaluated in the context of the models training and inference performance, as the model size sub-linearly predicts its performance. Finally, the proposed HTML understanding tasks highlight the relatively short context window that limits current LLMs, suggesting possibilities for future research that incorporate or eliminate this constraint. 9\n\nR EFERENCES\n\n 10\n\nA A PPENDIX A.1 D ATASET D ETAIL\n\n Examining the description distribution we found the original 400 K dataset to be very skewed; only 20 descriptions (such as Email and Password ) were covering 50% of the dataset. We sub-sampled the 11 dataset so that each unique description has at most 10 data points. We also found that for attributes are almost always de\ufb01ned for HTML label s. This could cause a model to over\ufb01t and just \ufb01nd the label element in the HTML and ignore everything else. To avoid this sort of \u2018cheating\u2019 we replace the tags of HTML label s by randomly sampling from { div, span, a, label } . These tags are also frequently used to inject text in HTML but they are very rarely used with for attributes. Finally we removed examples where there are only a single text in the HTML since models can trivially generate descriptions by \ufb01nding the only text in the HTML, which biases model weights and evaluation metrics. After this \ufb01nal step, we have a total of 85 K labeled examples.\n\nA.1.1 S NIPPET G ENERATION\n\n In Figure 5, we give a high-level overview of our snippet generation procedure.\n\nA.2 A DDITIONAL R ESULTS A.2.1 S EMANTIC C LASSIFICATION\n\n Error Analysis. We manually examined 50 errors of T5-3B model over the development set (Ta- ble 4) and assigned them into one of the 9 error types that we devised. We found that 32% of the errors are due to lack of information in the HTML snippets, which is mainly the result of lost in- formation during snippet extraction process. Annotation errors or email/username ambiguity make up 30% of the errors. These can\u2019t be improved without revising the annotated data or adding extra information to resolve the ambiguity. We also found that the model sometimes picks a more general category, or a nearby text misleads the model; the latter usually happens when the HTML snippet is long where majority of the elements are noise.\n\nError Type\n\n 30 \u201dunknown of 12 8 10 8 8 4 2 18 Few-Shot Prompting In Table 5, we present few-shot prompting performance of a 540B PaLM model. We probe the model using a prompt template <html> Role: <category> with 1 ex- ample per category and generate categories using greedy-decoding. In our preliminary experiments, we found that few-shot prompting achieves only 45.6 accuracy, much lower than a model \ufb01ne-tuned 12 on the same data (Figure 6). We found two common problems \u2013 the model is not able to canonicalize predictions into categories and many of the examples are dropped due to context length. We developed post-processing methods to al- leviate the canonicalization problem and pre- processing methods to reduce lengths of ex- amples. Adding a dictionary-based mapping on predictions \u2013 a manually curated paraphrase dictionary \u2013 improves the performance to 52.1. We also tried rewriting predictions by chang- ing the order of tokens around \u201d \u201d such as name \ufb01rst to \ufb01rst name which further improved the performance to 57.9. Finally, we cleaned examples in the prompt by removing certain el- ements such as \u201dsvg\u201d, \u201dpath\u201d, \u201dimg\u201d , and \u201diframe\u201d and also removing class attribute from every element; this pre-processing step gives 64.2.\n\nA.3 S AMPLE E PISODES FROM M INI W O B A.4 D ETAILED M INI W O B R ESULTS\n\n Realistic Alaska Airlines Websites. While evaluating on simulated websites is useful to compare performance of models in a controllable setup, it is important to understand the sim-to-real gap for web navigation systems to be useful in practice. We split publicly available demonstrations into training and development set. We have 291 demonstrations for evaluation. We report numbers using of\ufb02ine demonstration replay experiment, mainly, percentage of actions that overlap with ground truth (of\ufb02ine accuracy) and percentage of episodes where all actions are same as ground truth (of\ufb02ine success). We achieve 10.3 of\ufb02ine task success and 73.7 of\ufb02ine accuracy. The accuracy indicates that the model takes the correct action 73.7% of the time but it \ufb01nishes an episode correctly only 10.3% of the time.\n\nA.5 R ESOURCE R EQUIREMENTS\n\n 13 { { : ref: 12 } { : ref: 14 } 14 15 a n/a 0 0 n/a n/a n/a n/a n/a n/a n/a n/a 0 n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a / 1 n/a n/a 1 n/a n/a 1 0 n/a n/a n/a n/a n/a n/a n/a n/a 1 1 1 1 1 n/a 1 1 n/a 1 n/a n/a 1 n/ /a / n n/ 0 n/a 0 n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 1 0 0. n/a n/a n/a n/a n/a 0 /a n/a /a / / / / n/a n/a n/a n/a . 0. n/a n/a n/a n/a n/a n/a 1 1 0 n/a n/a n/a n/a n/a n/a n/a n/ 0 /a n/a n/a n/a n/a n/a n/a n/ n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 1 0 3 n/a n/a n/a n/a n/a n/a n/a n/a 1 n/a 0.77 n/a n/a 0 n/a 0.77 0 n/a n/a 1 0 n/a n/a n/a n/a n/a n/a n/a n/ n a n/a 1 0 n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 1 0.11 n/a n/a n/a n/a n/a n/a n/a n/a /a / 1 1 a a a a /a / 1 1 0 1 n/a 1 1 n/a 1 n/a n/a 0.9 n/a 0.52 0.9 n/ n/a n/a n/a n/a 0.31 / /a 16 17", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper explores using large language models (LLMs) for HTML understanding tasks, without relying on specialized architectures or HTML-specific training.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors define three benchmark tasks to evaluate HTML understanding capabilities: 1) Semantic Classification of HTML elements into categories, 2) Description Generation to produce natural language descriptions of HTML snippets, and 3) Autonomous Web Navigation of HTML pages given instructions.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They fine-tune various pre-trained LLMs, including encoder-only, encoder-decoder, and decoder-only architectures, on these tasks.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Results show that LLMs, especially encoder-decoder models like T5, transfer remarkably well to HTML understanding.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Fine-tuned LLMs outperform models trained from scratch, achieving up to 192x better sample efficiency on the web navigation task.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors also release a large HTML dataset distilled from CommonCrawl for the description generation task.", "topic_class_1": "Dataset Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Applying LLMs to HTML understanding is a novel and important problem with practical applications in web automation and retrieval.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This is one of the first works to systematically study this.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The three proposed benchmark tasks capture key aspects of HTML understanding.", "topic_class_1": "Experiment Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The tasks are well-motivated and the datasets used are suitable.", "topic_class_1": "Experiment Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Experimenting with a wide range of state-of-the-art LLM architectures provides useful insights into which models work best for these tasks.", "topic_class_1": "Experiment Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Demonstrating strong transfer from LLMs to HTML understanding tasks, without HTML-specific architectures or training, is an important finding that can spur more work in this direction.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Open-sourcing a large automatically labeled HTML dataset from CommonCrawl is a valuable contribution to the community.", "topic_class_1": "Dataset Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- More analysis could have been done to understand why encoder-decoder models like T5 perform best.", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "This is not considered as a good review. The reviewers didn't provide details and evidence about how to add new analysis.", "error_type": "Vague Critique"}, {"segment_text": "The bidirectional attention hypothesis is stated but not investigated in depth.", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The snippet extraction procedure for handling large HTML pages has some ad-hoc aspects.", "topic_class_1": "Experiment Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A more principled approach to processing long HTML sequences would be good to explore.", "topic_class_1": "Future Work", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- More discussion of the potential limitations of repurposing LLMs for HTML understanding and ideas for future improvements would have strengthened the paper.", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "No", "explanation": "However, there are a few grammaritcal mistakes in the paper. For example, in the caption of Figure 2, \"improves the task success 16%\" should be \"improves by.\" Also, in Section 8.2, \"We compare with a fine-tuned encode\" should be \"compare it with xxx.\"", "error_type": "Writing"}, {"segment_text": "The task formulations, model architectures, datasets, and experimental methodology are all described in sufficient detail.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The technical quality of the work is high.", "topic_class_1": "Comprehensive Experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are comprehensive and the results are significant.", "topic_class_1": "Comprehensive Experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Applying LLMs to HTML understanding benchmarks is novel, and the strong results advance the state-of-the-art, especially on the web navigation task.", "topic_class_1": "Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The datasets and model details are provided, facilitating reproducibility.", "topic_class_1": "Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open-sourcing the CommonCrawl dataset is a big plus.", "topic_class_1": "Dataset Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a valuable contribution by demonstrating the effectiveness of LLMs for HTML understanding tasks.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The strong transfer results, without requiring specialized architectures, can serve as a new baseline and open up avenues for future work.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While a few aspects could be improved, such as more analysis of architectures and snippet extraction, overall this is a high-quality paper with novel findings that advance an important research direction.", "topic_class_1": "Strength & Weakness Tradeoff", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The new dataset is also a useful resource for the community.", "topic_class_1": "Dataset Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper explores the application of Large Language Models (LLMs) for understanding HTML, a crucial aspect for tasks such as web automation, crawling, and browser-assisted retrieval.", "topic_class_1": "Topic Summary", "topic_class_2": "Summay of paper topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite the significant capabilities of LLMs in various natural language tasks, their potential in parsing raw HTML has been relatively unexplored.", "topic_class_1": "Research Gap", "topic_class_2": "Summay of paper topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This study introduces HTML understanding models, which are fine-tuned LLMs, and examines their performance across three tasks: Semantic Classification of HTML elements, Description Generation from HTML inputs, and Autonomous Web Navigation through HTML pages.", "topic_class_1": "Research Summary", "topic_class_2": "Summay of paper contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The results demonstrate that fine-tuned LLMs, particularly T5-based models with their bidirectional encoder-decoder architecture, significantly outperform models trained solely on task-specific datasets.", "topic_class_1": "Experiment Summary", "topic_class_2": "Summay of paper results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Moreover, the study contributes a large-scale HTML dataset distilled from CommonCrawl to encourage further research.", "topic_class_1": "Experiment Summary", "topic_class_2": "Summay of paper contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The findings highlight that LLMs pre-trained on natural language corpora can effectively transfer to HTML understanding tasks, achieving remarkable accuracy and sample efficiency without the need for specialized neural network architectures.", "topic_class_1": "Conclusion Summary", "topic_class_2": "Summay of paper finding", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper addresses an under-explored yet critical area of leveraging LLMs for HTML understanding, relevant for web-based automation and information retrieval.", "topic_class_1": "Novelty", "topic_class_2": "Strength on motivation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Demonstrates significant improvements in accuracy and sample efficiency over existing models by fine-tuning pre-trained LLMs on HTML understanding tasks.", "topic_class_1": "Experiment Strength", "topic_class_2": "Strength on experiment results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The comprehensive experiments across different models, tasks, and a novel dataset contribute valuable insights to the field.", "topic_class_1": "Experiment Strength", "topic_class_2": "Strength on experiment results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open-sourcing the large-scale HTML dataset distilled from CommonCrawl is a significant contribution that will likely spur further research.", "topic_class_1": "Dataset Strength", "topic_class_2": "Strength on potential impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper could benefit from a more detailed discussion on the limitations and challenges faced during the snippet extraction process and how they impact model performance.", "topic_class_1": "Analysis Weakness", "topic_class_2": "Concern onver lack of limitation discussion", "reliability": "No", "explanation": "The authors already discusses this in Section 8.2 and Table 4", "error_type": "Contradiction"}, {"segment_text": "A comparison with state-of-the-art models in related fields, such as document understanding or web page segmentation, might have provided a broader context.", "topic_class_1": "Baseline Weakness", "topic_class_2": "Concern onver missing comparisons with SOTA", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The ethical considerations of automating web navigation tasks, especially in terms of privacy and security, were not discussed.", "topic_class_1": "Privacy Weakness", "topic_class_2": "Concern onver lack of ethical discussion", "reliability": "No", "explanation": "An out of scope suggestion. This is not the focus of this paper.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written, with a clear structure that guides the reader through the objectives, methodology, results, and conclusions.", "topic_class_1": "Clarity", "topic_class_2": "Strength on clarity", "reliability": "No", "explanation": "However, there are a few grammaritcal mistakes in the paper. For example, in the caption of Figure 2, \"improves the task success 16%\" should be \"improves by.\" Also, in Section 8.2, \"We compare with a fine-tuned encode\" should be \"compare it with xxx.\"", "error_type": "Writing"}, {"segment_text": "Quality: The research is of high quality, with rigorous experimentation and a significant dataset contribution.", "topic_class_1": "Quality Strength", "topic_class_2": "Strength on quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The approach of fine-tuning pre-trained LLMs for HTML understanding tasks is novel, especially the application across diverse tasks like semantic classification, description generation, and web navigation.", "topic_class_1": "Novelty", "topic_class_2": "Strength on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: With the detailed methodology, open-sourced dataset, and clear descriptions of the fine-tuning process, the study appears highly reproducible.", "topic_class_1": "Reproducibility", "topic_class_2": "Strength on reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a novel approach to HTML understanding using fine-tuned Large Language Models, demonstrating impressive performance improvements across several tasks.", "topic_class_1": "Topic Summary", "topic_class_2": "Strength on reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is significant for its potential applications in web automation and information retrieval.", "topic_class_1": "Work Impact", "topic_class_2": "Summary of contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The open-source dataset further contributes to the field's advancement.", "topic_class_1": "Dataset Strength", "topic_class_2": "Strength on open-source dataset", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite minor weaknesses in discussing limitations and ethical considerations, the paper's quality, novelty, and clarity make it a valuable addition to the literature.", "topic_class_1": "Quality Summary", "topic_class_2": "Summary of strengths and weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the capabilities of large language models (LLMs) for understanding HTML.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It proposes three tasks to benchmark HTML understanding: Semantic Classification of HTML elements, Description Generation for HTML inputs, and Autonomous Web Navigation of HTML pages.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper evaluates various LLM architectures and sizes, with and without pre-training on NLP and HTML corpora.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The results demonstrate that LLMs, particularly encoder-decoder models like T5, transfer well to HTML understanding tasks and achieve high performance with significant sample efficiency.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, the paper introduces a new large-scale HTML dataset for description generation distilled from CommonCrawl.", "topic_class_1": "Dataset Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel approach: This work is among the first to explore LLMs for understanding raw HTML without pre-parsing, offering a new perspective on web automation and understanding.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strong performance: LLMs, especially T5 models, demonstrate remarkable performance across all tasks, achieving state-of-the-art results on the MiniWoB benchmark with significantly less training data.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Sample efficiency: Pre-trained LLMs require significantly less labeled data compared to models trained from scratch, reducing data collection costs and time.", "topic_class_1": "Data Efficiency", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open-source dataset: The paper contributes a new large-scale HTML dataset for description generation, promoting further research in this area.", "topic_class_1": "Dataset Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited context window: The paper highlights the context window length as a bottleneck for LLMs in handling real-world websites, which are often much larger than the pages used in the benchmarks.", "topic_class_1": "Experiment Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Snippet extraction impact: While necessary for longer pages, snippet extraction can lead to information loss and performance degradation.", "topic_class_1": "Experiment Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Error analysis: The error analysis could be more comprehensive, exploring more diverse error types and providing deeper insights into model behavior.", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "This is not regarded as a good review, which blurredly describe the weakness without providing concrete evidence.", "error_type": "Vague Critique"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is generally well-written and clear, although some details about the pre-processing and model training could be further elaborated.", "topic_class_1": "Calrity", "topic_class_2": NaN, "reliability": "No", "explanation": "However, there are a few grammaritcal mistakes in the paper. For example, in the caption of Figure 2, \"improves the task success 16%\" should be \"improves by.\" Also, in Section 8.2, \"We compare with a fine-tuned encode\" should be \"compare it with xxx.\"", "error_type": "Writing"}, {"segment_text": "Quality: The research is of high quality, employing rigorous evaluation methods and comparing against relevant baselines.", "topic_class_1": "Comprehensive experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: While the individual components (LLMs, HTML understanding tasks) are not entirely new, the combination and the focus on raw HTML processing offer significant novelty.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides details about the datasets and models, and with the planned release of the code and dataset, the results should be reproducible.", "topic_class_1": "Reporducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a compelling case for using LLMs in HTML understanding tasks.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The strong performance, sample efficiency, and minimal pre-processing requirements offer a promising direction for web automation and related applications.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the limitations of context window size and the impact of snippet extraction require further investigation.", "topic_class_1": "Experiment Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a valuable contribution to the field of NLP and web technologies, deserving acceptance and further exploration.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "rejected", "title": "WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus", "body_text": "\n\nW EB B RAIN : L EARNING TO G ENERATE F ACTUALLY C ORRECT A RTICLES FOR Q UERIES BY G ROUNDING ON L ARGE W EB C ORPUS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n In this paper, we introduce a new NLP task \u2013 generating short factual articles for queries by mining supporting evidence from the Web. In this task, called W EB - B RAIN , the ultimate goal is to generate a fluent, informative, and factually-correct short article ( e.g. , Wiki article) for a factual query unseen in Wikipedia. To en- able experiments on W EB B RAIN , we construct a large-scale dataset WebBrain- Raw by extracting English Wikipedia articles and their crawlable Wiki references. WebBrain-Raw is ten times larger than the previous biggest peer dataset, which can greatly benefit the research community. Besides, we empirically analyze the performances of the current state-of-the-art NLP techniques on W EB B RAIN and introduce a new framework ReGen, which enhances the generation factualness by improved evidence retrieval and task-specific pre-training for generation. Exper- iment results show that ReGen outperforms all baselines in both automatic and human evaluations.\n\n1 I NTRODUCTION\n\n Information acquisition is one of the fundamental daily needs of human beings. Acquiring infor- mation from the Web undoubtedly is a convenient and efficient way. However, with the exponential growth of the Web, information on the Web becomes scattered and evolves quickly, making it chal- lenging for users to acquire the expected information quickly. As a result, the Wikipedia articles become the best bet for most users when searching answers for factual queries on the Web (Singer et al., 2017). The reason is that the Wikipedia articles provide credible contents in which most claims can be supported by references from reputable sources. While Wikipedia is a good source of answers for factual queries, the need for manual editing (crowd-sourcing and editor checking) curbs its growth of coverage on a broader range of information needs. What if the Wikipedia articles can be automatically generated? In this paper, we introduce a new task, W EB B RAIN , exploring the capacity of generating short fac- tual articles for queries via a large web corpus. Given a factual query, the goal of the task is to enable a system to mine supporting evidence from the Web and generate a short factual article in which the claims are supported by the mined evidence (defined in Section 3.1). One of the potential generation targets for W EB B RAIN is the first section of a new Wiki page, based on which we can further explore generating long factual articles ( e.g. , a complete Wiki page). W EB B RAIN can be greatly helpful in various scenarios, including generating Wiki pages for new entities, intel- ligent writing assistance, knowledge-intensive QA, etc. W EB B RAIN \u2019s goal is considered one of the ultimate goals of the future search engine (Metzler et al., 2021). Figure 1 illustrates a case of our W EB B RAIN . 1 To establish the data foundation of W EB B RAIN , we construct a large-scale dataset, WebBrain-Raw, from scratch by extracting all English Wikipedia articles and all the corresponding reference arti- cles. To the best of our knowledge, W EB B RAIN -Raw is the biggest dataset sourced from Wikipedia (about 10 \u00d7 larger than the previous biggest peer WikiSum (Liu et al., 2018), introduced in Sec- tion 3.2). Along with W EB B RAIN , we empirically investigate the ability of the current state-of-the- art techniques and conclude that most current models lack the ability to correctly cite references and 1 the text generation result is obtained via the OpanAI\u2019s GPT3 API: https://beta.openai.com/ 1\n\nIntroduce GPT-3 Pre-trained Language Models\n\n (e.g.,BART, T5, GPT-3) GPT-3 is a machine learning platform that enables developers to train and deploy AI models . It is also said to be scalable and efficient with the ability to handle large amounts of data. Introduce GPT-3 query WebArticles Retrieval Module [1] Shead, Sam (July 23, 2020). \"Why everyone is talking about the A.I. text generator released by an Elon Musk-backed lab\". CNBC. [2] Bussler, Frederik (July 21, 2020). \"Will GPT-3 Kill Coding?\". Towards Data Science. Generative Module ReGen Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model, which is created by OpenAI, an artificial intelligence research laboratory. [1] GPT-3 is part of a trend in natural language processing (NLP) systems of pre-trained language representations . [2] Generation ( ) WebBrain generate factual descriptions. Based on the best current models (SPLADE+FiD+BART) (Izacard & Grave, 2021; Formal et al., 2021a), we propose a new framework, ReGen, which enhances the factual correctness of generation by (1) controlling the topic consistency of the retrieved supporting evidence; (2) introduce the citation mark into the generation process to indicate the source of a ref- erenced claim; (3) warm-up the generation model by pre-training with fact-oriented self-supervised tasks (described in Section 4). Current tasks that are similar to W EB B RAIN include: (1) text generation with generative pre-trained language models ( e.g. , GPT-3) (Brown et al., 2020); (2) retrieval-augmented QA ( e.g. , RAG (Lewis et al., 2020b), GopherCite (Menick et al., 2022), REALM (Guu et al., 2020)); (3) multi-document summarization (MDS) (Liu et al., 2018; Liu & Lapata, 2019); (4) WebGPT (Nakano et al., 2021). W EB B RAIN distinguishes itself from the existing tasks on the following aspects: (1) existing gener- ative models generate text solely depending on the implicit knowledge stored in the model\u2019s param- eters, which are prone to generating factually incorrect statements, a phenomenon commonly called hallucination . In contrast, our W EB B RAIN aims to generate factual statements depending on the supporting evidence mined from the Web; (2) Retrieval-augmented QA utilizes the retriever to en- hance answering specific questions whose answer is usually a text span or a single sentence. Given a factual query, our W EB B RAIN explores capturing all the knowledge available on the Web to gen- erate a comprehensive and accurate short article; (3) W EB B RAIN has a more complex pipeline than the MDS task, which focuses on summarizing documents that are already prepared. W EB B RAIN requires models to mine useful evidence before generating the factual article; (4) WebGPT mimics the behavior of a human by browsing the Web to answer a specific question. This involves many challenges that are difficult to solve ( e.g. , collecting training data by recording human behaviors). Instead, W EB B RAIN grounds on a large number of Wikipedia articles that are already created and edited by crowds, which makes the task much more realistic. In summary, our contributions are threefold: (1) we introduce a new task, W EB B RAIN , dedicated to answering factual queries by generating short factual articles based on evidence retrieved from a large web corpus; (2) we construct a large-scale dataset, WebBrain-Raw, to evaluate the potential of W EB B RAIN . WebBrain-Raw is ten times larger than the previous biggest peer dataset; and (3) we empirically analyze the performance of current state-of-the-art techniques on W EB B RAIN , and propose our factuality-enhanced framework, ReGen, which outperforms all baselines through both automatic and human evaluation. 2\n\n2 R ELATED W ORK\n\n Pre-trained Language Models Pre-trained language models (PLMs) have been widely applied to various natural language processing (NLP) tasks and achieved outstanding performance. In gen- eral, these models are first pre-trained on a large-scale corpus, and then fine-tuned on downstream datasets for specific tasks. By pre-training, the model can learn effective language representations, thus improving the performance on downstream tasks. Typical paradigms of PLMs include masked LM (Devlin et al., 2019; Zhang et al., 2019; Sun et al., 2019), left-to-right LM (Radford et al., 2019; Black et al., 2021), prefix LM (Dong et al., 2019; Bao et al., 2020), and encoder-decoder LM (Song et al., 2019; Raffel et al., 2020; Lewis et al., 2020a). Masked LMs are generally most suitable for natural language understanding or analysis tasks, e.g. , text classification and natural language inference. The other three types of PLMs can be naturally used for text generation. Wikipedia-related Tasks Wikipedia is a popular knowledge source for NLP tasks that aim to uti- lize external knowledge. Examples include knowledge-grounded conversation (Dinan et al., 2019), fact checking (Thorne et al., 2018), open-domain QA Kwiatkowski et al. (2019), and slot filling El- Sahar et al. (2018) etc. Petroni et al. (2021) propose a benchmark for those knowledge-intensive lan- guage tasks (KILT) in which all tasks apply the snapshot of Wikipedia as the knowledge base. Com- pared to the KILT, other Wikipedia-related tasks more focus on the Wikipedia itself. Liu et al. (2018) construct a dataset WikiSum from Wikipedia to enable multi-document summarization (MDS). The MDS task is then further explored by Liu & Lapata (2019) with a hierarchical model and Perez- Beltrachini et al. (2019) with a topic-guided structure. WikiWrite (Banerjee & Mitra, 2016) propose a systematic solution to constructing Wiki pages by assigning retrieved content into different topical sections. Fruit (Iv et al., 2022) explores update information of existing Wiki pages. Recently, Piktus et al. (2021) construct a dataset Sphere from CCNet (Wenzek et al., 2020), using which they explore refining the citation quality of Wikipedia pages. Retrieval-Augmented Text Generation Pre-trained language models are prone to generating hal- lucinations ( i.e. , factually incorrect statements) (Vinyals & Le, 2015; Koehn & Knowles, 2017; Rohrbach et al., 2018; Raunak et al., 2021). Retrieval-augmented text generation, as a new text gen- eration paradigm, can be a possible way to alleviate this problem. Compared with generation-based counterpart, this new paradigm can reduce the reliance of storing enormous knowledge in param- eters by providing more references. Retrieval-augmented text generation has been widely applied to many NLP tasks, such as dialogue generation (Weston et al., 2018; Zhu et al., 2020), machine translation (Gu et al., 2018; Cai et al., 2021), and open-domain question answering (Lewis et al., 2020b; Guu et al., 2020). In this work, we formalize a new retrieval-augmented text generation task \u2013 W EB B RAIN with two new features: (1) The retrieval base is a large-scale open-domain web cor- pus; and (2) The target is to generate a natural, informative, and factual text rather than a short span. Both features make the task extremely challenging. Our experiments will show that though existing models can be applied to this task, there is still a large space for research and exploration.\n\n3 T ASK : W EB B RAIN 3.1 T ASK D EFINITION\n\n Formally, given an input query q , our task is to generate a short article T q for q grounded by prior knowledge \u0398 . We have: 1) For a pre-trained language model solely, the knowledge prior \u0398 is stored in the PLM\u2019s internal parameters. For W EB B RAIN , the knowledge prior \u0398 contains the implicit language knowledge \u03b8 stored in the PLM\u2019s parameters and the explicit knowledge \u03b3 defined by the supporting evidence V , which is mined from an external large web corpus C . In practice, C can be an off-line processed large web corpus, an online search engine API, or a combination of both. Intuitively, coupling \u03b8 and \u03b3 , \u0398 = ( \u03b8, \u03b3 ) can be characterized by the strong language capacity from the PLM and the factual constraints from the supporting evidence. Supposing the generated T q contains n sentences ( t 1 , \u00b7 \u00b7 \u00b7 , t n ) and the retrieved supporting evidences V = ( v 1 , \u00b7 \u00b7 \u00b7 , v k ) \u2208 C , we expect each sentence t would be supported by a V \u2019s subset V \u03c4 , denoted 3 as t \u2190 V \u03c4 . More precisely, we have: Note that when the generated sentence only relies on the PLM\u2019s language knowledge \u03b8 ( e.g. , com- mon sense generation), the corresponding supporting evidence is V \u03c4 = \u2205 .\n\n3.2 D ATA C OLLECTION\n\n Wikipedia is one of the most inclusive and widely-used encyclopedias on the web. By the nature of an encyclopedia, Wikipedia can be regarded as a large collection of factual articles of various topics ( e.g. , entry of a Wiki page). The factual content in Wikipedia are generated by crowd-sourcing, in which most claims can be supported by the citations in the References section. Due to such nature, Wikipedia is the most appropriate resource to leverage for W EB B RAIN . The specific reasons are: (1) Wikipedia\u2019s format conforms to the W EB B RAIN task\u2019s definition in Eq. (2). We can view the Wiki page title as a factual query q , the citations (or parts of them) as the retrieved supporting evidence \u03b3 , and the Wiki article as the target factual article T \u2217 q ; (2) Wikipedia has a huge volume of references which are usually reputable sources. Using the references, we can build a large web corpus of good quality as the retrieval source; (3) Wikipedia covers a wide range of topics of common interests, which provides a good knowledge basis for models to generalize to open-domain queries. Many previous studies collected Wikipedia data for different tasks (Liu et al., 2018; Liu & Lapata, 2019; Perez-Beltrachini et al., 2019). However, none of these datasets provides full-size Wikipedia pages and the corresponding references. To lay a good data foundation to W EB B RAIN , we build a large-scale dataset from scratch by extracting all Wikipedia pages and all crawlable references. Table 1 shows the statistics of different datasets. In the Supplementary Material, we attach 500 raw data samples and the corresponding experiment data for demonstration. The full datasets we construct for W EB B RAIN will be released upon acceptance of this paper. Data Cleaning As shown in Table 1, the WebBrain-Raw data contains full-size Wiki pages and a huge volume of references which are open-domain web pages. Regarding data quality, the Wiki pages are good due to their server stability and format consistency. How- ever, the references contain much noise or are useless for our task. For example, they may be images, tables, or have empty content etc. To guarantee data quality, we process the WebBrain-RAW dataset in the following steps: (1) Wiki Article Cleaning : for Wiki pages, we remove the Wiki template 2 , special symbols ( e.g. , ASCII symbols), and other invalid text 3 . (2) Reference Cleaning : for reference, we filter out articles that are shorter than 16 words or non- English token ratio > 0 . 3 . We remove invalid text such as those containing only HTML tag, unusu- ally long sentences ( e.g. , > 256 tokens), text tables and URLs, etc. Furthermore, as the citations 2 see details in https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Templates 3 We observe some template text that may not be useful for model training ( e.g. , You can help Wikipedia by expanding it. ) 4 may be missing (uncrawlable or filtered out) or incorrectly cited (Piktus et al., 2021), we define the following metric to calculate the term recall of a reference to choose \u201creasonable\u201d references: where Q is the term set of a query q , T is the term set of a Wiki article\u2019 sentence t \u2190 V \u03c4 that cites a reference set V \u03c4 , V is the term set of V \u03c4 , and S is a set of stop words. Intuitively, if more terms in the sentence claim t can be found in the references V \u03c4 , the references would be more useful. Therefore, we only keep references that have P ST > \u03c1 . Reference Passage Selection The original Wiki articles and reference articles tend to be very long. To adapt the capacity of most pre-trained language models ( e.g. , BERT has a 512 token limit), we use the first section of Wiki articles as the generation target, and we select the passage from the reference article with the highest P ST value as the supporting passage. Specifically, we first split a reference article by sentences and concatenate the sentences into passages (stride = 1 , max length = 256 ) We then compute the P ST value for each passage and keep the highest one. Dataset Generation As defined in Eq. (2), given a query q , the W EB B RAIN task aims to re- trieve the supporting evidence V from a web corpus C , so as to generate a factual article T q . To enable training the in-domain retriever and generator for W EB B RAIN , we generate the WebBrain- R(etriever) and WebBrain-G(enerator) datasets from WebBrain-Raw. For WebBrain-G, we keep five supporting passages for each query 4 , and retrospectively assign reference marks to all unmarked sentences in the Wiki article by measuring their P ST value. We append a \u201c [0] \u201d mark to the sen- tence end for these fail to match any references. For WebBrain-R, we use the supporting passages as the positive and randomly select four negative passages from the Top30-50 retrieved results via a BM25 engine. Table 2 shows the statistics of the two datasets.\n\n4 F RAMEWORK : R E G EN 4.1 R ETRIEVER\n\n By the definition of W EB B RAIN in Eq. (2), ReGen starts from a retriever that mines the support- ing evidence V from the large web corpus C . ReGen utilizes a parametric sparse lexical model SPLADE (Formal et al., 2021a;b) as the retriever, which encodes a sequence s = ( w 1 , \u00b7 \u00b7 \u00b7 , w n ) into a sparse lexical representation s \u2208 R | D | by predicting token-level importance in WordPiece vocabulary size ( e.g. , | D | = 30522 ). Specifically, s is computed based on the dense representation ( h 1 , \u00b7 \u00b7 \u00b7 , h n ) output by the underlying PLMs: where W \u2208 R 768 \u00d7 768 and b \u2208 R | D | are a trainable transition matrix and a bias, respectively, and E \u2208 R 768 \u00d7| D | refers to the PLM\u2019s input embedding matrix. In the training phase, given a query q and a document d , the ranking score g ( q, d ) is computed by the dot product of the their representations s q and s d . Previous studies that utilize a retriever in the generation loop usually apply the traditional term-match-based retriever ( e.g. , BM25), dense passage retriever ( e.g. , DPR), or a hybrid re- triever (Karpukhin et al., 2020; Lewis et al., 2020b; Izacard & Grave, 2021; Glass et al., 2022) We build ReGen\u2019s retriever based on SPLADE because of the following reasons: (1) Factual queries in W EB B RAIN are usually open-domain and entity-centric, while recent work observed that DPR tend to perform poorly for entity-centric queries or the out-of-domain queries (Sciavolino et al., 2021; Xiong et al., 2021; Thakur et al., 2021). SPLADE, on the contrary, inherits the good properties of sparse retrievers such as exact-entity match and generalizability, making it suitable to W EB B RAIN . (2) SPLADE also inherits the contextualized semantics of the underlying PLMs, enabling the query encoder to perform contextualized query expansion (Formal et al., 2021a), which alleviates the vo- cabulary mismatch problem of BM25. (3) Empirical results also prove the effectiveness of SPLADE in W EB B RAIN (see Section 5.3). 4 statistics shows that the first section of 96%+ Wiki pages has equal or less than five references. 5 Ranking Loss Generally, given a query q , a positive document d + and a set of negative documents d \u2212 , the optimization goal of a retriever is to maximize the probability: where D represents the entire corpus. We mine the negative documents via a BM25 engine and also include in-batch negative samples into d \u2212 . We train the model with a contrastive LCE loss (Gao et al., 2021). Asymmetric Sparsity We introduce two sparsity constraints to control SPLADE\u2019s sparsity: L1 regularization and FLOPS regularization (Paria et al., 2020). For the query encoder and document encoder, we apply asymmetric sparsity regularization. We use the L1 regularization with a small weight \u03bb q for the query encoder and FLOPS regularization with a big weight \u03bb d for the doc encoder. Thus, the final loss for SPLADE becomes: We apply the asymmetric sparsity because: (1) a small \u03bb q lowers the sparsity of the query encoder, enabling the query encoder to generate semantically-rich query representation; (2) the FLOPS regu- larization is not sensitive to the number of documents in a batch so that we can compute L FLOPS on both positive and negative samples; (3) a big \u03bb d increases the sparsity of doc encoder, making the off-line document index more efficient, and producing a self-denoising effect \u2013 more unimportant tokens tend to be ignored. Topic Consistency and Diversity For a document pair ( d m , d n ) and their | D | -size sparse repre- sentations ( s m , s n ) , we measure their topic distance by: where s = ( s 1 , \u00b7 \u00b7 \u00b7 , s j ) , s \u2032 is a binary variant of s , and \u00b5 is a threshold to eliminate tokens with low importance. For ( d m , d n ) , when d T \u2192 0 , the two documents tend to have the same impor- tant tokens and are likely to be redundant; whereas when d T \u2192 1 , the two documents tend to have non-overlapping important tokens and are likely topically irrelevant. In the ReGen frame- work, we propose a strategy to maintain the topic consistency and avoid redundancy: Considering k sorted retrieved evidence documents ( d 1 , \u00b7 \u00b7 \u00b7 , d k ) , we first filter out the documents d i that have d T ( d 1 , d i ) > 0 . 9 , i \u2208 [2 , k ] (topically-irrelevant to the top-1 document). We then remove documents d i that have d T ( d i , d j ) < 0 . 1 , j \u2208 [1 , i \u2212 1] (redundant to fore-rank documents).\n\n4.2 G ENERATOR\n\n ReGen employs a sequence-to-sequence model with a Fusion-in-Decoder (FiD) structure (Izacard & Grave, 2021) as the generator. The generator takes the query and references as input, and generates a factual description. More precisely, given a query q and a set of k references { v 1 , v 2 , \u00b7 \u00b7 \u00b7 , v k } returned by the retriever, we add special tokens before the query/reference and concatenate the query with each reference as: These sequences are processed independently by the encoder as: Finally, the decoder conducts attention over the concatenation of the representations of all the se- quences, and outputs a text sequence T \u2217 with auto-regressive mechanism: 10 The model can perform evidence fusion in the decoder and generate more factual claims with the references. The generator is optimized via minimizing the negative log-likelihood function of the ground-truth text: 6 Model Warm-Up for Sentence-Reference Matching In our task, the model is trained to gener- ate claims for a query based on several references. This task is difficult as the model should first find clues from various references and then determine which part is the most relevant in the current generation status. To facilitate the model\u2019s capability of extracting key information from the ref- erence, we design a model warm-up stage. Specifically, for a target text, we break it into multiple sentences and extract the ones that are marked with references. As a result, we can obtain several \u27e8 reference, sentence \u27e9 pairs. Then, we concatenate the query with the reference, and train the model to generate the corresponding sentence. This warm-up stage has two advantages : (1) The task is to generate a sentence with a single reference. Compared with the final generation task ( i.e. , gener- ating multiple sentences with multiple references), this warm-up task is much simpler. Therefore, the overall training follows a curriculum learning paradigm, i.e. , learning from easy task to hard task. This learning paradigm has shown to be effective in improving performance (Bengio et al., 2009; Hacohen & Weinshall, 2019). (2) Existing models often apply general tasks for pre-training. For example, BART (Lewis et al., 2020a) uses several sequence denoising tasks for pre-training. These tasks have different objectives from text generation. Our proposed warm-up task is also a text generation task, so it can help model mitigate the gap of tasks between pre-training and fine-tuning.\n\n5 E XPERIMENTS 5.1 S ETTINGS\n\n ReGen applies SPLADE as the basis of retriever Formal et al. (2021a), improves the vanilla SPLADE with a training strategy of asymmetric sparsity and uses a ranking strategy considering documents\u2019 topic consistency and diversity. For comparison, we report the performance of the vanilla SPLADE, DPR (Karpukhin et al., 2020) and BM25. We use RetroMAE as the underlying PLM of DPR, which is shown to be a state-of-the-art model in many retrieval benchmarks (Liu & Shao, 2022). For the generator, we use FiD+BART as the backbone model. As a comparison, we report the performance of BART (Lewis et al., 2020a), GPT-2 (Radford et al., 2019), and vanilla FiD+BART (Izacard & Grave, 2021). All models are initialized with the off-the-shelf checkpoints provided by HuggingFace (Wolf et al., 2019). The implementation details are in Appendix A. To evaluate the performance, we employ several automatic metrics as follows and perform a human annotation: BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015): These are metrics based on n -gram overlapping between generated text and ground-truth text. A higher value indicates the generated text is more similar to the ground truth. We compute the metrics using the nlg-eval package (Sharma et al., 2017). QAGS (Wang et al., 2020) and Triple Score (Goodrich et al., 2019) are two metrics for evaluating factualness. We also compute them between generated text and ground-truth text. 5 For QAGS, several questions are generated from the generated text, which is then answered by both the ground- truth text and the generated text. A higher value reflects that more identical answers are obtained, which further suggests that the generated text is more factual. To compute Triple Score, the relations in both generated text and ground-truth text are first extracted by OpenIE (Angeli et al., 2015). Then, the score is computed by the precision of relations in the generated text. A higher score indicates that more relations are correctly generated. We compute QAGS and triple score by FactSumm package (Heo, 2021). As for human annotation, we compare the result of our ReGen with that of baselines with respect to fluency, informativeness, and faithfulness. Detailed description of the annotation criteria is given in Appendix B.\n\n5.2 O VERALL R ESULTS\n\n The results of various models are shown in Table 3 and Table 5. As can be seen, our ReGen model (in both sizes) achieves the best results on all metrics. This clearly demonstrates the effectiveness of our framework. Specifically, (1) compared with BART and GPT-2, FiD and ReGen achieve better performance, especially on factualness metrics (QAGS and triple score). This indicates that the knowledge from Web corpus is critical for W EB B RAIN task. (2) Large models can always perform better than base models, but the main improvement is made on language quality ( e.g. , BLEU 5 These scores can also be computed between generated text and reference, but how to combine the scores based on multiple references is still under-explored. 7 and ROUGE) rather than factualness ( e.g. , QAGS). This is consistent with our assumption that the external knowledge is hard to be \u201cremembered\u201d in model parameters. (3) Compared with FiD, we design a warm-up stage for ReGen model. This strategy is effective regardless of model size, but works better for larger model. It suggests that larger models with more capacity are promising for better results, but their potential needs to be activated through well-tailored tasks.\n\n5.3 D ISCUSSION\n\n Impact of Retriever To explore how different retrievers impact the end-to-end performance of ReGen, we replace ReGen\u2019s under- lying retriever with BM25, DPR (RetroMAE), and SPLADE. From the results shown in Table 4, we can make the following observa- tions: (1) Compared to using golden references, the BLEU-1 score increases while the other metrics decrease, indicating that the re- trieved knowledge enriches the informativeness of generation but undermines the generation quality. This implies that the retrieval quality is crucial for the factuality of generation and how to retrieve helpful knowledge remains a challenge in the W EB B RAIN task. (2) ReGen surpasses the other three retrievers regarding ROUGE-L, CIDEr, and TripleScore, which implies that ReGen could retrieve more helpful knowledge. With such knowledge, ReGen could gen- erate better answers regarding factuality, confirming the effective- ness of the fact-enhance strategies applied in ReGen. (3) Regarding retrieval performance, our Regen outperforms all baselines on all metrics (see full retrieval results in Table 10). We also evaluate ReGen\u2019s performance with different numbers of retrieved references. Specifically, we randomly sample 100 queries from the test set and feed them into ReGen\u2019s generator together with zero to five references retrieved by ReGen\u2019s retriever. Figure 2 illustrates the results. Generally, with more references, BLEU score and text length increase, but ROUGE score and triple score decrease. This implies that more content is generated but the quality cannot be guaranteed. The BLEU score achieves the best with four references. Combining with the retriever\u2019s performance, we attribute this effect to the noisy nature of retrieved references. Finally, we note that, even with references, generating long and faithful text is still a challenging problem. Impact of Number of References As references provide evidence for generating factual content, their quantity will impact the quality of generation. To investigate this, we randomly sample 500 8 queries from the test set with zero to five references respectively (3,000 in total). Different from the previous analysis, we use ground-truth references in this experiment. Figure 3 shows the re- sults. We observe: (1) For models using no reference ( i.e. , BART and GPT-2), their performance is decreasing ( 0 \u2192 5 references). Intuitively, target texts using more references are much more difficult for generation, as they require more knowledge to construct the content. This result val- idates our assumption that writing human-like descriptions for queries heavily relies on external knowledge, which is difficult for pre-trained models to store as parameters. (2) The performance of FiD and ReGen is improving in terms of BLEU and TripleScore. This is because target texts with more references are usually longer, and the references can provide more information for generation. However, as suggested by ROUGE, more noise also appears in the generated text. Performance vs. GPT-3 GPT-3 (Brown et al., 2020) is a large-scale pre-trained language model with 175B parameters. The model demonstrates strong few- or zero-shot performance on many text-based tasks, as vast knowledge has been stored in its parameters. To com- pare this strong model with ours, we first randomly sam- ple 100 queries from the test set. Then, for GPT-3, we use the prompt Introduce [X]: to generate zero-shot re- sults. 6 For our model, we use the query and evidence pro- vided by the retriever as input. The results on automatic metrics and human annotation are shown in Figure 4 and the last row of Table 5. Our model outperforms GPT- 3 by a large margin in terms of BLEU, METEOR, and ROUGE, This imply that our model can generate texts that are closer to the ground-truth (written by human), indicating that the explicit knowledge in web corpus is essential for generating natural and informative text. Only increasing the size of the lan- guage model would fail to capture such knowledge. We also observe a clear improvement of our model on factualness metrics. Even with a large amount of parameters, it is evident that GPT-3 can- not guarantee the factualness. In contrast, evident from web corpus can reduce these hallucinations. Due to limited space, we present (1) a reference mark correction strategy in Appendix C; (2) a case study in Appendix D; (3) a system demonstration in Appendix E; and (4) future work in Appendix F.\n\n6 C ONCLUSION\n\n In this paper, we introduce a new NLP task, W E B RAIN , which aims to generate a fluent, informative, and factually-correct description for factual queries via mining supporting evidence from a large web corpus. We construct a large-scale dataset WebBrain-raw as the data foundation of W E B RAIN . WebBrain-raw is constructed by crawling all Wikipedia articles and the corresponding reference articles. Through empirical analysis, we found that most current NLP techniques lack the ability to maintain the factualness on the W E B RAIN task. To improve the performance on this task, we propose a new framework ReGen which uses several specifically designed strategies. It outperforms all baselines on automatic and human evaluation. We believe that W E B RAIN would highlight a valuable research pathway in which AI models could acquire knowledge from the Web world by themselves and serve human beings by fulfilling a broader range of fact-oriented information needs. 6 We use the model text-davinci-002 provided in https://beta.openai.com/playground . 9\n\nR EFERENCES\n\n 10 11 12\n\nPatrick S. H. Lewis\n\n\n\nEthan Perez\n\n\n\nAleksandra Piktus\n\n\n\nFabio Petroni\n\n\n\nVladimir\n\n 13 The web is your oyster - knowledge-intensive NLP against a very large web corpus 14 15 16\n\nA I MPLEMENTATION D ETAILS\n\n In our ReGen the retriever is training on the WebBrain-R dataset for five epochs, initializing from efficient-splade-V-large 7 . We set the batch size to 192, the max query length to 16, and the max passage length to 256. We use the AdamW optimizer with a learning rate of 2e-5. We set ( \u03bb q , \u03bb d ) = (5 e \u2212 4 , 5 e \u2212 3) for the Eq. 6. For baseline models, we train SPLADE and DPR with their official settings on the WebBrain-R dataset for five epochs. The underlying RetroMAE of DPR is initialized from the BEIR checkpoint 8 . We implement the models with Pytorch and Pytorch-Lightning 9 . We train the models on 8 Tesla V100 32G GPUs. For the BM25 retriever, we use the Pyserini tool with its default parameter setting (Lin et al., 2021). As for the generator, we use BART-based and BART-large checkpoints provided by HuggingFace (Wolf et al., 2019) to initialize the model. For training, we use 32 and 64 Tesla V100 32G GPUs for base and large models, respectively. Correspondingly, the batch size of each GPU is set as 8 and 4. AdamW optimizer (Loshchilov & Hutter, 2019) is used for optimization with a learning rate of 5e-5. In inference stage, we apply beam search (width=5), and the maximum generation length is set as 512. This generation setting is also used for GPT-3 (prompt).\n\nB H UMAN E VALUATION\n\n To further evaluate our ReGen model, we perform human evaluation in terms of three metrics: (1) Fluency evaluates the linguistic coherency of the generated text;(2) Informativeness measures whether the generated text gives a response in specific details based on the query and references, (3) Faithfulness reflects the proportion of the generated text that can be supported by the retrieved supporting evidence. To reduce the influence of objective factors in the manual annotation, we perform a pairwise evaluation method: by giving the results generated by ReGen and other baseline models, three annotators are asked to judge which text is better or make a tie. For fairness, the results from different models are randomly shuffled, and the annotators would not know the underlying model in advance. The evaluation results are shown in Table 5. 7 https://huggingface.co/naver/efficient-splade-V-large-query and https:// huggingface.co/naver/efficient-splade-V-large-doc 8 https://huggingface.co/SamuelYang/SentMAE_BEIR 9 https://pytorch-lightning.readthedocs.io/en/stable/ 17\n\nC R EFERENCE M ARK C ORRECTION\n\n Reference mark is used to indicate the source of a referenced text. Correct reference marks can directly improve the factualness and readability of the generated text. However, as the FiD model simultaneously performs attention on the representations of all tokens in multiple references, it is hard to identify the correct source of a generated sentence. We try to alleviate this problem by a Reference mark-Enhanced Decoding (RED) strategy: in inference stage, if a reference mark (other than [0] ) has the largest generation probability at a decoding step, we extract the last generated sentence and match it with each reference. Then, the reference with the largest word overlapping ratio is selected as the source of the sentence ( i.e. , its reference mark is used for this sentence). To test this strategy, for each generated text, we compute a refer score as: P r = 1 n (cid:80) |T \u2229V\u2212 S | |V\u2212S| , where n is the number of sentences with references; T is the set of words in the sentence; V is the set of words in the corresponding reference; and S is the set of stopwords. This score is similar to STP score defined in Eq. 3 but focuses on how much content of sentence is quoted from the corresponding reference. From the results shown in Figure 5, we can see this strategy improves refer score greatly but has less influence on other metrics. This indicates that the reference mark is labeled more accurately. We apply this strategy in our demo (described in Appendix E), which also improves the demo\u2019s readability. However, our strategy is just an early exploration on reference mark accuracy. It is only involved in the inference stage, but not in model training. Moreover, how to evaluate the reference mark accuracy is also an unsolved problem. We leave these problems as our future work. 18\n\nD C ASE S TUDY\n\n To make a qualitative analysis of our model, we conduct a brief case study, as shown in Table 11. The green texts are correct information, while red ones are incorrect. In the first case, as no reference is given, FiD, BART and ReGen generates similar results, but ReGen can provide more details. We attribute this to the warm-up strategy, it can effectively mitigate the gap of tasks between BART\u2019s pre-training and our fine-tuning. In the second case, with one reference provided, both FiD and ReGen can generate more faithful text than BART. This clearly reflects the important of external knowledge. Intriguingly, we can also observe that ReGen can better capture clues from the reference. The result also demonstrates the effectiveness of our proposed warm-up strategy, as it can facilitate our model\u2019s capability of sentence-reference matching.\n\nE S YSTEM D EMONSTRATION\n\n Though there is a gap between our study and our ultimate goal of generating Wikipedia articles, we still explore potential application scenarios for our current model. We build a demonstration system as shown in Figure 6. This system can accept a user query and extract the keywords. Then, several references are retrieved from ReGen\u2019s retriever, based on which the generator can create a short article for the query. Furthermore, with the reference mark-enhanced decoding strategy, the reference mark is generated more accurately, which greatly enhance the readability of the generated text. This system can help us obtain some basic knowledge of entities. 19\n\nF L IMITATION AND F UTURE W ORK\n\n In this work, we explore generating short factual articles (like Wikipedia page) for queries by col- lecting evidence from the Web. This is a very early step towards our ultimate goal: automatically building Wikipedia pages for unseen topics. We collect a new dataset to support future research, evaluate several existing models on the new task, and propose some effective strategies. However, there are still many limitations in our study, which needs further investigation in the future. (1) The retriever is far less than perfect. As shown in our experiments, ReGen\u2019s performance with retrieved evidence is worse than that with golden references. So, two problems appear: (a) how to improve the retriever\u2019s performance; and (b) how to generate better result with non-perfect refer- ences. (2) As we use FiD as our backbone model, it is hard to determine the reference mark accurately. Moreover, the reference mark accuracy cannot be measured by existing metrics. We plan to design new attention mechanism to facilitate the model\u2019s judgement on these reference marks . (3) In this study, we try to generate a short article for given queries . However , the original Wikipedia page contains more information than a short article. Generating long articles will be another chal- lenge in the future . 20 , Query : Charles Lovell (trade unionist) Ground-truth reference 1 : The Guardian Charles Lovell, who has died aged 91, led the plumbers\u2019 trade union the ptu (later the eetpu) from the 1960s to the 1980s, a period of great turmoil and change in the world of industrial relations. Charles developed partnerships between his union and employer bodies representing the industries in which his members served. The outcome was enormously beneficial to both parties and to the industry. Son of Michael, a warehouseman, and Isabella, Charles was born in Newcastle upon tyne and began work in 1939. He was offered an apprenticeship as a plumber and joined the plumbing trade union, but as the second world war approached, he was drafted to swan hunter\u2019s shipyard under an essential works order. He was never happy there and returned to construction as soon as he was able. After the war, he also immersed himself in trade union activities and became chairman of the wallsend trades council and a labour councillor, as well as learning esperanto and providing for his beloved wife, Irene (nee lamb), whom he married in 1944, and their three daughters and two sons. In 1966 he was elected to the executive of the ptu and in 1968 became its general secretary. To make his union more secure, one of his first acts was to merge it with the electrical trade union to form the eetpu. Ground-truth target : Charles Lovell (1923 \u2013 13 october 2014) was a british trade unionist. Born in Newcastle- upon-Tyne, Lovell left school in 1939 and began an apprenticeship as a plumber, also joining the plumbing trades union. Soon after, he was drafted to work at the swan hunter shipyard, but returned to plumbing imme- diately after world war ii.[1] Lovell joined the labour party and was elected as a councillor, and also chaired wallsend trades council. He was elected to his union\u2019s executive council in 1966, and two years later won election as its general secretary.[1] He immediately led it into a merger with the electrical trades union, becom- ing national officer of the new electrical, electronic, telecommunications and plumbing union. In this role, he created a national pension scheme for plumbers, and championed more flexible apprenticeships, which could be completed under a variety of employers.[1] Lovell retired in 1987, and was made an officer of the order of the British empire. In his spare time, he learned Esperanto.[1] BART : Charles Lovell was a British trade unionist. Born in london, Lovell worked as a coal miner, and became active in the miners\u2019 federation of great Britain. He was elected as the union\u2019s general secretary, and also served on the general council of the national union of mineworkers. Lovell also served as a member of the executive of the trades union congress, and was a delegate to the general conference of the international labour organization. He stood unsuccessfully for the labour party at the 1918 united kingdom general election, but was elected at the 1922 general election. In 1923, he was appointed as general secretary of the union, a post he held until his death. FiD : Charles Lovell (1921 \u2013 30 June 2011) was a British trade unionist. Born in Newcastle upon Tyne, Lovell began working as a plumber in 1939. He joined the plumbing trade union, and was elected to its executive in 1953. He was also elected to the wallsend trades council, and served as its chairman in the 1960s.[1] In 1966, Lovell was elected as the general secretary of the plumbers\u2019 trade union, and led the union through a period of turmoil and change in the world of industrial relations.[1] Lovell was also active in the labour party, serving as a councillor on wallsend borough council and as a member of the national executive committee of the trades union congress.[1] He retired from the union in 1986, and died in 2011, aged 91.[1] ReGen : Charles Lovell (1928 \u2013 15 July 2016) was a British trade unionist. Born in Newcastle upon Tyne, Lovell began work in 1939. He joined the plumbing trade union, but as the second world war approached, he was drafted to swan hunter\u2019s shipyard under an essential works order. He was never happy there and returned to construction as soon as he was able.[1] After the war, he also immersed himself in trade union activities and became chairman of the wallsend trades council and a labour councillor, as well as learning Esperanto and providing for his beloved wife, Irene (nee lamb), whom he married In 1944, and their three daughters and two sons. In 1966 he was elected to the general council of the trades union congress.[1] He led the ptu from the 1960s to the 1980s, a period of great turmoil and change in the world of industrial relations. Charles developed partnerships between his union and employer bodies representing the industries in which his members served.[1] The outcome was enormously beneficial to both parties and to the industry.[1] 21", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper introduces a new task called WebBrain, which aims to generate short factual articles for queries by mining supporting evidence from a large web corpus.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "To enable experiments on WebBrain, the authors construct a large-scale dataset called WebBrain-Raw by extracting English Wikipedia articles and their crawlable Wiki references.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "WebBrain-Raw is ten times larger than the previous biggest peer dataset.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors empirically analyze the performance of current state-of-the-art NLP techniques on WebBrain and propose a new framework called ReGen, which enhances generation factualness through improved evidence retrieval and task-specific pre-training.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiment results show that ReGen outperforms all baselines in both automatic and human evaluations.", "topic_class_1": "Summay of experiments", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1.The paper addresses an important and challenging task of generating factual articles for queries using evidence from the web.", "topic_class_1": "strength on novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This has significant practical applications.", "topic_class_1": "strength on novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "2.The WebBrain-Raw dataset constructed by the authors is very large-scale (10x bigger than prior datasets) and will be a valuable resource for the research community.", "topic_class_1": "strength on dataset scale", "topic_class_2": "contribution", "reliability": "No", "explanation": "It is large-scale, but the quality of the dataset has not been verified, so it is unclear how much value it has to the community.", "error_type": "Inexpert Statement"}, {"segment_text": "3.The proposed ReGen framework introduces several novel and effective techniques to improve factual correctness of the generated articles, such as asymmetric sparsity regularization in the retriever, topic consistency and diversity strategies in evidence selection, and task-specific pre-training for the generator.", "topic_class_1": "strength on method novelty", "topic_class_2": "contribution", "reliability": "No", "explanation": "This framework does not introduce novel techniques into the pipeline steps.", "error_type": "Unstated statement"}, {"segment_text": "4.Extensive experiments are conducted, demonstrating that ReGen outperforms various strong baselines on both automatic metrics and human evaluation of fluency, informativeness and faithfulness.", "topic_class_1": "strength on comprehensive experiments", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1.While the WebBrain-Raw dataset is very large, the paper lacks details on how the queries and articles are split/sampled to create the WebBrain-R and WebBrain-G datasets used for experiments.", "topic_class_1": "weakness on lack of dataset details", "topic_class_2": "weakness: lack details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "More dataset statistics and analysis would be helpful.", "topic_class_1": "weakness on lack of dataset details", "topic_class_2": "weakness: lack analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "2.The generator in ReGen uses a simple reference mark correction strategy in decoding, but a more principled approach to improve the accuracy of citing evidence during generation is needed.", "topic_class_1": "comments on the method", "topic_class_2": "weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors do discuss this as a limitation and future work.", "topic_class_1": "recitation of the authors' remarks", "topic_class_2": "weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "3.Generating complete Wikipedia-style articles is extremely challenging.", "topic_class_1": "comments on the task", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The current work focuses on generating the first section, which is an important first step but still quite far from the ultimate goal.", "topic_class_1": "comments on the limitation of the current ", "topic_class_2": "weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "More discussion on the gaps and potential roadmap would strengthen the motivation and impact.", "topic_class_1": "comments on experiments and the need for more discussions", "topic_class_2": "weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "4.While the human evaluation results are encouraging, details are lacking on the specific criteria, annotation process, number of samples, and inter-annotator agreement.", "topic_class_1": "weakness on lack of human annotation descriptions", "topic_class_2": "weakness: evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A larger-scale and more rigorous human evaluation would make the results more convincing.", "topic_class_1": "comments on human evaluations", "topic_class_2": "weakness: evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "strength on writting", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed WebBrain task and ReGen framework are well-motivated and described.", "topic_class_1": "Summay of paper topic", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The overall quality of the work is high, with the main contributions being the new large-scale WebBrain-Raw dataset and the novel factuality-enhanced ReGen framework.", "topic_class_1": "comments on the overall quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty is demonstrated through the new task formulation, dataset construction, and methodological innovations in ReGen.", "topic_class_1": "Comments on novelty", "topic_class_2": "contribution", "reliability": "No", "explanation": "not agree 100%", "error_type": "Misinterpret Novelty"}, {"segment_text": "Assuming the code and data are released, the experiments should be reproducible.", "topic_class_1": "Comments on reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes valuable contributions in introducing the WebBrain task and dataset for generating factual articles grounded in web evidence, and proposing the ReGen framework that outperforms baselines through novel strategies for evidence retrieval and factuality-enhanced generation.", "topic_class_1": "Summay of paper topic", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While there is still a big gap to generating full Wikipedia articles, this work represents an important step in that direction.", "topic_class_1": "Comments on the significance of this work", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper could be strengthened with more dataset details, deeper analysis on remaining challenges, and more rigorous human evaluation.", "topic_class_1": "suggestion for improvements", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "But overall, this is a high-quality submission with notable contributions to the field.", "topic_class_1": "comments on the overall quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces a new NLP task, WEB BRAIN, which aims at generating short factual articles in response to queries by mining supporting evidence from the web.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors construct the WebBrain-Raw dataset from English Wikipedia articles and their references, making it significantly larger than existing datasets for similar tasks.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They analyze the performance of state-of-the-art NLP techniques on WEB BRAIN and propose a new framework, ReGen, which improves the generation of factually correct content through enhanced evidence retrieval and task-specific pre-training.", "topic_class_1": "summary of the method", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Their empirical results demonstrate that ReGen outperforms all baselines in both automatic and human evaluations.", "topic_class_1": "summary of the experiment results", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a clear, technically sound, and experimentally rigorous approach.", "topic_class_1": "strength on method", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The introduction of the WebBrain-Raw dataset is a notable strength, providing a valuable resource for the research community.", "topic_class_1": "strength on the new dataset", "topic_class_2": "contribution: dataset", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed ReGen framework's emphasis on factual correctness through improved retrieval and pre-training is innovative and addresses a significant challenge in generating content based on web-sourced evidence.", "topic_class_1": "strength on method", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the paper lacks a thorough discussion on the limitations of their approach, potential biases in the dataset, and the scalability of their solution to other languages or domains.", "topic_class_1": "weakness on the lack of in-depth discusssions of the method and dataset", "topic_class_2": "weakness: lack analyses", "reliability": "No", "explanation": "Studying the scalability of their method to other languages/domains should not be regarded as a weakness but  future work.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written, making it easy to understand the proposed task, methodology, and the significance of the results.", "topic_class_1": "strength on writting", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research quality is high, with thorough experimentation and a clear presentation of results.", "topic_class_1": "comments on the overall quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty lies in the task formulation and the ReGen framework, contributing new insights into factual content generation from web sources.", "topic_class_1": "strength on novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors' commitment to releasing the WebBrain-Raw dataset and the detailed description of the ReGen framework enhance the reproducibility of their work.", "topic_class_1": "strength on reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper makes a significant contribution to the field of NLP by introducing a challenging new task and a corresponding large-scale dataset.", "topic_class_1": "Summay of paper topic", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed ReGen framework effectively addresses the task, demonstrating superior performance over existing methods.", "topic_class_1": "summary of the experiment results", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the paper is strong in most aspects, further discussion on its limitations and broader applicability would strengthen its impact.", "topic_class_1": "comments on the overall quality", "topic_class_2": "weakness: lack analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 10", "Confidence": " 5"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces WebBrain, a new NLP task focused on generating short, factual articles for queries by mining supporting evidence from the web.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also presents WebBrain-Raw, a large-scale dataset built from Wikipedia articles and their references, and ReGen, a framework designed to improve factual correctness in generated articles.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "ReGen utilizes a sparse lexical model (SPLADE) for evidence retrieval and a sequence-to-sequence model with Fusion-in-Decoder (FiD) architecture for text generation.", "topic_class_1": "summary of the method", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper compares ReGen to several baselines and demonstrates its superior performance through automatic and human evaluations.", "topic_class_1": "summary of the experiment results", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel and valuable task: WebBrain addresses the challenge of generating factual articles, which has significant potential for applications like automatic Wikipedia page creation, writing assistance, and knowledge-intensive QA.", "topic_class_1": "Strength on potential application", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Large-scale dataset: WebBrain-Raw is ten times larger than similar datasets, providing a valuable resource for research on this task.", "topic_class_1": "Strength on dataset scale", "topic_class_2": "contribution: dataset", "reliability": "No", "explanation": "It is large-scale, but the quality of the dataset has not been verified, so it is unclear how much value it has to the community.", "error_type": "Misunderstanding"}, {"segment_text": "Improved factual correctness: ReGen's focus on evidence retrieval and task-specific pre-training leads to demonstrably more factual articles compared to baselines.", "topic_class_1": "Strength on improved results", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Detailed analysis: The paper provides a thorough analysis of the task, dataset, and model performance, including comparisons with different retrievers and varying numbers of references.", "topic_class_1": "Strength on detailed analysis", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Retrieval quality: While ReGen shows improvement, the reliance on retrieved evidence still leads to performance below that achieved with gold references, highlighting the need for further advancements in retrieval quality.", "topic_class_1": "weakness on retrieval quality", "topic_class_2": "weakness: performance relies on retrieval quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reference mark accuracy: The current model struggles to accurately identify the source of information for generated sentences, impacting factualness and readability.", "topic_class_1": "weakness on reference mark accuracy", "topic_class_2": "weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited article length: The current focus is on generating short articles, whereas full Wikipedia pages contain much more information.", "topic_class_1": "weakness on limited artical length", "topic_class_2": "weakness: usefulness in practical world", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Generating longer, comprehensive articles remains a challenge.", "topic_class_1": "comments on future challenges", "topic_class_2": "weakness: usefulness in practical world", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written and clearly presents the task, dataset, and model architecture.", "topic_class_1": "strength on writting", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-designed and executed, with comprehensive experiments and analysis.", "topic_class_1": "strength on the overall quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: WebBrain is a novel task, and ReGen introduces new strategies for improving factual correctness in generated articles.", "topic_class_1": "strength on novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides implementation details and plans to release the datasets upon acceptance, facilitating reproducibility.", "topic_class_1": "strength on reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a valuable contribution to the field of NLP by introducing WebBrain, a novel and challenging task with significant potential applications.", "topic_class_1": "Summay of paper topic", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed ReGen framework shows promising results in improving factual correctness, although challenges remain in retrieval quality and reference mark accuracy.", "topic_class_1": "summary of the method", "topic_class_2": "contribution and weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Future work should focus on addressing these limitations and expanding the task to generate longer, more comprehensive articles.", "topic_class_1": "comments on future challenges", "topic_class_2": "suggestions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Pre-trained Language Models can be Fully Zero-Shot Learners ", "body_text": "\n\nP RE - TRAINED L ANGUAGE M ODELS CAN BE F ULLY Z ERO -S HOT L EARNERS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose n on p arametric prompt ing PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evalu- ate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with abso- lute gains of 12.8% in accuracy on text classification and 18.9% on the GLUE benchmark.\n\n1 I NTRODUCTION\n\n Natural language understanding (NLU) has been important in many applications such as intelligent dialog assistants, online search, and social media analysis. Recent advancement of NLU has been driven by emergent pre-trained language models (PLMs) including BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018; 2019; Brown et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Prior studies show that PLMs obtain substantial knowledge during pre-training on raw text corpus (Petroni et al., 2019; Feldman et al., 2019). By fine-tuning on task- specific labeled data, PLMs exploit such knowledge and gain impressive accuracy on a wide range of NLP tasks, such as text classification (Kowsari et al., 2019), question answering (Rajpurkar et al., 2016), machine reading comprehension (Campos et al., 2016), etc. However, fine-tuning approaches are expensive. It requires labeled datasets, which are rarely avail- able for many tasks. Significant computational efforts are needed to update PLMs\u2019 parameters for multiple tasks. In addition, fine-tuning results in one distinct model for each task to maintain. How can we generalize a pre-trained model to many NLP tasks, without labeled or additional un- labeled data? Existing few-shot and zero-shot approaches propose to construct prompts to elicit desired predictions from PLMs (Brown et al., 2020). The main idea of prompting PLMs is to con- vert an input utterance to one with masked templates. For example, in text classification an input can be \u201cThe Warriors won the NBA championship 2022\u201d and it is instead converted to \u201cA [MASK] news: The Warriors won the NBA championship 2022\u201d. A PLM (e.g. BERT) takes the converted text and produces predictions for the masked token, along with the probability. Ideally, a PLM will generate a higher probability for the word \u201csports\u201d than \u201cpolitics\u201d on the [MASK] token. Although these prompting-based methods are effective, they require unlabeled data for training or huge human efforts to construct prompts and to choose designated tokens to represent class labels (Schick & Sch\u00a8utze, 2021a;b; Gao et al., 2021). In addition, these manually constructed verbalizers , i.e. mapping from words (e.g. \u201cbasketball\u201d) to class labels (e.g. S PORTS ), do not extend to new emerging categories after PLMs are deployed. 1 In this paper, we investigate the fully zero-shot learning problem for NLU where only the target label names are available but not the extra raw text. We propose n on p arametric prompt ing PLM (NPPrompt), a novel method to generate predictions for semantic labels without any fine-tuning. NPPrompt uses PLM\u2019s own embeddings to automatically find relevant words to labels (e.g. \u201cbas- ketball\u201d and \u201cNBA\u201d for S PORTS ), therefore it does not need humans to construct verbalizers. Our key idea is to search for the top k nearest neighbors to a label name in the embedding manifold and then generate and aggregate PLM\u2019s predicted logits from masked prompts. In the above case, both predicted values for \u201cbasketball\u201d and \u201cNBA\u201d contribute to the final prediction for the S PORTS cate- gory. In this way, NPPrompt can be easily generalized to any new categories as long as the category names are semantically meaningful. The contributions of this paper are as follows. a) We develop NPPrompt, a novel method for fully zero-shot learning with PLMs. b) We conduct extensive experiments on diverse language under- standing tasks including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results show that NPPrompt outperforms the previous zero-shot methods by abso- lute 12.8% in accuracy on text classification and 18.9% on the GLUE benchmark. Surprisingly, NPPrompt is on a par with the best prior method that trained with manual verbalizers, an additional knowledge base, and extra unlabeled data.\n\n2 R ELATED W ORK\n\n Prompting The success of GPT-3 (Brown et al., 2020) has attracted much attention to prompting engineering, a new way to leverage pre-trained language models. Brown et al. (2020) concatenate a few input and output pairs and feed them to the large-scale GPT-3 language model, which is an intuitive in-context learning paradigm, allowing the model to generate answers for additional cases autoregressively. Recent works (Schick & Sch \u00a8 utze, 2021a;b) show that small-scale pre-trained lan- guage models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and ALBERT (Lan et al., 2019) can also achieve decent performance using prompt-tuning. Prompting has been applied to a large variety of tasks such as Text Classification (Schick & Sch \u00a8 utze, 2021a), Natural Language Understanding (Schick & Sch \u00a8 utze, 2021b), Knowledge Probing (Petroni et al., 2019), and Relation Extraction (Han et al., 2021). Typically, a piece of prompt contains a template and a verbalizer. The language model predicts a probability distribution over vocabulary given the template and the verbalizer transforms it into a prediction over class labels. In this work, we focus on designing the verbalizers automatically. Verbalizer Design The verbalizer is an important component in prompting which bridges model outputs and labels and greatly impacts the performance. Schick & Sch \u00a8 utze (2021a) design human written verbalizers for prompting, however, they are highly biased towards personal vocabulary with inadequate coverage. Apart from manually designed verbalizers, some recent studies explore automatic verbalizer construction. Auto-L (Gao et al., 2021) uses re-ranking to find the label words set by fine-tuning the model on the candidates searched by RoBERTa; AutoPrompt (Shin et al., 2020) applies gradient-based search to create both prompts and label words automatically with a few trigger examples. But these approaches need to update parameters with gradient descent, which turns out to be infeasible without access to the model weights (e.g., GPT-3). KPT (Han et al., 2021) incorporates external knowledge into the verbalizer in which the unlabeled dataset is needed to refine the label words and thus is not applicable to scenarios where only label names are known. In contrast, our approach NPPrompt directly finds, without any gradient update, relevant words to label names with only PLM\u2019s initial word embedding. Zero-shot Text Classification The general zero-shot text classification usually focuses on clas- sifying texts into classes that are unseen during the training process. Transferring knowledge from seen classes to unseen ones requires accurate and discriminative descriptions of all classes (Liu et al., 2019a; Xia et al., 2018), joint embeddings of categories and documents (Nam et al., 2016) or seman- tic correlations among classes (Rios & Kavuluru, 2018; Zhang et al., 2019). However, these methods require supervised data for the known label set and thus cannot be extended to settings where no la- beled pairs for any category is available. (Meng et al., 2020) propose the LOTClass model that uses label names with self-training to do zero-shot classification. But LOTClass still requires unlabeled corpus for extracting the topic-related words and performing self-training. In this work, NPPrompt achieves competitive and even better performance without using any unlabeled dataset. 2 basketballsports\u2026physicsathleticsfootballtechnology POLITICS\u2026 Closest \u201cSPORTS\u201d Predict\n\n3 B ACKGROUND : P ROMPT - BASED T UNING FOR PLM S\n\n We first provide a standard paradigms, prompt-based tuning, that perform well in few-shot scenar- ios, before introducing our approach for the zero-shot case. Take N way text classification as an example. We aim to predict the label y \u2208 Y for each sentence, where Y is the label set with N distinct classes. Prompt-based tuning tunes PLM using customized prompts (Brown et al., 2020). The regu- lar prompt-based tuning converts a specific task to a cloze-style mask language modeling prob- lem. For each input example x (single sentence or sentence pair), we first apply a task tem- plate T on it, converting original input x to x prompt . For instance, we concatenate the template \u201c T ( \u00b7 ) = A [MASK] news : \u201d with the original input \u201cThe Warriors won the NBA championship 2022\u201d and wrap it into: (1) The verbalizer f in vanilla prompt engineering map a set of selected words V from the vocabulary to the original label space Y , i.e., f : V \u2192 Y . Inversely, we use M ( y j ) to denote the label words in V that are mapped into a specific label y j , \u222a y j \u2208Y M ( y j ) = V . Then we calculate the probability of label y where g ( \u00b7 ) is for aggregating the probability of label words into the probability of the label. Then PLMs can be fine-tuned by minimizing the cross-entropy loss with supervised examples.\n\n4 P ROPOSED M ETHOD : NPP ROMPT\n\n We inherit PLM with verbalizers framework but keep PLM\u2019s parameters frozen (Gao et al., 2021). The key idea of NPPrompt is using PLM\u2019s word embeddings to automatically construct verbalizers \u2013 mapping from words to labels \u2013 in a fully zero-shot way. It does not need any additional raw text corpus for fine-tuning. NPPrompt consists of two steps to compute predictions for any labels in a nonparametric form (Figure 1). 1) We search for all label words closely related to each class y j in PLM\u2019s token embedding manifold. 2) Then we use the PLM to predict values for [MASK] , filter them using each class\u2019s set of label words, and aggregate the properly weighed outputs to produce the final prediction. In the following, we describe NPPrompt for text classification but it generalizes to other language understanding tasks. k -Nearest-Neighbor Verbalizer Construction For each class label (e.g. \u201cSPORTS\u201d), we search over the whole vocabulary V for the top- k words nearest to the label name in the PLM\u2019s embedding 3 space. Here, the distance between words and label names is measured using the cosine similarity score. Other distance metrics work as well and are examined in Section 5. We denote k as the neighborhood number . Assuming the embeddings of word v i and label name y j are emb ( v i ) and emb ( y j ) respectively, the label words of the verbalizer for y j are selected by top- k ranking: where S ( \u00b7 ) is the cosine similarity function: S ( emb ( v i ) , emb ( y j )) = emb Since the PLM is already pre-trained on raw text corpus, it acquires sensible semantic knowledge and relatedness of words in the vocabulary. We use PLM\u2019s embedding to search for label words semantically relevant to given label names. For illustration, we show the found label words of two categories in the AG News dataset (Zhang et al., 2015) and the corresponding similarity scores in Table 1. Nonparametric Aggregation of Prompted Pre- dictions For each input text x , we construct a prompt-augmented sequence x prompt = T ( x ) with a [MASK] token. We use the PLM to pre- dict tokens for [MASK] . In contrast to previous prompting methods which directly calculate the probability over the surface labels, we use the nearest label words from above to compute the prob- ability for each output label. Only the words in a label\u2019s top- k neighborhood will contribute to the class prediction. The contribution from each label word is non-equal. To be specific, with T ( x ) , a PLM produces the logit vector \u0398 [MASK] for all possible words at the [MASK] token. Notice that if the whole vocabulary is V , \u0398 [MASK] \u2208 R |V| . Then we compute the class probability for a label y j by aggregating the logits filtered by the verbalizer\u2019s label words. We use kernel smoothing to aggregate as follows: Where the weight between label word v i and class name y j are defined as: Finally, the best class prediction is selected from the maximum of all labels: Notice since we use kernel smoothing on logits instead of probability, Q is also unnormalized prob- ability. For example, AG News has two classes y 1 = { S CIENCE } , y 2 = { S PORTS } . From Table 1, the verbalizer for S PORTS M ( y 1 ) includes label words \u201csports\u201d, \u201cathletics\u201d, etc, and the ver- balizer for B USINESS M ( y 2 ) includes label words \u201cbusiness\u201d, \u201ccorporate\u201d, etc. Given an in- put text x \u201cThe Warriors won the NBA championship 2022\u201d, the prompt-augmented sequence x prompt will be \u201cA [MASK] news : The Warriors won the NBA championship 2022\u201d. The PLM computes logits for every word \u0398( [MASK] = v | x prompt ) . NPPrompt computes the un- normalized probabilities for S PORTS and B USINESS , Q ( S PORTS | x ) = w ( \u201csports\u201d , S PORTS ) \u00b7 \u0398( [MASK] = \u201csports\u201d | x prompt ) + w ( \u201cathletics\u201d , S PORTS ) \u00b7 \u0398( [MASK] = \u201cathletics\u201d | x prompt ) + \u00b7 \u00b7 \u00b7 , Q ( B USINESS | x ) = w ( \u201cbusiness\u201d , B USINESS ) \u00b7 \u0398( [MASK] = \u201cbusiness\u201d | x prompt ) + w ( \u201ccorporate\u201d , B USINESS ) \u00b7 \u0398( [MASK] = \u201ccorporate\u201d | x prompt ) + \u00b7 \u00b7 \u00b7 . If the aggregated predic- tion Q for S PORTS is larger than B USINESS , NPPrompt outputs S PORTS . 4 There are certain conditions where one class has label names containing little semantic meaning or where several keywords are needed to define a label. For instance, in the DBPedia dataset (Lehmann et al., 2015), one class is related to N ATURAL P LACE , then we can use keywords { \u201criver\u201d, \u201clake\u201d, \u201cmountain\u201d } to represent this class. In this setting, we pick out the keyword with the maximum score calculated by Equation 4 to represent each label first. Then we choose the label with the largest score. We use \u03a6( y j ) to denote all keywords in class y j , and the final prediction is :\n\n5 E XPERIMENT\n\n We conduct extensive zero-shot learning experiments to demonstrate the effectiveness of our method. We present our implementation details together with the main results and address several research questions pertaining to NPPrompt in this section.\n\n5.1 D ATASETS , P ROMPT T EMPLATES , AND E XPERIMENTAL S ETUP\n\n We adopt sentiment classification tasks on two datasets, IMDB (Maas et al., 2011) and Amazon (McAuley & Leskovec, 2013), and topic classification tasks on another two datasets, AG News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). All datasets are in the English lan- guage. For each task, we directly use the test set to assess model performances, without incorporating validation or training sets for post- tuning or cherry-picking hand-crafted prompts. The statistics of each dataset are shown in Table 2. To concentrate on the verbalizer and reduce the influence of templates, we adopt multi- ple fixed manual templates following Hu et al. (2022). We report the best template used for the RoBERTa-large model in Table 3. We implement our experiments based on an open-source toolkit OpenPrompt (Ding et al., 2021), which aims to conduct prompt learning easily. We choose RoBERTa-large (Liu et al., 2019b) as our pre-trained language model. We report the best accuracy of classification results for all ex- periments using different neighborhood numbers. Since we directly use the pre-trained models for testing, there is no randomness (random seed) in this process. All experiments are conducted on Nvidia A6000 GPUs and more details can be found in Appendix A.1.\n\n5.2 B ASELINES\n\n We evaluate the following baseline methods. Semantic Retrieval We utilize sentence embedding models (Reimers & Gurevych, 2019) to ob- tain the embedding for each sentence and descriptions for each class. Then we calculate the cosine similarity between sentences and label descriptions. We assign the most similar class labels to the sentence. Particularly, we use all-mpnet-base-v2 from Hugging Faceas the sentence embedding model, and the descriptions for each class can be found in Appendix A.1. NSP-BERT Sun et al. (2021) propose text entailment tasks to replace text classification tasks and then use the Next Sentence Prediction (NSP) head to predict the results. We show the template we use in Appendix A.1. ManualVerb Manual verbalizers are defined by human experts with domain knowledge and we simply use the label words provided by OpenPrompt (Ding et al., 2021). LOTClass Meng et al. (2020) employ pre-trained neural language models with unlabeled data for category understanding, i.e., finding words similar to label names. They then introduce a self- training approach to the entire unlabeled corpus to generalize the model. 5 GPT-3 with descriptions Following Brown et al. (2020), we manually write the descriptions for each class and query GPT-3 where the predicted token serves as the prediction. We show the de- scriptions in Appendix A.1. KPT Hu et al. (2022) propose knowledgeable prompt-tuning, which expands the label words space using external knowledge bases (KB). KPT also refines the expanded label words based on the unlabeled data. We show the best results of KPT in the zero-shot setting. Null Prompt IV et al. (2022) insert a token at the end of the text (i.e. using the prompt template \u201c [ x ][MASK] \u201d ) and then use the prediction of the [MASK] token to perform zero-shot classification. Multi-Null prompting Wang et al. (2021) find that simply introducing a few prompt [MASK] s can improve the performance and robustness of the Null Prompt in the zero-shot settings.\n\n5.3 M AIN R ESULTS\n\n We demonstrate our experimental results in Table 4. Overall NPPrompt outperforms Null Prompt and Multi-Null Prompt remarkably by over 10 percent in a fully zero-shot setting. NPPrompt achieves an accuracy of over 85% on AG News and DBPedia and over 90% on IMDB and Amazon. We conjecture that topic classifications in AG News and DBPedia are more complicated than binary sentiment classifications in IMDB and Amazon, hence the higher accuracy on the latter. NPPrompt is only slightly worse than KPT but outperforms most baseline methods in which human efforts/external knowledge or unlabeled data are strictly required. It\u2019s worth noting that NPPrompt performs much better than ManualVerb, suggesting that the label words generated by our method are more comprehensive and unbiased than human-designed ones. Besides, NPPrompt can beat GPT-3 by 4% in terms of average accuracy, a strong sign of the great potential for RoBERTa-large with 355M parameters compared to 175B parameters giant GPT-3. To explore how our method NPPrompt performs on different kinds of tasks, we conduct experi- ments on GLUE benchmark (Wang et al., 2018). Specifically, we test on Multi-Genre Natural Lan- guage Inference Matched (MNLI), Multi-Genre Natural Language Inference Mismatched (MNLI- mm)(Williams et al., 2018) , Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016) and Recognizing Textual Entailment (RTE) (Bentivogli et al., 2009) for Natural Language Inference (NLI); Microsoft Research Paraphrase Matching (MRPC) (Dolan & Brockett, 2005) and Quora Question Pairs (QQP) (Chen et al., 2018) for Paraphrase Similarity Matching; Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) for Sentiment Classification; The Corpus of Linguistic Ac- ceptability (CoLA) (Warstadt et al., 2019) for Linguistic Acceptability. As shown in Table 5, NPPrompt outperforms all other methods in fully zero-shot setting. Auto- L (Gao et al., 2021) and AMuLaP (Wang et al., 2022) are both automatic label words searching methods utilizing few-shot examples. Our method NPPrompt can even outperform them without any unlabeled data or few-shot training examples. 6 With human designed prompts / few-shot data al. (2020) with RoBERTa-large; Auto-L: method in Gao et al. (2021); AMuLaP: method in Wang et al. (2022); Majority: majority class.\n\n5.4 E FFECTS OF SIMILARITY FUNCTIONS IN NONPARAMETRIC AGGREGATION\n\n Both weight and similarity functions play a critical role in the design of NPPrompt and we test how NPPrompt performs on AG News with different con- figurations. The \u201cDefault\u201d setting is as stated in Equation 3 and 5. We fix the similarity function set w ( v i , y j ) = 1 for the \u201cSame weight\u201d setting and w ( v i , y j ) = , for the \u201cAverage weight\u201d setting. Besides cosine similarity, the Euclidean distance and the dot product are also common similarity measures for embeddings. Con- sequently, we fix the weight w ( v i , y j ) = 1 , choose S ( emb ( v i ) , emb ( y j )) = \u2212\u2225 emb ( v i ) \u2212 emb ( y j ) \u2225 for the \u201cEuclidean distance\u201d setting and S ( emb ( v i ) , emb ( y j )) = emb ( v i ) \u00b7 emb ( y j ) for the \u201cDot product\u201d setting. It can be informed from Figure 2 that with a fixed similarity function, different weight calculations yield comparable results, but with a fixed weight, cosine similarity is the optimal similarity measure.\n\n5.5\n\n C AN WE SUM OVER PROBABILITIES ? NPPrompt sums up all logits for a label word set as shown in Equation 4. Another possible approach is to sum up the probabilities from PLM\u2019s prediction for the label words and choose the argmax for all different labels as the prediction: P ( y j | x prompt ) = P ( y j | x prompt ) We conduct experi- ments on AG News to compare the above two ap- proaches, one that sums up logits (\u201csum logit\u201d) and one that sums up probabilities (\u201csum prob\u201d). Figure 3 presents the results and we find that \u201csum logit\u201d performs better at small k but \u201csum prob\u201d delivers better results when k exceeds 30. \u201csum logit\u201d achieves the best result at k = 12 among all experi- ments. 7\n\n5.6 H OW MANY LABEL WORDS SHOULD WE CHOOSE ?\n\n The number of the label words impacts the performance of our method NPPrompt as well. In Figure 4, we display the performances of different models with varied neighborhood numbers. In general, NPPrompt attains similar test accuracy across different neighborhood numbers. Regardless of the choice for neighborhood number, NPPrompt-RoBERTa-large achieves over 80% accuracy in topic classification tasks on AG News and DBPedia, and it gains over 90% accuracy in sentiment classification tasks on IMDB and Amazon. In real-world applications, we can simply choose a fixed neighborhood number (e.g. 8-10) to achieve decent performance.\n\n5.7 H OW DOES NPP ROMPT PERFORM WITH DIFFERENT PLM S ? Method\n\n AG News DBPedia IMDB Amazon Avg. NPPrompt NPPrompt highly depends on the pre-trained language model. The label words for the categories with various PLMs are different, a result of their unique initial word embedding and vocabularies. To study the effect of applying different PLMs, we conduct extra experiments using BERT-base-cased, BERT-large-cased and RoBERTa-base models. The results are shown in Table 6. NPPrompt with RoBERTa-large generates the best performance, which may result from the fact that RoBERTa-large has the largest number of parameters and that it is pre-trained on the largest corpus. In general, larger models (RoBERTa-large/BERT-large) achieve better performances than base models (RoBERTa- base/BERT-base) as expected and RoBERTa shows better accuracy than BERT models on average.\n\n5.8 W HAT LABEL WORDS DO DIFFERENT PLM S CHOOSE ?\n\n We summarize the label words of different PLMs for S CHOOL category in DBPedia in Table 7. RoBERTa-large and RoBERTa-base share similar sets of label words yet with a minor discrepancy 8 between their similarity scores. RoBERTa-large usually produces larger similarities than RoBERTa- base. In contrast, the label words in RoBERTa are quite different from those in BERT.\n\n6 D ISCUSSION\n\n NPPrompt achieves superior results in zero-shot text classifications. We attribute good performance to two aspects. Firstly, compared to fixed words or human-designed label words, using the initial word embedding from PLMs enables us to find cognates of the label words. For example, we have { \u201c school\u201d, \u201c School\u201d, \u201c schools\u201d, \u201c SCHOOL\u201d... } for the S CHOOL category, as shown in Table 7. Secondly, we effectively elicit the potential of pre-trained language models. During the pre-trained process, language models are required to predict the masked token. The prediction of the [MASK] token of the PLM is not fixed in the inference stage, so that there is no standard correct answer to fit into the context and instead, multiple words sharing similar meanings can be predicted. Our approach reformulates the zero-shot classification problem to the masked token prediction problem which is well aligned with the pre-training process. NPPrompt points out a promising way to deal with dynamic and open zero-shot classification prob- lems where new classes can emerge or old classes should be deleted. Efficient PLMs and category names are all we need. Together with the key words design in Equation 7, NPPrompt can also work in special scenarios where label names do not have semantic meaning (e.g. category with label name { \u201cA\u201d, \u201cB\u201d, \u201cC\u201d } ). This technique can be widely deployed in real-world applications. Limitations and Future Directions For those label names without semantic meanings, several keywords are still required for NPPrompt to work well. In addition, we only focus on the zero- shot setting. There are more to explore when considering the few-shot examples in many practical applications. Besides, we only test on text classification and NLU tasks from the GLUE benchmark. Whether NPPrompt works on other tasks such as ranking or relation extraction remains an open question.\n\n7 C ONCLUSION\n\n In this paper, we propose NPPrompt, a\n\nnovel and effective method for fully zero-shot learning with pre-trained language models. We use initial word embedding of PLM to automatically find related words for category names, which enables us to construct the verbalizers without manual design or unlabeled corpus. Experimental results show that NPPrompt outperforms the previous zero-shot methods by large margins.\n\n 9\n\nR EFERENCES\n\n 10 11\n\nA A PPENDIX A.1 E XPERIMENTAL D ETAILS\n\n Table 8 shows all the manual templates of NSP-BERT. Table 9 summarizes manual designed de- scriptions of each dataset for Semantic Retrieval. As for GPT-3, we query the OpenAI API 1 and test with Davinci model. The prompts for GPT-3 are shown in Table 10. We list all templates and label names for NPPrompt of all experiments in Table 11. of this text is label 1 https://openai.com/api/ 12\n\nDescriptions\n\n The politics category is related to politics, government, and law. The sports category is related to sports, competition, and athletics. The business category is related to business, portfolio, economics, and money. The technology category is related to technology, software, system, and science.\n\n:\n\n The company category is related to company, corporation, enterprise, brand, and business. The school category is related to school, academy, university, and college. The artist category is related to artist, art, painter, musician, singer, and creative. The athlete category is related to athletes, sports, Olympic, and gym. The politics category is related to politics, government, and law. The transportation category is related to transportation, transport, vehicle, and traffic. The building category is related to buildings, construction, and structure. The mountain category is related to river, lake, bay, and mountain. The village category is related to village, town, and rural. The animal category is related to animal, wildlife, and nature. The plant category is related to plant, shrub, tree, and forest. The album category is related to album, lyrics, cd, and song. The film category is related to film, movie, cinema, and video. The book category is related to book, novel, and publication. IMDB : The bad category is related to negative and bad reviews. The good category is related to positive and good reviews. Amazon : The bad category is related to negative and bad reviews. The good category is related to positive and good reviews.\n\nPrompts for GPT-3 AG News :\n\n [Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the fol- lowing sentence into one of the four different categories. The categories are: \u201cpolitics\u201d, \u201csports\u201d, \u201cbusiness\u201d, and \u201ctechnology\u201d. Input: [x] . Output: DBPedia : [Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of the fourteen different categories. The categories are: \u201ccompany\u201d, \u201cschool\u201d, \u201cartist\u201d, \u201cathlete\u201d, \u201cpolitics\u201d, \u201ctransportation\u201d, \u201cbuilding\u201d, \u201cmountain\u201d, \u201cvillage\u201d, \u201can- imal\u201d, \u201cplant\u201d, \u201calbum\u201d, \u201cfilm\u201d, and \u201cbook\u201d. Input: [x] . Output: IMDB : [Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of the two categories. The categories are: \u201cbad\u201d and \u201cgood\u201d. Input: [x] . Output: Amazon : [Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of the two categories. The categories are: \u201cbad\u201d and \u201cgood\u201d. Input: [x] . Output: 13 is a [MASK] x All in all, it was [MASK] . x All in all, it was [MASK] . x 1 It was [MASK] . x 1 ? [MASK] , x 2 x 1 ? [MASK] , x 2 entailment: maybe ? [MASK] , x Indeed Overall Well However 1 ? [MASK] , x 2 1 [MASK] , x 2 1 [MASK] , x 2 x 1 This is [MASK] . 14", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes NPPrompt, a novel method for fully zero-shot learning with pre-trained language models (PLMs) for natural language understanding tasks.", "topic_class_1": "Summay of paper topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The key idea is to use the PLM's own word embeddings to automatically find relevant words to the target label names, without requiring any labeled data, additional unlabeled corpus, or manual prompt engineering.", "topic_class_1": "Summay of paper method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "NPPrompt searches for the top-k nearest neighbor words to each label name in the PLM's embedding space to construct the verbalizer.", "topic_class_1": "Summay of paper method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It then aggregates the PLM's prediction logits for the [MASK] token filtered by the label words to make the final prediction.", "topic_class_1": "Summay of paper method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on text classification, textual entailment, paraphrasing and other tasks demonstrate NPPrompt outperforms previous zero-shot methods by large margins, with absolute gains of 12.8% on text classification and 18.9% on GLUE.", "topic_class_1": "Summay of paper results", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- NPPrompt enables fully zero-shot learning for PLMs without relying on any labeled data, additional unlabeled corpus, or human-designed prompts/label words.", "topic_class_1": "Method Summary", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This makes it more practical and flexible for real-world applications.", "topic_class_1": "Strength of method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Using the PLM's own embedding to find label words is an effective way to elicit PLM's knowledge and adapt to new tasks/labels.", "topic_class_1": "Strength of method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The nonparametric aggregation of prompted predictions is simple yet powerful.", "topic_class_1": "Strength of method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- NPPrompt achieves superior zero-shot performance on a wide range of language understanding tasks, showing substantial improvements over previous methods.", "topic_class_1": "Strength on results", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "On some tasks it even approaches methods using extra knowledge or data.", "topic_class_1": "Strength on method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- For label names without semantic meanings, NPPrompt still requires some keywords to work well.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "How to extend it to arbitrary labels is unclear.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- NPPrompt focuses on the fully zero-shot setting without using any example data.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "No", "explanation": "This is the what the zero-shot learning should do.", "error_type": "Inexpert Statement"}, {"segment_text": "How to incorporate few-shot examples when available to further improve performance is not explored.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper only experiments with English language tasks.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "No", "explanation": "This is a high expectation from the reviewer.", "error_type": "Out-of-scope"}, {"segment_text": "Whether the technique generalizes to other languages is unknown.", "topic_class_1": NaN, "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Strenghth on writing", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed NPPrompt method is simple and intuitive.", "topic_class_1": "Strenghth on method simiplicity", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive experiments are good quality, covering a diverse set of important NLP tasks.", "topic_class_1": "Strenghth on experiments", "topic_class_2": "Quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The key novelty is enabling an effective fully zero-shot learning approach for PLMs without relying on any labeled data, unlabeled corpus or human annotations.", "topic_class_1": "Strenth on novelty", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This tackles a major limitation of previous prompting methods.", "topic_class_1": "Strength on method", "topic_class_2": "Limitation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The strong empirical results also push forward the state-of-the-art in zero-shot learning.", "topic_class_1": "Strength on result", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The method is straightforward to implement based on the descriptions.", "topic_class_1": "Strength on method", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper provides comprehensive details on experimental setups.", "topic_class_1": "Strength on writing", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Therefore the results are likely to be reproducible.", "topic_class_1": "Strength on reproducilbility", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the code is not provided with the submission, so reproducing the full results may still take some effort.", "topic_class_1": "Concern over missing code", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents NPPrompt, a novel and effective method for zero-shot learning with PLMs without using any labeled data or extra corpus.", "topic_class_1": "Summary of paper", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "NPPrompt leverages the PLM's own embedding space to find relevant label words and makes predictions via nonparametric aggregation.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments show it outperforms previous zero-shot approaches by large margins on a variety of language tasks.", "topic_class_1": "Summary of results", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The method is simple, intuitive and practical.", "topic_class_1": "Summary of method strength", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It addresses important limitations of prior prompting techniques and achieves remarkable zero-shot performance.", "topic_class_1": "Summary of contributions", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A few weaknesses exist such as reliance on label semantics and focus on English tasks.", "topic_class_1": "Summary of weaknesses", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "But overall this is a strong paper that makes good contributions to advancing PLM's zero-shot learning capabilities.", "topic_class_1": "Summary of overall contributions", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper introduces Nonparametric Prompting (NPPrompt), a novel approach for fully zero-shot language understanding with pre-trained language models (PLMs), without requiring labeled data, additional raw corpus for fine-tuning, or manual construction of prompts.", "topic_class_1": "Summay of paper topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "NPPrompt leverages PLM embeddings to find relevant words for labels and uses these to generate predictions for various NLP tasks such as text classification, text entailment, similar text retrieval, and paraphrasing.", "topic_class_1": "Summay of paper contribution", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The method outperforms existing zero-shot approaches by significant margins on standard benchmarks.", "topic_class_1": "Summery of paper results", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1.Novelty: NPPrompt introduces a novel approach for fully zero-shot learning, leveraging PLMs without additional data requirements. 2.Performance: Demonstrates superior performance over existing methods across diverse NLP tasks, indicating broad applicability. 3.Methodological Rigor: The paper provides a clear and detailed explanation of the methodology, supported by extensive experiments.", "topic_class_1": "Strength on novelty Strength on performance Strength on experiments and analysis", "topic_class_2": "Strength", "reliability": "No", "explanation": "The strengths do not talk about the non-parametric nature of prompting.", "error_type": "Neglect"}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1.Reproducibility: While the paper mentions experiments and results, detailed settings for reproducibility (e.g., hyperparameters, computational resources) could be better emphasized.", "topic_class_1": "Concern over reproducibility", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "2.Comparison with Baselines: The paper could benefit from a deeper analysis of why NPPrompt outperforms baselines, including insights into cases where it might not perform as expected.", "topic_class_1": "Concern over lack of analysis on baselines' performance", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "3.Generalization to other languages or domains: The paper primarily focuses on English; extending and evaluating the approach for other languages or specific domains could provide further insights.", "topic_class_1": "Concern over generalizability", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is clearly written and structured, making it accessible to readers.", "topic_class_1": "Strength of paper clarity", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The quality of research is high, with rigorous experimentation and a novel contribution to the field. - The novelty of the method is one of the paper's strengths, presenting a new pathway for zero-shot learning in NLP.", "topic_class_1": "Strength of experiments and contribution Strength of novelty", "topic_class_2": "Quality Novelty", "reliability": "No", "explanation": "The reviews here are very generic and lack any depth.", "error_type": "Superficial Review"}, {"segment_text": "Reproducibility could be enhanced with more detailed experimental settings and access to code or datasets.", "topic_class_1": "Concern over reproducibility", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This work is a significant contribution to the field of NLP, introducing a novel approach for zero-shot learning that leverages the inherent capabilities of PLMs without additional data requirements.", "topic_class_1": "Summary of paper contributions", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The method shows impressive performance across a range of tasks, indicating its potential impact.", "topic_class_1": "Summary of paper contributions", "topic_class_2": "Summary", "reliability": "No", "explanation": "The range of tasks is over stated in the review here.", "error_type": "Unstated statement"}, {"segment_text": "However, improvements in documenting reproducibility and extending analyses could further strengthen the paper.", "topic_class_1": "Suggestion", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes NPPrompt, a novel method for enabling pre-trained language models (PLMs) to perform fully zero-shot learning on various natural language understanding (NLU) tasks.", "topic_class_1": "Summay of paper topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Unlike existing approaches that require fine-tuning on labeled data or manual prompt construction, NPPrompt leverages the PLM's own embedding space to automatically identify words related to class labels, eliminating the need for human-designed verbalizers.", "topic_class_1": "Summay of paper contribution", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper evaluates NPPrompt on tasks like text classification, text entailment, and paraphrase identification, demonstrating significant performance improvements over previous zero-shot methods.", "topic_class_1": "Summery of paper experiments", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: NPPrompt introduces a novel approach for fully zero-shot learning with PLMs, eliminating the need for labeled data or manual prompt engineering. Effectiveness: The paper demonstrates significant performance gains over existing zero-shot methods on various NLU tasks. Generalizability: NPPrompt can be easily applied to new categories without requiring additional human intervention, making it adaptable to dynamic settings. Efficiency: NPPrompt utilizes the pre-trained knowledge within the PLM effectively, achieving competitive results even with smaller models compared to larger models like GPT-3.", "topic_class_1": "Strength on novelty Strength on effectiveness Strength on generalizability Strength on efficiency", "topic_class_2": "Strength", "reliability": "No", "explanation": "The following rows are written in a very generic way.", "error_type": "Superficial Review"}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited analysis for certain scenarios: The paper acknowledges that NPPrompt may require additional keywords for label names lacking semantic meaning.", "topic_class_1": "Weakness of limited analysis", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Focus on zero-shot setting: The study primarily focuses on the zero-shot setting, leaving the exploration of few-shot scenarios for future work.", "topic_class_1": "Weakness of experiment setting", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited task coverage: The evaluation primarily focuses on text classification and NLU tasks from the GLUE benchmark.", "topic_class_1": "Weakness of task coverage", "topic_class_2": "Weakness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further investigation is needed to assess its effectiveness on other NLP tasks like ranking or relation extraction.", "topic_class_1": "Future work suggestion", "topic_class_2": "Weakness", "reliability": "No", "explanation": "While I agree, the following tasks may be beyond scope for this study.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written and clearly presents the proposed method and experimental results.", "topic_class_1": "Strength of paper clarity", "topic_class_2": "Clarity", "reliability": "No", "explanation": "Very generic", "error_type": "Writing"}, {"segment_text": "Quality: The research is well-conducted, with comprehensive experiments and detailed analysis.", "topic_class_1": "Strength of analysis quality", "topic_class_2": "Quality", "reliability": "No", "explanation": "Very generic, can be apply to many papers", "error_type": "Superficial Review"}, {"segment_text": "Novelty: The automatic verbalizer construction using PLM embeddings is a novel contribution with significant potential for zero-shot learning.", "topic_class_1": "Strength of novelty", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: While the paper lacks code, it provides detailed descriptions of the methodology and experimental setup, facilitating potential replication by other researchers.", "topic_class_1": "Discuss of reproducibility", "topic_class_2": "Repoducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "NPPrompt presents a promising approach for fully zero-shot learning with PLMs, achieving impressive performance improvements over existing methods.", "topic_class_1": "Summary of paper contribution", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Its ability to adapt to new categories and its efficient utilization of pre-trained knowledge make it a valuable tool for various NLU applications.", "topic_class_1": "Summary of paper contribution", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, further investigation is needed to address its limitations in handling labels with limited semantic meaning and to explore its applicability to other NLP tasks and few-shot learning scenarios.", "topic_class_1": "Limitation", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Leveraging Large Language Models for Multiple Choice Question Answering", "body_text": "\n\nL EVERAGING L ARGE L ANGUAGE M ODELS FOR M ULTIPLE C HOICE Q UESTION A NSWERING\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n While large language models (LLMs) like GPT-3 have achieved impressive re- sults on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., \u201cA\u201d) associated with its chosen answer option. This approach allows the model to ex- plicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer se- lection. For the natural approach to be effective the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.\n\n1 I NTRODUCTION\n\n Current state of the art (SOTA) methods on many multiple choice question answering (MCQA) tasks involve specialized models, prompt engineering, and individualized tuning in general. What if one model could do just as well as each of these models does individually? This is part of a general vision for so-called foundation models (Bommasani et al., 2021). Foun- dation models include large pre-trained language models (LLMs) that have derived enough broad knowledge (spanning, for example, linguistic, factual, and commonsense (Liu et al., 2019; Amrami & Goldberg, 2018; Petroni et al., 2020; Bosselut et al.; Bouraoui et al.; Zuo et al., 2018; Bhagavat- ula et al., 2019)) to transfer from a simple language modelling objective to a huge array of natural language tasks. Interestingly, while LLMs have achieved SOTA results on many tasks, they generally fall short on MCQA. Why is this the case, given their general language modelling prowess as suggested by the low cross-entropy loss they attain with all their parameters, data, and compute (Kaplan et al., 2020; Henighan et al., 2020; Hernandez et al., 2021)? Should they not excel, or at least be highly competitive? In this paper, we argue that they fall short because dominant methods used with them conflate prob- abilities of sentences with probabilities of correct answers . We hypothesize that there are funda- mental problems with the near-universal approach to MCQA for LLMs, which we refer to as \u201ccloze prompting\u201d (CP). Specifically, these problems include 1) the conflation of the grammaticality, com- monality, and \u201cnaturalness\u201d of a text and its likelihood qua question-answer, 2) the computational expense of scoring multiple candidate answers, 3) the fact that the LLM cannot explicitly reason about and compare different candidate answers, and 4) finicky normalization due to tokenization schemes. The centerpiece of our paper is an extensive investigation of an alternative: we explain how these problems might be solved by what we call multiple choice prompting (MCP). In MCP, 1 the language model receives both the question and also a list of candidate answers as on a multiple choice test, with each answer associated with (or \u201cbound\u201d to) a symbol such as \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, etc. We explain how this approach might be why MCP outperforms CP in Section 3. More importantly, though, we demonstrate that when we prompt LLMs with MCP instead of CP, performance often dramatically improves \u2013 approaching or even surpassing SOTA performance. On a varied group of 20 datasets, we show that MCP outperforms CP on all but 4 of the datasets, with a mean gap of 9.7% on all tasks and a max gap of 44%. MCP surpasses old SOTA scores on 9 of 20 datasets (by as much as 15% on a single task), and averaged across all datasets, MCP scores fall 0.6% shy of SOTA. This implies that the de facto method for prompting LLMs has led them to be considerably under- estimated for MCQA, and that there exists a better general way to prompt a single LLM that scores within a percent of accuracy of all other previous SOTA scores, on average. For the 20 different datasets we consider, SOTA accuracy required 14 customized models and approaches \u2013 nearly three individualized setups for every four datasets. We argue that the fact that MCP is comparable to or surpasses SOTA, with no task-specific tuning, is evidence for the efficiency, generality, and overall promise of foundation models in MCQA. Our primary contribution is three-fold: 1) We present an argument for multiple-choice prompting over cloze prompting and formally define multiple choice symbol binding (MCSB), a required abil- ity for an LLM to benefit from MCP; 2) We show that not all LLMs are equally skilled in this regard; and 3) Across 20 diverse datasets, we show that the models most capable of MCSB can individu- ally approach or beat SOTA on most of the considered tasks when prompted with multiple choice prompting instead of the near-universal approach of cloze prompting. Code is available online. 1\n\n2 R ELATED W ORK\n\n Transformers (Vaswani et al., 2017) have revolutionized the field of NLP by allowing models to effectively absorb much larger datasets via massive scaling in parameter count and compute; these three factors are proportional to lower loss in models (Kaplan et al., 2020; Henighan et al., 2020; Hernandez et al., 2021). Parameter counts have quickly grown from 1.5B in 2018 (Radford et al., 2018) to 540B in 2022 (Chowdhery et al., 2022), and in general, larger models are tested on a more extensive suite of tasks to demonstrate their capacity for transfer. This invariably includes multiple choice question answering tasks, and nearly every LLM we know of uses cloze prompting for these tasks (Brown et al., 2020; Du et al., 2022; Smith et al., 2022; Chowdhery et al., 2022; Lieber et al., 2021). It was, in part, these massive language models that prompted the coining of the phrase \u201cfoundation models\u201d (Bommasani et al., 2021). This is a family of large models that are heavily trained on massive datasets in a self-supervised fashion. They derive general knowledge about a modality and can transfer with impressive sample efficiency to a great number of downstream tasks. A key part of the vision of these models is that they can be repurposed, avoiding the energy, storage, and human capital costs associated with ad hoc models. Our work supports this vision of LLMs as one such foundation model (if prompted appropriately), in contrast to previous views that they are unsuitable for large sets of MCQA tasks. We are not the first to use the MCP approach. Rae et al. (2021) use this approach on just two out of their massive suite of tasks, namely TruthfulQA (Lin et al., 2021b) and RACE (Lai et al., 2017). Kadavath et al. (2022) use MCP while investigating to what extent language models are well-calibrated, and to what degree they \u201cknow what they know.\u201d In this work we explore what makes a model compatible with MCP and how prompting with MCP compares to prompting with CP in terms of LLM accuracy. We are also not the first to notice that the method of prompting matters for LLM performance. A whole subfield of prompt engineering exists, with papers suggesting methods for ranking prompts based on mutual information (Sorensen et al., 2022), showing that LLMs suffer from majority label 1 URL removed for anonymous submission, but code is attached 2 and recency biases (Zhao et al., 2021), and arguing that most few-shot learning is not, in fact, few- shot (Perez et al., 2021). Given that CP has, up to this point, been the de facto prompting method for LLMs on MCQA tasks, several works have endeavored to improve it with normalization schemes that outperform those used in Brown et al. (2020). These include Contextual Calibration (Zhao et al., 2021) and Domain Conditional PMI (Holtzman et al., 2021). While in this work we consider methods for effectively leveraging LLMs for MCQA tasks, past work has successfully used many types of models for these tasks. A wide selection of these models are referenced in Table 2. Two notable examples of models that perform well across question answering datasets are UnifiedQA (Khashabi et al., 2020) and UNICORN Lourie et al. (2021).\n\n3 C LOZE VS . M ULTIPLE C HOICE P ROMPTING\n\n In this section, we specify what is meant by cloze prompting and multiple choice prompting, and enumerate the problems that are inherent to the former and ameliorated by the latter. In cloze prompting, a question is passed to an LLM, the candidate answers are each independently scored by the model, and the answer option selected by the model is chosen to be the one assigned the highest probability. Recognizing that probabilities of answers might be skewed by especially common or uncommon tokens or sequences of varying length, Brown et al. (2020) made use of two different normalization procedures. In one, the sequence\u2019s probability is normalized for length by taking the n th root, or P ( x 1 , x 2 , ..., x n ) = n (cid:112)(cid:81) ni =1 P ( x i ) . In the other, the answer\u2019s probabil- ity is normalized by the unconditional probability of the answer, or P ( completion | context ) P ( completion | answer context ) where answer context is the string \u201c Answer: \u201d. In the remainder of the the paper, we refer to length normalization as LN, unconditional normalization as UN, and neither as Raw. See Figure 1 for a visualization on how these strategies work. In MCP, on the other hand, a question and its symbol-enumerated candidate answers are all passed to an LLM as a single prompt. The prompt is structured such that the LLM must only predict a single token (such as \u201cA\u201d, \u201cB\u201d, etc.). The answer choice associated with the token assigned the highest probability by the model is chosen to be the model\u2019s answer. The probabilities of these 3 symbols therefore serve as a proxy for each answer\u2019s probability. There are several problems with cloze prompting that do not apply to multiple choice prompting: Conflation of likelihood as answer and likelihood as natural language One outstanding prob- lem with CP is that the likelihood of the answer\u2019s text could be conflated with the likelihood of the text as an answer. For example, if asked which quote is from Shakespeare\u2019s Mac- beth , the phrase Give sorrow words; the grief that does not speak knits up o-er wrought heart and bids it break. might be less common or grammati- cal than other sentences, which could artificially deflate its score. MCP does not face this problem because there is no grammar to alter the score. Reliance on normalization procedures With CP, use of special normalization strategies are typ- ically essential for achieving high performance. These often incur a computational cost or depend on choice of tokenization scheme. With MCP, there is no need for any normalization strategy. No direct comparison between answers In CP, candidate answers are not compared to each other except implicitly through their final probabilistic scores. MCP gives LLMs the ability to explicitly compare and contrast different answer options. This also makes LLM+MCP more comparable to SOTA methods that typically have all answer options presented at once. Expense Lastly, cloze prompting is computationally expensive. For a question with n possible answer choices CP requires n forward passes through the LLM for the raw or LN normalization strategies, and 2 n forward passes for the UN strategy. MCP only requires a single pass (which is independently slightly cheaper than CP forward passes because the model only needs to generate a single output token).\n\n4 T HE C HALLENGE OF M ULTIPLE C HOICE S YMBOL B INDING\n\n When presenting a multiple choice question, the candidate answers must be enumerated in some order. Ideally, humans\u2019 answers to such questions are order-invariant. If an LLM exhibits the same characteristic, we say that it is capable of multiple choice symbol binding (MCSB). Interestingly, LLMs vary substantially in terms of this ability. Consider the multiple choice prompt example from Figure 1. Given the answer order \u201cPizza\u201d, \u201cLollipop\u201d, \u201cFrench beans\u201d (as shown in the figure) GPT-3 (Davinci) Brown et al. (2020) assigns the highest probability to the token \u201cA,\u201d which is associated with pizza. However, if we change the ordering to \u201cFrench beans\u201d, \u201cLollipops\u201d, \u201cPizza\u201d, GPT-3 surprisingly still assigns the highest probability to \u201cA,\u201d which is now associated with French beans. Simply changing the order of the candidate answers changes the model\u2019s answer. How can we compare the relative symbol binding ability of two models? One way is to measure what we term Proportion of Plurality Agreement (PPA). Given a question with n answer options, there are n ! different ways these options can be associated with an ordered, fixed set of symbols. To measure PPA for a given model and question we present that question to the model with each different ordering and for each ordering record the answer assigned the highest probability by the model. PPA for that question is the proportion of orderings that chose the plurality answer among all orderings. For a dataset PPA is averaged over all questions. Importantly, PPA measures order invariance irrespective of model ability to perform a task. If a model performs poorly on a task but answers consistently across possible orders of answer options it will still have a high PPA. For a dataset where each question has n answer choices, the baseline PPA is 1 /n . Using PPA, we compare several popular LLMs\u2019 MCSB ability. The first is GPT-3 (Brown et al., 2020) (Davinci). We also consider Codex (Chen et al., 2021) (Davinci) and Instruct (Ouyang et al., 2022) (Curie and Davinci). These two models were trained, respectively, for code modelling and instruction following, both having been initialized from GPT-3 weights. We also evaluate the MCSB ability of GPT-2 Radford et al. (2019), CodeParrot (Tunstall et al., 2022) (GPT-2 with further train- ing on code), and Jurassic-1 (Lieber et al., 2021) (Jumbo). The parameter count of these models 4 spans from 1.5B (GPT-2) to 178B (Jurassic-1 Jumbo). We use API requests for GPT-3, Codex, In- struct, and Jurassic-1. For GPT-2 and CodeParrot we use the model checkpoints from Hugging Face Transformers (Wolf et al., 2020). We evaluate on the OpenBookQA dataset (Mihaylov et al., 2018), a multiple choice QA dataset composed of 500 science questions. We randomly sample 100 instances from the dataset to reduce computational cost. Each question has four answer options, meaning that the PPA random baseline is 25%. We choose to use OpenBookQA because of its relatively small size and because it is a traditional multiple choice dataset, with questions modeled after human multiple choice exams. The results of our evaluation can be found in Figure 2. The most immediately interesting result is that Codex (Davinci) and Instruct (Davinci) significantly outperform the other models in terms of PPA, showing remarkable invariance to answer ordering. GPT-3 seems to perform about half as well, interestingly outperforming the larger Jurassic-1 (Jumbo) model. GPT-2, CodeParrot, and Instruct (Curie) all have PPA scores close to the 25% baseline. Another apparent trend is that providing exemplars to a model increases PPA consistently across models. This is especially notable for GPT-3. While we do not explore what about these models causes high MCSB ability, it appears that model size could be an important factor because Instruct Davinci has a much higher PPA than Instruct Curie. This hypothesis is in line with the work of Wei et al. (2022). It also seems like further training (on source code or with reinforcement learning based on hu- man preferences) is essential (Codex and Instruct both outperform GPT-3 substantially even though they are basically fine-tuned versions of GPT-3). Intuitively, it makes sense that training on source code increases symbol binding ability because performing well on a next token prediction task for code requires grasping symbol binding as it is used in e.g., dictionaries and list indexing. Training on source code alone does not seem sufficient in and of itself, though, because CodeParrot didn\u2019t achieve notably better PPA than did GPT-2. Given the strong multiple choice symbol binding ability of Codex and Instruct, a natural next ques- tion is whether using these models with MCPs results in higher accuracy. We address this question in the next section.\n\n5 E XPERIMENTAL S ETUP\n\n We evaluate the performance of a model with strong MCSB ability and multiple choice prompts across a set of 20 diverse datasets. In this section we discuss how that model was chosen (Section 5.1), and which datasets we use for evaluation (Section 5.2). We also address the possibility of dataset leakage into model training data (Section 5.3) and explain our prompt engineering choices (Section 5.4). 5\n\n5.1 M ODELS\n\n In Section 4 we observed that models like Codex (Davinci) (Chen et al., 2021) and Instruct (Davinci) (Ouyang et al., 2022) demonstrated much stronger MCSB ability than did GPT-3. In this section we explore whether higher MCSB ability leads to higher multiple choice task accuracy. We evaluate model performance on a commonsense reasoning dataset (OpenBookQA (Mihaylov et al., 2018)), a cloze/completion dataset (StoryCloze (Mostafazadeh et al., 2016)), and a reading comprehension dataset (RACE-m Lai et al. (2017)). See Section 5.2 for more information on these datasets. We randomly sample 500 instances from StoryCloze and RACE-m each to reduce computational costs. The results of our comparison can be found in Table 1. We note that whereas choosing answer options based on cloze prompts (with max raw probability (Raw), max raw probability after length normalization (LN), or max raw probability after unconditional normalization (UN)) performs best for GPT-3, MCP always performs best for Instruct and Codex, and it does so by a sizeable margin. Because these models have high MCSB ability they are able to effectively leverage MCP prompts to achieve higher accuracy. Instruct and Codex both outperform GPT-3 by large margins across all tasks. For all of our further experiments we choose to use Codex (Davinci) because its training setup seems to explicitly promote MCSB ability, it had the highest PPA value of all models we tested, and it was the most computationally cheap (since it is currently in free beta). Additionally, Codex should not add any dataset leakage issues that were not already present in GPT-3 since it was exclusively fine-tuned on Python files.\n\n5.2 D ATASETS\n\n We compare multiple choice prompts and cloze prompts across a diverse array of datasets. Examples of questions from each of these datasets can be found in Appendix A. Common sense reasoning ARC (Clark et al., 2018) consists of grade-school science questions. Its challenge set is filtered down to questions not correctly answered by simple retrieval and co- occurence solvers. CODAH (Chen et al., 2019) questions were adversarially generated to fool BERT-base Devlin et al. (2019) fine-tuned on SWAG (Zellers et al., 2018). CommonsenseQA (Tal- mor et al., 2019) questions are based on ConceptNet (Speer et al., 2017) concepts, allowing for diversity and incorporation of world knowledge into questions. COPA (Roemmele et al., 2011) is a causal reasoning dataset that tasks models with determining the cause or effect of a premise. Composed of questions about metaphors, Fig-QA (Liu et al., 2022a) is designed to test nonliteral reasoning abilities. MedMCQA (Pal et al., 2022) has questions from many areas of the medical domain drawn from medical exams (see Appendix F for a full list of subjects covered). The Mas- sive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021) consists of diverse tasks from STEM, the humanities, and the social sciences (a full list of tasks can be found in Appendix G). OpenBookQA (Mihaylov et al., 2018) endeavors to test world knowledge and reason- ing using science questions. PIQA (Bisk et al., 2020) questions focus on the area of commonsense reasoning related to understanding of physical interactions with the world. RiddleSense (Lin et al., 2021a) and Social IQa (Sap et al., 2019) are designed to test for model ability to reason about riddles and social situations respectively. 6 Natural language inference ANLI (Nie et al., 2020) is a dataset of adversarially generated NLI questions. These questions were generated by annotators in three rounds, with annotators seeking to generate questions challenging for increasingly capable models trained for NLI. Cloze and completion tasks HellaSwag (Zellers et al., 2019) tasks models with predicting the best continuation for a video caption or WikiHow article. StoryCloze (Mostafazadeh et al., 2016) is also a continuation prediction task, but with short, four-sentence stories. Text classification AG News (Zhang et al., 2015) is a news classification task. Winograd-style tasks Winogrande (Sakaguchi et al., 2021) is a set of winograd schema questions that was carefully crowdsourced and then filtered with a bias mitigation algorithm. Because we are doing evaluation without fine-tuning in this work we evaluate in the smallest training data setting (XS), but also include results in the largest setting (XL) to facilitate comparison with prior work. Reading comprehension Cosmos QA (Huang et al., 2019) is a commonsense reasoning based reading comprehension task designed to test model ability to \u201cread between the lines.\u201d The questions in the DREAM (Sun et al., 2019) dataset are based on dialogues. LogiQA (Liu et al., 2020) tasks models with answering questions from a human exam that tests critical thinking. RACE (Lai et al., 2017) is a widely used reading comprehension dataset with questions taken from middle and high school English exams for Chinese students.\n\n5.3 A DDRESSING D ATASET L EAKAGE\n\n A concern in all work with LLMs is that an LLM being used for evaluation may have been exposed to the contents of an evaluation dataset while it was being pre-trained. We only evaluate on Codex, which was initialized from GPT-3\u2019s weights and fine-tuned exclusively on Python files. This means that the risks of dataset leakage we face are effectively the same of those faced by the authors of the GPT-3 paper (Brown et al., 2020). The dataset leakage risk of the 9 of our datasets GPT-3 was tested on in the GPT-3 paper was evaluated extensively by the authors of that paper. The risk of leakage remains for the other 11 datasets. Although we weren\u2019t able to acquire the GPT-3 training data to perform a manual collision check, there are several reasons why we suspect dataset leakage is not meaningfully impacting our results. First, we note (anectdotally) that Codex+MCP errors in evaluation occur disproportionately on ques- tions that are ill-defined (not solvable by humans, having multiple answers, etc.) (see, for example, non-cherry-picked examples in Appendix D). If there were training set leakage, this set would be more independent of being well-formed than it is; that is, failures by the model seem to be due more to the deck being stacked against good reasoning than to the model having failed to memorize the data. Second, shuffling the order of the symbol-enumerated candidate answers doesn\u2019t systemati- cally harm performance (see Appendix C). Third, CP+MCP performance on publicly available test sets and private test sets is generally comparable. The strongest argument that MCP\u2019s performance is not determined by training set leakage is CP\u2019s meaningfully inferior performance for the same model; if the model had memorized the training data, it would have ascribed more probability to the correct answer regardless of the prompting method.\n\n5.4 P ROMPT E NGINEERING\n\n In an effort to keep the comparison of cloze prompts and multiple choice prompts as simple as possible we systematically avoid prompt engineering. Unlike Brown et al. (2020) we do not tune K , the number of few-shot exemplars, based on a development set, nor do we develop highly task specific prompt phrasings. K is always chosen to be as high as possible while respecting Codex\u2019s 4,000 token context limit. Our prompt phrasing is always consistent: We prefix the raw question with Question: , list answer options with associated letters (like A. Lollipop ), and finish prompts with Answer: . We mea- 7 Dataset N Few Server SOTA CP CP MCP AG News 4 38 68.2 83 77 6 87. 90.1 89 4 95.6 a ANLI R1 3 27 45.3 33.2 35 6 61 7 58.4 64.2 75.5 b ANLI 3 26 39 33 6 35.7 53.0 51.8 55.2 58.6 c ANLI 3 26 37 8 34 35 47 8 54 2 54.5 53.4 c ARC (Challenge) 4 50 58.9 81.7 64.1 82.8 66.6 86.1 86.5 d ARC (Easy) 4 57 84.2 93.1 85.9 93.5 87.8 94.7 94.8 d CODAH 4 63 56.8 76.0 65.4 87.8 73.6 91.9 84.3 e CommonsenseQA 5 79 68.5 72.0 73.1 78.9 78.6 83.2 76.6 79.1 f COPA 2 113 92.0 89.0 95.0 99.0 96.0 100.0 \u2014 99.2 d Cosmos QA 4 24 43.0 75.5 44.0 81.8 38.1 82.4 83.5 91.8 g DREAM 3 7 72.7 91.3 82.5 93.3 84.3 94.1 92.6 h Fig-QA 2 99 79.6 84.7 82.4 86.7 82.5 94.0 93.1 90.3 i HellaSwag 4 16 \u2014 71.0 \u2014 75.1 \u2014 73.6 \u2014 93.9 g LogiQA 4 16 36.6 44.5 37.5 45.3 37.8 47.3 42.5 j MedMCQA 4 58 37.8 52.1 42.1 53.9 41.2 54.4 58.0 41.0 k MMLU 4 5 49.5 62.1 \u2014 68.2 \u2014 69.5 67.5 l OpenBookQA 4 83 63.2 72.0 64.0 81.6 71.2 87.0 87.2 f PIQA 2 35 83.7 73.7 84.1 81.8 86.1 84.5 \u2014 90.1 g RACE-h 4 4 52.3 82.1 53.2 85.1 55.2 86.2 89.8 m RACE-m 4 8 67.5 85.4 70.5 89.3 71.7 90.3 92.8 m RiddleSense 5 59 79.8 67.6 89.1 77.1 91.3 83.9 80.0 68.8 f Social IQa 3 72 52.1 64.4 58.1 72.2 62.4 74.9 76.0 83.2 g StoryCloze 2 44 80.3 97.5 83.4 98.3 88.2 98.5 89.0 n Winogrande (XL) 2 102 62.5 64.5 71.6 71.6 75.5 72.1 72.3 91.3 g Winogrande (XS) 2 102 63.0 64.8 71.0 71.3 76.2 73.6 73.8 79.2 g sure model probability for an answer via the probability of the symbol associated with it. When a passage is included as part of a question (as in reading comprehension) we insert the passage before the question and prefix it with Passage: ( Story: for StoryCloze (Mostafazadeh et al., 2016) and Dialogue: for DREAM (Sun et al., 2019)). See Appendix A for examples. Our prompts are simple and modelled after those in Brown et al. (2020). Our goal in this work is to provide a fair comparison between cloze prompts and multiple choice prompts, and not to maximize accuracy by careful prompt engineering. However, we can say anec- dotally that multiple choice prompts seem very robust to different wordings and symbol choices. Prompt engineering for MPCs could be interesting future work.\n\n6 R ESULTS\n\n The results of our experiments can be found in Table 2. In the table the CP columns represent cloze prompting with the best possible strategy on the test set. That is, for each dataset and cloze prompts we calculated test accuracy based on raw accuracy, raw accuracy with length normalization, and raw accuracy with unconditional normalization. The CP column contains the highest accuracy of 8 any strategy . This is an unrealistically strong baseline, since no single normalization scheme is universally optimal. The results of each individual scheme on all datasets can be found in Table 4. The results in Table 2 validate our hypothesis that for a model with high MCSB ability (Codex) using MCP outperforms CP. This holds consistently across datasets and number of exemplars. MCP increases accuracy over CP by 8.3, 12.2, and 9.7 percentage points on average in the zero, one, and few-shot settings, respectively. The dataset with the largest gain between the cloze prompts and multiple choice prompts is Cosmos QA. For this dataset using MCP instead of CP increases accuracy by 32.5, 37.8, and 44.3 pecentage points in the zero, one, and few-shot settings, respectively. This substantial improvement on task performance is likely due to the fact that Cosmos QA questions (including their answer options) have somewhat irregular spacing 2 . This poses no issue for MCPs, but is a serious issue for CPs that rely on the linguistic representation of answer options. There are four datasets where CP outperforms MCP - AG News, PIQA, RiddleSense, and Wino- grande. One thing in common between AG News, Winogrande, and RiddleSense is they tend to have short, often one word answers. For these datasets CP is acting more like MCP because an- swer option length and wording have less impact. Some PIQA questions have answer options much longer than normal ones, perhaps making MCSB more challenging. Additionally, PIQA questions sometimes appear very \u201ccloze-friendly\u201d (see example prompt in Appendix A). In addition to consistently outperforming Codex+CP, Codex+MCP sets a new state of the art for 9 datasets. For MedMCQA Codex+MCP has an accuracy 13.4% above the old SOTA model, Pubmed- BERT (Gu et al., 2020). This seems to suggest that, in contrast to prior work (Moradi et al. (2021)), large language models may have high potential in the biomedical domain. The key is prompting them in a way that effectively aligns them to the task.\n\n7 C ONCLUSION\n\n In this work we have argued for multiple choice prompting over the universally-practiced cloze prompting, formalized the idea of multiple choice symbol binding (MCSB), showed that large lan- guage models vary greatly in their MCSB ability, and demonstrated that for a model with high MCSB ability like OpenAI Codex (Chen et al., 2021) multiple choice prompts generally elicit much more accurate responses than do cloze prompts. This approach led to a new state-of-the-art on 9 popular datasets, and on average scored within a percentage point of previous SOTA, all using a single model and single prompting approach. This demonstrates the power of LLMs as foundation models and is a strong case study in how these models might be used broadly in the future. The future for symbol-binding is exciting, with potential to teach LLMs new concepts and symbols representing them and have language models fold these new frameworks and ideas into their already- rich world models. There is also a concrete lesson to be drawn from seeing such drastic improvement in LLM perfor- mance with such a simple change to prompting: an intuitive, sensible change to the way we train and use our models can quickly unlock their potential in ways that we have previously been blind to as a field. This should fill us with motivation to seek out such improvements. Promising directions for future work include prompt engineering for multiple choice prompts; eval- uating prompts on more datasets, tasks, and models; and assessing what factors are responsible for high MCSB ability. Our results suggest that the performance of large language models on multiple choice question answering tasks has been previously underestimated, and we hope that increased adoption of multiple choice prompts will lead to more effective probing and utilization of large language models. Reproducibility Statement Source code for replicating all experiment results can be found at [URL removed for anonymous submission, but code is attached]. Names of all model checkpoints and API endpoints used can be found in constants.py . The only randomness in our experiments comes from the selection of few-shot exemplars, strong shuffling, and downsampling datasets. To ensure reproducibility, all of these use a single random seed that can be found in constants.py . 2 https://huggingface.co/datasets/cosmos_qa 9\n\nR EFERENCES\n\n https://aclanthology.org/W19-2008 . Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas 10 11 12 13 / .ch/books?id= 7hhyzgEACAAJ 14\n\nA P ROMPTS U SED F OR E ACH D ATASET\n\n In this section we provide examples of the prompts used for each dataset. Any typos in questions are present in the datasets they are drawn from. The examples are multiple choice prompts (MCPs) 15 Article: At Disney, Mending Fences or Moving On? Without Michael D. Eisner at the helm of the Walt Disney Company, will Harvey Weinstein and Steven P. Jobs stay as partners? Question: What is the best classification for this article? Answer: Premise: press release: Did you know that Marquette University owns the original manuscripts for J. R. R. Tolkien\u2019s The Hobbit and The Lord of the Rings? William Fliss, Archivist in Marquette\u2019s Department of Special Collections and University Archives, will share the remarkable tale of how these literary treasures came to Wisconsin, and he will explain what these manuscripts can tell us about one of the most iconic authors of the twentieth century. Cost: Suggested donation of $3/person Hypothesis: Attendees will pay $3. A. Hypothesis is definitely true given premise B. Hypothesis might be true given premise C. Hypothesis is definitely not true given premise Answer: Question: What adaptation is necessary in intertidal ecosystems but not in reef ecosystems? A. the ability to live in salt water B. the ability to use oxygen in respiration C. the ability to cope with daily dry periods D. the ability to blend into the surroundings Answer: 16 Question : Two friends are looking at cakes in a bakery. They A. start throwing water at each other. B. pay the bartender for the cake and leave the pub. C. run a marathon. D. order a cheesecake. Question: How does a bishop move from one place to another? Question: The pond froze over for the winter so . People on the pond Passage: Today I went to the new Trader Joe \u2019s on Court Street . It is so pretty . It \u2019s inside what appears to be an old bank . It was spacious and there were no NYU students wearing velour sweatpants . Question: What was the narrator very impressed with ? A. None of the above choices . B. The grocery store . C. The NYU campus . D. The bank workers . Dialogue: M: I want to send this package by first-class mail. W: Do you want it insured? M: Yes, for 50 dollars, please. I\u2019d also like some stamps--a book of 22 and three airmail. W: You\u2019ll have to get those at the stamp window over there, next to general delivery. M: Can I get money orders there, too? W: No, that\u2019s to the left, three windows down the hall. Question: Where can the man get money orders? A. At the stamp window. B. Next to general delivery. C. Three windows down the hall 17 Question: The chihuahua believes it is a wolf, meaning A. The small dog thinks it is undefeatable B. The small dog always stays on your lap Answer: Passage: [header] How to get around london easily [title] Know how you\u2019re going to travel. [step] The easiest method of travel in london is the tube. For this, it is easiest to buy what is called an\u2019 oyster card\u2019 or a get a travelcard for all zones from one of the automated machines in a tube station. Question: Which choice best continues the passage? A. People take an oyster card (this is a permanent, digital card) for optimal services and there are a number of reputable card companies that buy oyster cards. [title] Firstly, when considering destination, are you travelling with a package? [step] Do you want to surprise your friends and family at london. B. These cover buses, tubes, trams and overground trains throughout the city. This is usually the best option , especially for tourists, as you can travel as much as you\u2019d like in one day with one flat fare. C. [title] Know the locations of the railway stations you are going to. [step] Look for normal bus lines around london. D. The card lets you ride on the tube without the added cost of any rail, bus, or train stops. You can also travel by car (train makes easier to return for rides in london if you\u2019re travelling as non-railway cars), train from the station, or post office. Answer: Passage: (Kayaking) Man is kayaking in a calm river. Man is standing in te seasore talking to the camera and showing the kayak . Question : Which choice best continues the passage? A. man is getting in the sea and sits in a kayak. B. man is kayaking in rafts and going through mountains. C. man is kayaking on a snowy river. D. man is returning in a river with a land trail and a shop. Answer: Question: Statistics show that train accidents in a country mostly occur in the southern region, so it is safer to travel by train in the northern region. Which of the following can best refute the above argument? A. Slower train speeds in the north of the country B. There are many more train lines in the south of the country than in the north C. Many lines in the south of the country already use EMUs D. Most of the northern part of the country is mountainous and is more suitable for car driving Answer: 18 Question: Keratin in skin is softer than keratin in nail because keratin in skin has - . Less number of disulphide . Less number of salt bridges . High sodium . Different affinity for water Answer: Question: A state has recently enacted a statute prohibiting the disposal of any nuclear wastes within the state. This law does not contravene or conflict with any federal statutes. A man operates a company in the state that is engaged in the disposal of nuclear wastes. Subsequent to the passage of the state statute, the man, not yet aware of the new law, entered into contracts with many out-of-state firms to dispose of their nuclear wastes in the state. On account of this new law, however, the man will be unable to perform these contracts. Assume that the man has standing to challenge this state law. Which of the following presents his strongest constitutional grounds to challenge the state law prohibitin the disposal of nuclear wastes within the state? A. The commerce clause. B. The equal protection clause of the Fourteenth Amendment. C. The privileges and immunities clause of Article IV, Section 2. D. The contract clause. Question: Greenhouses are great for plants Question: To clear snot out of your nose, A. place a tissue over your nose and blow the snot out. B. place a tissue over your nose and suck the snot in. 19 Passage: Food is very important.Everyone needs to eat well if he wants to have a strong body.Our minds also need a kind of food.This kind of food is knowledge . When we are very young,we start getting knowledge.Kids like watching and listening.Color pictures especially interest them.When kids are older, they enjoy reading.When something interests them,they love to ask questions.Ourminds,like our bodies,always need the best food.Studying on our own brings the most knowledge. If someone is always telling us answers,we never learn well.When we study correctly and get knowledge on our own,we learn more and understand better. Question: We start getting knowledge _ . when we are old . when we are very young C. when we are pupils D. when we are parents Answer: Question: This is an ancient suit that is not worn with a tie . shirt Question: Cameron returned home with a bag of candy to eat all night long. What will Others want to do next? A. . buy the candy to eat Story: Jon loved the night sky. He would spend many of his nights looking at the stars. His mom saw that he loved the night sky. His mom bought him a telescope. Question: Which sentence best completes the story? A. Jon then watched germs with his microscope. B. Jon used his telescope often. Answer: 20 Question: So _ plays video games because Leslie has a lot of free time while Nelson has to work all the time. :\n\nB C OMPUTATIONAL C ONSTRAINTS\n\n While the OpenAI Codex Beta 3 being free enabled the high volume of experiments we performed, we were limited by its maximum 20 API requests per minute limit (which we werent\u2019t able to hit in practice). Computing just the zero-shot CP value for MMLU (Hendrycks et al., 2021) in Table 2 took over a week.\n\nC R ESULTS U NDER S TRONG S HUFFLE OF A NSWER O PTIONS\n\n 3 https://openai.com/blog/openai-codex/ 21 Dataset N K Zero-Shot One-Shot Few-Shot No Shuffle No Shuffle No Shuffle AG News 4 38 83.5 \u2014 87.1 \u2014 89.4 \u2014 ANLI R1 3 27 33.2 38.2 61.7 54.0 64.2 66.2 ANLI R2 3 26 33.6 34.8 53.0 48.1 55.2 57.1 ANLI R3 3 26 34.3 35.4 47.8 50.1 54.5 56.3 ARC (Challenge) 4 50 81.7 82.2 82.8 83.0 86.1 85.6 ARC (Easy) 4 57 93.1 92.8 93.5 93.4 94.7 94.4 CODAH 4 63 76.0 75.9 87.8 87.5 91.9 92.5 CommonsenseQA 5 79 72.0 71.2 78.9 78.0 83.2 82.9 COPA 2 113 89.0 86.0 99.0 96.0 100.0 99.0 Cosmos QA 4 24 75.5 76.3 81.8 82.8 82.4 83.7 DREAM 3 7 91.3 91.8 93.3 94.1 94.1 94.0 Fig-QA 2 99 84.7 81.8 86.7 86.0 94.0 92.5 HellaSwag 4 16 71.0 70.6 75.1 \u2014 73.6 \u2014 LogiQA 4 16 44.5 39.3 45.3 42.9 47.3 41.6 MedMCQA 4 58 52.1 49.4 53.9 52.5 54.4\n\nD N ON -C HERRY -P ICKED M ISSED Q UESTIONS FROM C OMMONSENSE QA\n\n 22 Question: James was looking for a good place to buy farmland. Where might he look? A. midwest B. countryside C. estate D. farming areas E. illinois Answer: Question: What would vinyl be an odd thing to replace? A. pants B. record albums C. record store D. cheese E. wallpaper Answer : Question: Aside from water and nourishment what does your dog need? A. bone B. charm C. petted D. lots of attention E. walked Answer: Question: Though the thin film seemed fragile, for it\u2019s intended purpose it was actually nearly what? A. indestructible B. durable C. undestroyable D. indestructible E. unbreakable Answer : Question: What is someone who isn\u2019t clever, bright, or competent called? A. clumsy ineffectual Answer: Question: Blue read material outside of his comfort zone because he wanted to gain what? new perspective entertained understanding Answer Question: What must someone do before they shop? 23\n\nE T RADITIONAL P ROMPT P ERFORMANCE BY N ORMALIZATION M ETHOD\n\n 24 ARC Challenge) Easy) COPA 96 Cosmos QA 24 38 DREAM 7 72 71 81.5 82.5 80.0 84.2 84.3 82.0 Fig-QA 2 99 73.2 74.0 79.6 76.8 79.5 82.4 79.0 81.8 82.5 HellaSwag 4 16 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 LogiQA 4 16 25.5 30.0 36.6 26.1 29.5 37.5 25.7 30.9 37.8 MedMCQA 4 58 34.1 37.6 37.8 37.8 41.1 42.1 38.0 41.2 41.2 MMLU 4 5 46.1 48.9 49.5 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 OpenBookQA 4 83 36.0 47.0 63.2 41.8 51.0 64.0 46.4 57.0 71.2 PIQA 2 35 82.7 83.7 68.6 83.2 84.1 67.4 84.5 86.1 70.7 RACE-h 4 4 48.5 52.3 49.3 49.5 53.2 52.3 51.3 55.2 53.0 RACE-m 4 8 64.2 67.5 63.3 67.3 70.5 66.0 68.9 71.7 67.5 RiddleSense 5 59 79.8 68.7 77.4 89.1 84.4 86.3 91.3 86.4 88.2 Social IQa 3 72 51.1 50.7 52.1 53.7 58.1 55.5 59.1 62.4 58.2 StoryCloze 2 44 76.3 80.3 79.5 80.1 83.4 82.\n\nF M ED MCQA T EST P ERFORMANCE BY S UBJECT\n\n 25\n\nG MMLU P ERFORMANCE BY T ASK\n\n 26 Subject Zero-Shot One-Shot Five-Shot abstract algebra 29.0 29.0 31.0 anatomy 62.2 59.3 65.9 astronomy 73.0 79.6 81.6 business ethics 69.0 72.0 71.0 clinical knowledge 70.2 70.9 71.7 college biology 75.0 78.5 81.2 college chemistry 50.0 45.0 42.0 college computer science 53.0 54.0 57.0 college mathematics 35.0 39.0 37.0 college medicine 65.3 71.7 72.3 college physics 42.2 46.1 45.1 computer security 75.0 75.0 79.0 conceptual physics 63.8 65.1 66.0 econometrics 44.7 50.0 50.9 electrical engineering 55.9 64.8 69.7 elementary mathematics 48.9 53.4 54.8 formal logic 27.8 49.2 54.0 global facts 34.0 48.0 45.0 high school biology 80.0 81.3 85.2 high school chemistry 53.2 56.2 55.2 high school computer science 79.0 77.0 80.0 high school european history 80.0 85.5 86.7 high school geography 79.8 84.8 84.3 high school government and politics 89.1 92.2 93.8 high school macroeconomics 66.7 73.1 71.5 high school mathematics 37.4 40.7 40.4 high school microeconomics 69.3 70.6 74.4 high school physics 39.1 44.4 41.7 high school psychology 85.9 87.3 89.9 high school statistics 54.2 59.7 61.6 high school us history 83.8 83.3 87.7 high school world history 81.0 84.4 86.1 human aging 74.0 77.1 74.0 human sexuality 80.9 81.7 80.9 international law 76.9 82.6 85.1 jurisprudence 80.6 81.5 85.2 logical fallacies 81.0 74.2 79.1 machine learning 45.5 50.0 54.5 management 74.8 83.5 86.4 marketing 61.5 87.2 89.7 medical genetics 72.0 69.0 74.0 miscellaneous 85.8 85.4 87.7 moral disputes 13.6 75.1 80.1 moral scenarios 24.7 47.5 46.3 nutrition 72.2 72.2 76.5 philosophy 74.3 74.6 75.9 prehistory 74.1 78.7 81.2 professional accounting 51.8 50.7 49.3 professional law 52.2 53.9 54.8 professional medicine 73.5 73.9 72.1 professional psychology 69.9 74.2 74.5 public relations 66.4 70.0 73.6 security studies 67.3 73.5 75.5 sociology 80.6 86.6 87.6 us foreign policy 87.0 86.0 87.0 virology 50.0 51.8 54.8 world religions 52.0 84.2 85.4 27", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper focuses on using large language models (LLMs) like GPT-3 and Codex for multiple choice question answering (MCQA) tasks.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors argue that the traditional \"cloze prompting\" (CP) approach used with LLMs for MCQA is sub-optimal and leads to underestimation of LLM capabilities.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of claim", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They propose an alternative \"multiple choice prompting\" (MCP) approach where the question and answer options are presented together to the LLM, which then outputs the symbol associated with its chosen answer.", "topic_class_1": "summary of the method", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors define the concept of \"multiple choice symbol binding\" (MCSB) - the ability of an LLM to associate answer options with their representing symbols in an order-invariant way - as a key requirement for MCP to be effective.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They show that LLMs vary greatly in MCSB ability, with models like Codex and InstructGPT demonstrating much higher MCSB than GPT-3.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Across experiments on 20 diverse MCQA datasets, the authors find that using MCP with a high MCSB model like Codex leads to significantly higher accuracies compared to using CP.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Codex with MCP matches or exceeds previous state-of-the-art results on 9 out of the 20 datasets.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors conclude that the MCQA capabilities of LLMs have been previously underestimated due to reliance on CP, and that MCP is a more effective approach for aligning LLMs to MCQA tasks.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper presents a novel and intuitive idea in multiple choice prompting (MCP) that leads to significantly improved MCQA performance from LLMs compared to the widely used cloze prompting (CP) approach.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive experiments convincingly demonstrate the effectiveness of MCP.", "topic_class_1": "strength on extensive experiments", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors identify and formalize an important capability required for MCP to work well - multiple choice symbol binding (MCSB).", "topic_class_1": "summary of the paper", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Their analysis of MCSB abilities of various LLMs provides useful insights.", "topic_class_1": "strength on model analysis", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The breadth of the empirical evaluation is a key strength - experiments span 20 diverse MCQA datasets and the results consistently support the main claims.", "topic_class_1": "strength on extensive experiments", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Matching/exceeding SOTA on 9 datasets with a single LLM and prompting approach is quite remarkable.", "topic_class_1": "summary of the experiments", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is clearly written, well-structured and easy to follow.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Assumptions and experimental details are properly spelled out.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the paper demonstrates the effectiveness of MCP, it doesn't provide many insights into what enables higher MCSB in some models. A deeper analysis of model properties that lead to high MCSB would have been useful.", "topic_class_1": "concern on the lack of discussions on high MCSB ability", "topic_class_2": "Concern on the limited insights", "reliability": "No", "explanation": "The analysis for high MCSB ability falls outside the scope of this paper", "error_type": "Out-of-scope"}, {"segment_text": "- The authors avoid any prompt engineering and use very standardized prompts throughout. While this helps keep the comparison between CP and MCP clean, it would have been interesting to see if prompt engineering can further boost MCP performance.", "topic_class_1": "concern on the lack of prompt engineering analysis", "topic_class_2": "Concern on the experiment and analysis", "reliability": "No", "explanation": "More prompt engineering will introduce extra noise and may lead to an unfair comparison", "error_type": "Out-of-scope"}, {"segment_text": "- Dataset leakage is a potential concern that could have been analyzed in a bit more depth, though the authors do provide reasonable arguments for why it's unlikely to be a major factor.", "topic_class_1": "concern on dataset leakage", "topic_class_2": "Concern on the dataset leakage issue", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- It would have been nice to see results on some non-English MCQA datasets to further demonstrate the generality of the approach.", "topic_class_1": "concern on the lack of multilingual experiments", "topic_class_2": "Concern on the coverage of datasets", "reliability": "No", "explanation": "More work across language & domains can always be done, but this is not the main focus of this paper. Therefore, it should not be one of the major weaknesses.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors do a commendable job of detailing the experimental setup and grounding the work in prior literature.", "topic_class_1": "strength on extensive experiments", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The overall quality of research seems high, with extensive experiments supporting the key claims.", "topic_class_1": "opinion on the quality", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The idea of using multiple choice prompts for MCQA with LLMs is quite novel to the best of my knowledge.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While some prior works have used this prompting approach on a couple of datasets, this paper is the first to extensively explore MCP, introduce key related concepts like MCSB, and demonstrate its effectiveness over CP on a wide range of MCQA tasks.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors have provided source code and specified the model versions and hyperparameters, which should allow reproducibility of results.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Use of a fixed random seed is a good practice.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "In summary, this is a high-quality paper that makes a valuable contribution to the field of using LLMs for MCQA.", "topic_class_1": "opinion on the quality", "topic_class_2": "Summary of strengths", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors present multiple choice prompting as a more effective alternative to cloze prompting, introduce the concept of multiple choice symbol binding ability, and demonstrate significant improvements in MCQA accuracy across a wide range of datasets.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of strengths", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written and easy to follow, with extensive experiments supporting the key claims.", "topic_class_1": "strength on writing", "topic_class_2": "Summary of strengths", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A deeper analysis of what enables high MCSB and the effect of prompt engineering could have made the work even stronger.", "topic_class_1": "concern on the lack of prompt engineering analysis", "topic_class_2": "Summary of weaknesses", "reliability": "No", "explanation": "More prompt engineering will introduce extra noise and may lead to an unfair comparison", "error_type": "Out-of-scope"}, {"segment_text": "Overall, this paper has the potential to shift how LLMs are applied to MCQA tasks going forward.", "topic_class_1": "opinion on the influence of this paper", "topic_class_2": "Summary of potential impacts", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 10", "Confidence": " 5"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes an innovative approach to multiple choice question answering (MCQA) tasks using large language models (LLMs), specifically focusing on improving the presentation of questions and answer choices to the LLM.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It introduces a method termed \"multiple choice symbol binding\" (MCSB), allowing the model to more effectively associate answer choices with their corresponding symbols (e.g., \"A\", \"B\", \"C\"), which significantly enhances its performance on MCQA tasks across 20 diverse datasets.", "topic_class_1": "summary of the method", "topic_class_2": "Summary of method", "reliability": "No", "explanation": "The approach studied in this paper is called multiple choice prompting (MCP). The authors showed that models with strong MCSB ability can perform better on MCQA with MCP. MCSB itself is not a method.", "error_type": "Misunderstanding"}, {"segment_text": "The authors claim that this approach not only simplifies the computational process but also yields accuracy improvements, thereby narrowing the gap with state-of-the-art (SOTA) methods without requiring task-specific tuning.", "topic_class_1": "summary of the method's advantage", "topic_class_2": "Summary of claim", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is well-structured, making it easy to follow the authors' thought process and the logical progression of their work.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Novelty in addressing the MCQA task by changing the interaction pattern between the question-answering model and the input data.", "topic_class_1": "strength of novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Comprehensive evaluation across 20 diverse datasets demonstrates the method's effectiveness and generalizability.", "topic_class_1": "strength on experiments", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The provision of code and detailed methodological descriptions supports the reproducibility of the results.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper could benefit from a more in-depth discussion of the limitations of the proposed MCSB approach, including potential biases introduced by symbol binding.", "topic_class_1": "concern on experiment analysis", "topic_class_2": "Concerns on the ethical issues", "reliability": "No", "explanation": "MCSB is not an approach but an ability of the language model. ", "error_type": "Misunderstanding"}, {"segment_text": "- Comparison with state-of-the-art methods focuses primarily on performance metrics, with less emphasis on computational efficiency and scalability.", "topic_class_1": "concern on evaluation metrics", "topic_class_2": "Concerns on the metrics", "reliability": "No", "explanation": "The authors have discussed the computational efficiency in section 6 (i.e., \"This improved performance also comes without reliance on\nspecialized normalization procedures, and with 4.3x less API calls (or forward passes of batch size 1) than the chosen CP strategies across tasks and exemplar settings.\")\n\nThere is no scalability issue in this approach.\n\nThis work primarily focuses on the newly proposed \"multiple choice symbol binding\" capability of LLMs and its effects on MCQA tasks. As such, it's appropriate to use performance metrics as the main evaluation method, rather than efficiency or scalability.", "error_type": "Neglect"}, {"segment_text": "- The impact of MCSB's effectiveness across languages or domains (beyond the datasets tested) remains unclear.", "topic_class_1": "concerns on the lack of multilingual/domain shifting settings", "topic_class_2": "Concerns on broader experiment settings", "reliability": "No", "explanation": "The authors have conducted an extensive series of experiments, utilizing approximately 20 datasets under a variety of settings for several language models. These experiments are comprehensive and sufficiently support the claims made in this paper. Expanding the experiments to encompass more domains or languages is beyond the scope of this particular study.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and structured, presenting a novel approach to MCQA tasks that contribute to the field's understanding of leveraging LLMs in a more efficient and effective manner.", "topic_class_1": "strength on writing and novelty", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The quality of research is high, demonstrated by rigorous experimental design and thorough evaluation.", "topic_class_1": "strength on rigorous experiments", "topic_class_2": "Strength on the experiment", "reliability": "No", "explanation": "Conflict with the previous part, say, weaknessess part.", "error_type": "Contradiction"}, {"segment_text": "The novelty of the MCSB method and its impact on LLM performance in MCQA tasks are well-articulated.", "topic_class_1": "strength on writing and novelty", "topic_class_2": "Strength on the novelty and results", "reliability": "No", "explanation": "MCSB is not an approach, but an ability of the language model.", "error_type": "Misunderstanding"}, {"segment_text": "Reproducibility is supported by the availability of code and detailed experimental setup.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review", "topic_class_1": "section ttile", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents an innovative and effective approach to improving LLMs' performance on MCQA tasks through MCSB.", "topic_class_1": "summary of the paper topic", "topic_class_2": "Summary of contributions", "reliability": "No", "explanation": "The proposed approach is not MCSB but MCP (Multiple Choice Prompting)", "error_type": "Unstated statement"}, {"segment_text": "The method's generalizability and potential to simplify computational processes in MCQA tasks are significant contributions to the field.", "topic_class_1": "strength on the generalizability of the approach  ", "topic_class_2": "Summary of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the paper is of high quality and novelty, further exploration of its limitations and a broader comparison with existing methods could strengthen its impact.", "topic_class_1": "suggestion for further improvemens ", "topic_class_2": "Summary of concerns", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section ttile", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the effectiveness of different prompting methods for utilizing large language models (LLMs) in multiple-choice question answering (MCQA) tasks.", "topic_class_1": "summary of the paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It argues that the traditional cloze prompting (CP) approach, where the LLM independently scores each answer option, has limitations.", "topic_class_1": "summary of the paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes multiple-choice prompting (MCP) as an alternative, where the LLM receives the question and all answer options jointly.", "topic_class_1": "summary of the paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors introduce the concept of multiple-choice symbol binding (MCSB) ability, which is crucial for LLMs to benefit from MCP.", "topic_class_1": "summary of the paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They demonstrate that Codex, an LLM with high MCSB ability, significantly outperforms CP across 20 diverse datasets when prompted with MCP, often approaching or exceeding state-of-the-art (SOTA) performance.", "topic_class_1": "summary of the experiments", "topic_class_2": NaN, "reliability": "No", "explanation": "This is not accurate. The authors have done experiments for multiple LLMs, out of which Codex and Instruct models have extraordinary performance.", "error_type": "Misunderstanding"}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section ttile", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section ttile", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Compelling argument for MCP: The paper clearly outlines the limitations of CP and presents a strong case for the advantages of MCP in MCQA tasks.", "topic_class_1": "strength on soundness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Extensive evaluation: The authors evaluate their approach on a wide range of 20 datasets, demonstrating the robustness and generalizability of MCP across different domains and question types.", "topic_class_1": "strength on extensive experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Significant performance improvement: Codex with MCP achieves substantial accuracy gains over CP and even surpasses previous SOTA results on several datasets.", "topic_class_1": "summary of experiment results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This suggests that LLMs have been underestimated for MCQA due to suboptimal prompting methods.", "topic_class_1": "summary of experiment results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addresses dataset leakage concerns: The authors acknowledge the potential issue of dataset leakage and provide arguments and evidence suggesting that it does not significantly impact their results.", "topic_class_1": "strength on comprehensive discussion of experiment setup", "topic_class_2": NaN, "reliability": "No", "explanation": "Although the authors did provide a discussion on potential dataset leakage problems and the limited impact on their results, this should not count as a major strengths of this paper as it is more of a common practice to justify the validity of their results.", "error_type": "Experiment"}, {"segment_text": "Weaknesses:", "topic_class_1": "section ttile", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited model analysis: While the paper shows that Codex performs well with MCP, it does not delve deeper into the specific characteristics of LLMs that contribute to high MCSB ability. Further analysis could provide valuable insights for future model development.", "topic_class_1": "concern on lack of in-depth model analysis", "topic_class_2": NaN, "reliability": "No", "explanation": "The root cause for high MCSB ability falls outside the scope of this paper.", "error_type": "Out-of-scope"}, {"segment_text": "Prompt engineering not explored: The authors intentionally avoid prompt engineering to ensure a fair comparison between CP and MCP.", "topic_class_1": "concern on experiment setting", "topic_class_2": NaN, "reliability": "No", "explanation": "Minimum prompt engineering in the experiments is essential to ensure a fair comparison between CP and MCP. Exploring prompt engineering will introduce more variance and lead to a unfair comparison.\nThe authors avoided prompt engineering so as to make fair comparisons between CP and MCP methods. Not exploring prompt engineering improves the validity of their results.", "error_type": "Out-of-scope"}, {"segment_text": "However, exploring prompt engineering techniques for MCP could potentially lead to further performance improvements.", "topic_class_1": "opinion on new experiment settings", "topic_class_2": NaN, "reliability": "No", "explanation": "The authors avoided prompt engineering so as to make fair comparisons between CP and MCP methods. Not exploring prompt engineering improves the validity of their results.", "error_type": "Out-of-scope"}, {"segment_text": "Computational cost: While MCP is more efficient than CP per question, the evaluation process across numerous datasets and prompting methods remains computationally expensive. This highlights the need for more efficient evaluation strategies for large-scale LLM research.", "topic_class_1": "concern on computational cost", "topic_class_2": NaN, "reliability": "No", "explanation": "MCP can be done in zero-shot. Thus, there should be no computational issues in zero-shot setting.\n\nThis is NOT a weakness and is irrelevant to the main claims and results of the paper. MCP does not introduce extra cost. Evaluation across many datasets is almost equally expensive for both MCP and CP. ", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written and clearly presents the research question, methodology, and findings.", "topic_class_1": "strength on writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, with a comprehensive evaluation across diverse datasets.", "topic_class_1": "strength on comprehensive evaluation", "topic_class_2": NaN, "reliability": "No", "explanation": "The reviewer has a different opinion when discussing the weakness.", "error_type": "Contradiction"}, {"segment_text": "The concept of MCSB and the use of MCP for LLMs in MCQA are novel contributions.", "topic_class_1": "strength on novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors provide code and details to facilitate the reproduction of their experiments.", "topic_class_1": "strength on reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a significant contribution to the field of LLM research by demonstrating the effectiveness of multiple-choice prompting for MCQA tasks.", "topic_class_1": "summary of the contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive evaluation and substantial performance improvements suggest that MCP can unlock the potential of LLMs for MCQA and contribute to the vision of LLMs as foundation models.", "topic_class_1": "summary of the experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further investigation into the factors influencing MCSB ability and exploration of prompt engineering for MCP are promising directions for future work.", "topic_class_1": "suggestion on further explorations", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Words are all you need? Language as an approximation for human similarity judgments", "body_text": "\n\nW ORDS ARE ALL YOU NEED ? L ANGUAGE AS AN AP - PROXIMATION FOR REPRESENTATIONAL SIMILARITY\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Human similarity judgments are a powerful supervision signal for machine learn- ing applications based on techniques such as contrastive learning, information retrieval, and model alignment, but classical methods for collecting human similar- ity judgments are too expensive to be used at scale. Recent methods propose using pre-trained deep neural networks (DNNs) to approximate human similarity, but pre-trained DNNs may not be available for certain domains (e.g., medical images, low-resource languages) and their performance in approximating human similarity has not been extensively tested. We conducted an evaluation of 611 pre-trained models across three domains \u2013 images, audio, video \u2013 and found that there is a large gap in performance between human similarity judgments and pre-trained DNNs. To address this gap, we propose a new class of similarity approximation methods based on language. To collect the language data required by these new methods, we also developed and validated a novel adaptive tag collection pipeline. We find that our proposed language-based methods are significantly cheaper, in the number of human judgments, than classical methods, but still improve performance over the DNN-based methods. Finally, we also develop \u2018stacked\u2019 methods that combine language embeddings with DNN embeddings, and find that these consis- tently provide the best approximations for human similarity across all three of our modalities. Based on the results of this comprehensive study, we provide a concise guide for researchers interested in collecting or approximating human similarity data. To accompany this guide, we also release all of the similarity and language data, a total of 206,339 human judgments, that we collected in our experiments, along with a detailed breakdown of all modeling results.\n\n1 I NTRODUCTION\n\n Similarity judgments have long been used as a tool for studying human representations, both in cognitive science (Shepard, 1980; 1987; Tversky, 1977; Tenenbaum & Griffiths, 2001), as well as in neuroscience, as exemplified by the rich literature on the representational similarity between humans and machines (Schrimpf et al., 2020; Kell et al., 2018; Linsley et al., 2017; Langlois et al., 2021; Yamins et al., 2014)). Recent research in machine learning suggests that incorporating human similarity judgments in model training can play an important role in a variety of paradigms such as human alignment (Esling et al., 2018), contrastive learning (Khosla et al., 2020), information retrieval (Parekh et al., 2020), and natural language processing (Gao et al., 2021). However, building a large dataset based on human similarity judgments is very expensive and often infeasible since the number of judgments required is quadratic in the number of stimuli \u2013 for N stimuli, O ( N 2 ) judgments are required 1 . For example, to fully quantify the similarity of all possible dyadic pairs of 50,000 images, one needs to collect on the order of 1.25 billion ( \u223c 50000 2 2 ) human similarity judgments. Thus, human judgments are the main bottleneck for machine-learning methods based on similarity. For this reason, the majority of available human similarity datasets are small by machine learning standards (up to a few thousand objects). 1 Depending on various assumptions, the full range of classical methods can require between O ( N log N ) (Jamieson & Nowak, 2011) and O ( N 3 ) (Hebart et al., 2020) human judgments. In this work, we used O ( N 2 ) human judgments (collecting all unique dyadic pairs) as the baseline for comparison 1 Advancements in deep learning have brought an alternative approach that does not require extensive collection of human judgments. Specifically, the idea is to use the similarity between hidden repre- sentations in pre-trained deep neural networks (DNNs) to approximate human similarity (Peterson et al., 2018; Jha et al., 2020; Marjieh et al., 2022; Hebart et al., 2020; Roads & Love, 2021). Some of these methods also suggest fine-tuning representations on a small training set of human similarity judgments (Peterson et al., 2018). This, in turn, results in a significant reduction in the number of required human judgments down to O (1) (given the pre-trained model). While such methods are promising, they still require access to strong pre-trained models which may not necessarily be available in all domains (e.g., medical datasets, niche modalities, low-resource languages, etc.). In addition, representations obtained from neural networks may not always overlap with human similarity representations, given that the models can be trained for different objectives (i.e., their embeddings may be poor approximations for human similarity). A comprehensive comparison to assess which models perform well in predicting human similarity across different modalities is currently lacking in the literature. To this end, one of our main contributions in this paper is providing a first-of-its-kind large-scale evaluation of over 600 publicly- available pre-trained models as approximations for human similarity judgments on three modalities (images, audio, video). Our experiments reveal that there is a large gap in performance between the O (1) DNN methods and the classical O ( N 2 ) similarity method we used as the baseline. To address this gap, we propose a new class of O ( N ) methods to efficiently and accurately ap- proximate human similarity based on language. This is motivated by a long line of research in cognitive science suggesting that language is an extremely efficient way for humans to communicate information about their sensory environment (Murphy, 2004; Zaslavsky et al., 2018; Piantadosi et al., 2011; Jaeger & Levy, 2006). This in turn suggests that we can use textual descriptors to approximate similarity judgments across different modalities. Moreover, such textual descriptors can be collected Pairwise similarity Human ground truth How similiar are the animals? Prediction approaches Captions WFA similarity LLM similarity DNN similarity Stacked Tags + humaninteractions \ud835\udc42(\ud835\udc41 2 ) humaninteractions \ud835\udc42(1) humaninteractions \ud835\udc42(\ud835\udc41 ) humaninteractions \ud835\udc42(\ud835\udc41 ) We used data from three modalities: images, audio, and video. For each modality, we extracted deep model embeddings and gathered human captions and tags. Word- and language-embedding models, as well as simple word-frequency analysis, were used to predict human similarity judgments. 2 at the cost of O ( N ) human judgments (as people describe individual stimuli rather than pairs), which renders this method scalable. We consider two approaches for approximating similarity from text data. One approach is to use pre-trained Large Language Models (LLM) to produce vector embeddings of the textual descriptions, and then use a measure of distance between these embeddings to approximate human similarity. This method is more domain-agnostic than the O (1) deep learning methods as it only requires access to a pre-trained LLM regardless of the modality of the original dataset. However, there are some cases where the domain may be out-of-distribution for all available LLMs (e.g., niche technical fields), or where no LLMs are available at all (e.g., low-resource languages). In such cases, the other approach is to use Word-Frequency Analysis (WFA) methods from classical text processing literature (Barrios et al., 2016; Rouge, 2004; Beel et al., 2016), As for the textual descriptions themselves, we consider two types, namely, free-text captions and concise word tags. Collecting captions for machine learning datasets is a well-established practice and can easily be done through crowdsourcing platforms. On the other hand, there is no consensus on best practices for collecting tags without a pre-existing taxonomy (i.e., open-set labels). To address this, we propose a novel adaptive tag mining pipeline called Sequential Transmission Evaluation Pipeline (STEP-Tag) which we describe in Section 2.2.4. As we will show, STEP-Tag allows to collect meaningful, diverse, and high-quality word tags for target stimuli in an online crowdsourcing environment. Finally, we propose one additional set of hybrid approximation methods that combine sensory information with textual descriptions while still requiring O ( N ) human judgments. For this approach, we propose to stack the embeddings derived from both domain-specific models (e.g., output from the last layer of an image classifier) with the LLM embedding of the respective textual description. When multi-modal models are available, we can similarly leverage the joint embedding of both the stimulus and its textual description. We evaluate all of these novel and existing methods across multiple modalities. We test the relative contributions of linguistic and sensory information in approximating human similarity and show that our proposed language-based methods provide both accurate and efficient approximations across modalities, even though they do not require a trained modality-specific deep learning model. Crucially, with this large-scale evaluation, we are able for the first time to provide researchers with a comprehensive guide of the tools to use for approximating human similarity at scale. To summarize, our contributions are as follows: \u2022 We conduct a comprehensive comparison of human similarity approximation methods. \u2022 We propose a novel modality-agnostic method for approximating similarity based on text and show that it is both efficient and competitive in terms of performance. \u2022 We propose STEP-Tag, a novel adaptive tagging pipeline, and show that it is effective for crowdsourcing high-quality and diverse sets of word tags. \u2022 We synthesize our findings into a detailed guide for researchers interested in approximating human similarity judgments at scale. \u2022 We collect and release ground-truth and approximated versions of a large behavioral dataset ( N = 1,492) across three different domains (images, audio, video), including two text- approximated similarity matrices for 1,000 audio clips and 1,000 video clips.\n\n2 D ATASETS 2.1 S TIMULI\n\n Throughout this work, we considered five stimulus datasets across three different modalities \u2013 images, audio, and video \u2013 consisting of a total of 31,320 dyadic pairs labeled with similarity. Images For images, we considered three datasets of common objects introduced in Peterson et al. (2018) \u2013 namely, animals, furniture, and vegetables \u2013 each consisting of 7,140 dyadic pairs (all unique pairs over 120 images). 3 Audio For audio, we used the RAVDESS corpus (Livingstone & Russo (2018), released under a CC Attribution license), which consists of semantically neutral sentences spoken by 24 US American actors to convey a specific target emotion. To construct a 1,000-recording subset, we selected 3 emotions per speaker per sentence. We randomly omitted 104 emotional stimuli and included all 96 neutral recordings (the dataset only contains 2 neutral recordings per speaker per sentence). To construct the subset composed of 4,950 dyadic pairs (all unique pairs over 100 recordings), we randomly selected \u223c 13 recordings per emotion from the 1,000. Video Finally, for the video dataset, we considered the Mini-Kinetics-200 dataset (Xie et al. (2018), released under a CC BY 4.0 International License), which contains a large set of short video clips of human activities from 200 activity classes. Specifically, we focused on the validation split, which contains 5,000 videos in total. To construct our 1,000-video dataset, we sampled 5 random videos from each of the 200 activity categories. The 100-video subset (4,950 dyadic pairs) used in the similarity judgment collection experiment was then generated by sampling 100 random stimuli from the 1,000 list.\n\n2.2 H UMAN JUDGMENT COLLECTION 2.2.1 P ARTICIPANTS\n\n We collected data from N = 1,492 US participants for the new behavioral experiments reported in this paper. Participants were recruited anonymously from Amazon Mechanical Turk and provided informed consent under an approved IRB protocol prior to participating in our studies. Participants earned 9-12 USD per hour, and each session lasted less than 30 minutes. To help recruit reliable participants, we required that participants are at least 18 years of age, reside in the United States and have participated in more than 5,000 previous tasks with a 99% approval rate (see Supplementary Section B for additional details about the behavioral experiments). All experiments were implemented with the Dallinger and PsyNet frameworks designed for automation of large-scale behavioral research (Harrison et al., 2020). In Supplementary Section A.1, we include the data that was collected, instructions used, and code for replication of the behavioral experiments. We also provide the code for computational experiments and analysis.\n\n2.2.2 S IMILARITY JUDGMENTS\n\n We collected two batches of pairwise similarity judgements, one for each of the audio and video subsets, and were provided access to the similarity matrices for the three image datasets by the authors of Peterson et al. (2018). For each pair we collected \u223c 5 similarity judgments to average out inter-rater noise.\n\n2.2.3 C APTIONS\n\n We collected free-text captions for the video and audio datasets. Captions for the image datasets were already collected by Marjieh et al. (2022) and used here with permission. For each stimulus, we collected \u223c 10 captions.\n\n2.2.4 T AGS\n\n We propose a novel adaptive tag pipeline for simultaneous data collection and evaluation called Sequential Transmission Evaluation Pipeline (STEP) and apply it in the context of semantic tag mining (STEP-Tag). Our paradigm, STEP-Tag, allows researchers to efficiently collect high-quality word tags for a given stimulus (Figure 2) and extends existing crowdsourcing text-mining techniques (Von Ahn & Dabbish, 2008; 2004; Krishna et al., 2017; Law et al., 2007) by integrating ideas from transmission chain experiments (Kirby et al., 2008; Griffiths & Kalish, 2005). In STEP-Tag, participants adaptively create tags for a set of target stimuli and simultaneously evaluate the annotations made by previous participants. In each trial, participants are first given a stimulus (e.g., an image or audio fragment) and rate the relevance of tags that were created by other participants (on a 5-interval Likert scale) or flag a tag if they find it inappropriate (with tags removed if more than two people flag the tag). Next, participants are also given the opportunity to add new tags if they feel a relevant tag that describes the stimulus is missing. The results of the annotation procedure of one participant then 4 brussel sprouts 3 propagate to the next participant (additional details about the paradigm, and screenshots are provided in Supplementary Section B.6). Ultimately, as the process unfolds over many iterations, meaningful tags are extracted and validated by multiple participants, enabling efficient open-label collection of a desired dataset. To validate STEP-Tag, we compared it against several baselines: (i) randomly selecting only a single high-rated tag from the last iteration of STEP-Tag per stimulus, (ii) using tags only from the first iteration of STEP-Tag (equivalent to non-adaptive tag collection), and (iii) using class labels instead of tags. We found that tags produced after multiple iterations of STEP-Tag outperformed all three baselines in terms of quality (i.e., downstream performance for similarity reconstruction) and diversity (see Supplementary Section B.6.1).\n\n3 M ODELS 3.1 DNN- BASED METHODS\n\n We tested a wide range of pre-trained ML models that do not rely on text (overall we tested 611 models) and compared their internal representations to human similarity judgments and text-based predictions (Figure 1A). We compiled our model pool by leveraging pre-trained model reposito- ries (or zoos) available online. In particular, for images we use 569 pre-trained models from the pytorch-image-models package timm (Wightman, 2019), for audio we use 36 pre-trained models available in the torchaudio package (Yang et al., 2021) (see also Supplementary Figure 10 for an analysis of layer depth), and for video we use 6 pre-trained models available from the PyTorchVideo package (Fan et al., 2021). Because of the recent success of multimodal training, we additionally included 9 multimodal models based on CLIP from OpenAI\u2019s public implementa- tion ( https://github.com/openai/CLIP ) for the image datasets, and compared them to \u201cstacked\u201d representations (i.e., concatenating embeddings from separate image and text models).\n\n3.2 LLM- BASED METHODS 3.2.1 T AGS\n\n To embed tags we used ConceptNet Numberbatch (CNNB) which is a word-embedding model trained on the ConceptNet knowledge graph that leverages other popular word embedding models such as word2vec and GloVe (Speer et al., 2017). We experimented with several algorithms for computing similarity between sets (or multi-sets) of tags and share the details in Supplementary Section C.1.2. 5\n\n3.2.2 C APTIONS\n\n To embed captions, we used four pre-trained LLMs from HuggingFace (Wolf et al., 2020): \u2018bert-base- uncased\u2019, \u2018deberta-xlarge-mnli\u2019, \u2018sup-simcse-bert-base-uncased\u2019, and \u2018sup-simcse-roberta-large\u2019. SimCSE is a pre-training procedure that uses semantic entailment in a contrastive learning objective (Gao et al., 2021). According to BERTScore (Zhang et al., 2020), the latter three models are ranked in the top 40 models in terms of correlation with human evaluations on certain tasks, with \u2018deberta- xlarge-mnli\u2019 ranked first. However, in our experiments, we found that embedding similarity computed from \u2018sup-simcse-roberta-large\u2019 has the highest correlation with human similarity judgments out of the four models. For SimCSE-based models, we used representations from the (final) embedding layer (where the SimCSE contrastive objective is actually applied). For the other two models, we computed embeddings from every layer, but restricted the main analysis to embeddings from the penultimate layers. This was done in order to be consistent with our procedure for DNNs.\n\n3.3 S TACKING METHODS\n\n We produce stacked representations for each modality by concatenating the single best-performing (see Figure 3) LLM\u2019s embeddings with the embeddings from the five best-performing DNNs into a single set of long embeddings. Since the two sets of embeddings come from different spaces, we add a single tunable hyperparameter for rescaling the LLM embeddings. This hyperparameter can be set manually, but we use a small number of ground-truth similarity judgments (we use dyadic pairs for just 20 stimuli) to optimize it automatically.\n\n3.4 W ORD F REQUENCY A NALYSIS (WFA) METHODS\n\n The aim of the WFA methods is to enable similarity approximation from language using traditional embedding-free techniques. Such techniques are particularly useful for low-resource languages or cross-cultural comparisons (Cowen & Keltner, 2017; Barrett, 2020), for which pre-trained models are lacking, as they work solely on the basis of the text itself. The WFA methods we considered included measuring co-occurrence, Rouge score, bm25s, and tf-idf. We provide details on each of these procedures in Supplementary Section C.2.\n\n3.5 P ERFORMANCE METRIC\n\n We quantified performance by computing the Pearson correlation r between approximated similarity scores and the ground-truth human similarity scores for all the unique dyadic pairs in a dataset. We compared the performance of the different prediction methods to the inter-rater reliability (IRR) of participants, which serves as an approximate upper-bound on performance. Following Peterson et al. (2018), we computed IRR for each human similarity matrix using the split-half correlation method with a Spearman-Brown correction (Brown, 1910).\n\n4 R ESULTS\n\n Figure 3 summarizes the performance of the various techniques across the three modalities. Note that the image modality results in Figure 3A are averaged across the three image datasets and only show the top 50 methods for this modality due to space constraints. Figure 3D shows the mean performance of the methods of each type for each modality. When viewing these results, a clear hierarchy emerges. While no approximation methods can perfectly match the ground-truth pairwise similarity, (see the gap between the methods and IRR), stacked ones get close and are consistently more aligned with human similarity than other methods across all three modalities. Text-based methods come next in this hierarchy, followed by DNN-based ones. The pre-eminence of stacked results suggests that LLMs and DNNs capture at least some different sources of variance in human similarity judgments. This is reinforced by our surprising finding that stacked representations from CLIP, a state-of-the-art jointly pre-trained multi-modal model, do not outperform stacked representations from independently trained models. We hypothesize that this happens because information is lost from both modalities when optimizing for a joint embedding. However, we note that the modest size of the performance gap between stacked and LLMs/DNNs, 6 0.3 0.4 0.5 0.6 0.7 0 8 0.9 M ean c o rr e l a t i on A Inter reliability R o BE R T a ( S i m C SE ) + w a v 2 v e c 2 . 0 l a r ge l v 60 k R o BE R T a ( S i m C SE ) + w a v 2 v e c 2 . 0 l a r ge l v 60 k ( 10 m ) R o BE R T a ( S i m C SE ) + w a v 2 v e c 2 . 0 l a r ge l v 60 k ( 960h ) R o BE R T a ( S i m C SE ) + w a v 2 v e c 2 . 0 l a r ge l v 60 k ( 100h ) R o BE R T a ( S i m C SE ) + w a v 2 v e c 2 . 0 l a r ge X L S R 53 R o BE R T a ( S i m C SE ) T ag s c o - o cc u rr en c e W a v L M T ag s m ean T ag s m ean ( no s p li t ) T ag s r ouge R T ( i m C SE ) T ag s quan t i z ed H u BE R T ag s o v e r l ap T ag s c o - o u rr en c e -r ep T ag s b m 25 S T ag s tf - i d f - c s i ne C ap t i on s tf - i d f - c s i ne ap t i on s m 25 S w a v 0 T M ean c o rr e l a t i on B reliability + S l o w f a s t r 50 T ag s m ean + S l o w f a s t r 101 T ag s m ean + S l o w r 50 T ag s m ean + X 3d M T ag s m ean + X 3d S T ag s m ean T ag s m ean ( no s p li t ) R o BE R T a ( S i m C SE ) C ap t i on s b m 25 S C ap t i on s tf - i d f - c o s i ne s l o w f a s t BE R T ( S i m C SE ) D e BE R T a T ag s b m 25 S T ag s tf - i d f - c o s i ne s l o w T ag s quan t i z ed T ag s r ouge T ag s c o - o cc u rr en c e T ag s c o - o cc u rr en c e -r ep T ag s o v e r l ap BE R T x 3d suggests that there is also significant overlap between aspects of human similarity captured by language and perception. To investigate the effect of architecture and downstream task (e.g., classification) performance on alignment of DNNs with human similarity, for the image modality we compared similarity approximation performance against the number of model parameters on a log scale (Figure 4A) and ImageNet classification performance (Deng et al., 2009) (Figure 4B). Overall, we found a positive correlation between similarity approximation performance and the number of model parameters 7 ( r = 0 . 39 , p < 0 . 001 ) and a smaller but still significant positive correlation with performance on ImageNet ( r = 0 . 26 , p < 0 . 001 ), There were some notable exceptions with particularly high ImageNet performance but low similarity performance, such as the image transformer BEiT (Bao et al., 2021). Finally, we leverage both DNN-based methods and our proposed language-based methods to approxi- mate similarity matrices that would otherwise require an unaffordable number of human similarity judgments to collect all dyadic pairs. Specifically, we approximate the two similarity matrices corresponding to all 1,000 audio clips and 1,000 video clips in our datasets using every method listed for each of those modalities in Figure 3. We provide visualizations of the resulting matrices at https://words-are-all-you-need.s3.amazonaws.com/index.html . We note that to exhaustively collect all dyadic pairs with five judgments per pair would normally require roughly 2.5 million human judgments for each of these matrices. Number of C o rr e l a t i on w i t h hu m an s i m il a r i t y C o rr e l a t i on w i t h hu m an s i m il a r i t y\n\n5 D ISCUSSION\n\n In this work, we compared novel and existing methods for approximating human similarity judgments. Based on the results outlined above, we are now able to provide researchers with a best-practice guide to collecting similarity datasets. Our guide is based on two bottlenecks that researchers may face: one is the limit on the number of judgments that can be collected (e.g., due to cost) and the second is the availability of pre-trained models (i.e., either DNNs or LLMs). Our results make it clear that deep learning can provide good approximations for human similarity. In fact, when both pre-trained LLMs and DNNs are available, stacking their representations is consistently the best approach. However, 8 even when neither type of pre-trained models are available, we suggest that classical word-frequency analysis methods still provide researchers with an efficient and competitive method for approximating human similarity. Our guide, comprehensively covering these and other cases, is laid out in Figure 5. One limitation of this work is that while similarity proxies generated from our pipeline can support ML datasets, they are also at risk of baking in high-level human biases that can lead to adverse societal implications, such as amplifying race and gender gaps. Researchers should devote utmost care to what they choose to incorporate in their training objective. On the positive side, we believe that our approach paves the way for the study of cross-cultural variation of human semantic representations by providing efficient tools for crowdsourcing high-quality semantic descriptors across languages. This is particularly relevant for low-resource languages, where our tag-mining techniques can work even with the absence of pre-trained ML models (Thompson et al., 2020; Barrett, 2020). Taken together, our results showcase how we can leverage language to make machine representations more human- like. Moreover, it highlights the importance of combining machine learning and cognitive science approaches for mutually advancing both fields. In particular, we believe that the methodologies adopted in this work have the potential to greatly advance basic research on naturalistic representations in cognitive science.\n\nR EFERENCES\n\n Alexei 9 10 11\n\nSteven T\n\n\n\nHarry\n\n\n\nEdward\n\n\n\nMirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung- Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Fran\u00e7ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori\n\n\n\nand Yoshua\n\n arXiv 2106.04624.\n\nHa Hong, Najib J Majaj, Rishi Rajalingham\n\n\n\nElias B Issa, Kohitij\n\n 12 . 13\n\nS UPPLEMENTARY MATERIALS A S TIMULI AND DATA A.1 C ODE AND DATA AVAILABILITY\n\n A view-only anonymous link is provided to the public, containing all the data collected for this project during the review stage 2 . It includes the new human behavioral data, the computational experiments with machine learning models, and all the necessary analyses scripts for producing the results. Additionally, the repository includes the Dallinger/PsyNet source codes for reproducing the behavioral experiments. Finally, we present an interactive visualization 3 for exploring the similarity between stimuli as experienced by humans and different methods reported in the paper.\n\nB B EHAVIORAL P ARADIGMS B.1 P ARTICIPANTS\n\n The exact number of participants for each of the 9 new behavioral experiments is reported in Table 1. Note \u2018 N \u2019 denotes the number of participants included in the analysis; \u2018LX\u2019 denotes the LexTALE ; \u2018HT\u2019 denotes .\n\nB.2 I MPLEMENTATION\n\n All behavioral experiments were implemented using the Dallinger 4 and PsyNet (Harrison et al., 2020) frameworks. Dallinger is a modern tool for experiment hosting and deployment which automates the process of participant recruitment and compensation by integrating cloud-based services such as Heroku 5 with online crowd-sourcing platforms such as AMT. PsyNet is a novel experiment design framework that builds on Dallinger and allows for flexible specification of experiment timelines as well as providing support for a wide array of tasks across different modalities (visual, auditory and audio-visual). Participants interact with the experiment through their web-browser, which in turn communicates with a backend Python server responsible for the experiment logic.\n\nB.3 P RE - SCREENING\n\n A common technique for filtering out participants that are likely to deliver low-quality responses, as well as automated scripts (bots), is to implement pre-screening tasks prior to the main part of 2 Code and data: https://osf.io/kzbr5/?view_only=3dea58e008ce41c290ef0f374bdbf444 3 Interactive plots: https://words-are-all-you-need.s3.amazonaws.com/index. html 4 https://dallinger.readthedocs.io/en/latest/ 5 https://www.heroku.com/ 14 each experiment. Failing the pre-screening tasks results in early termination of the experiment. Nevertheless, participants are still compensated for their time regardless of whether they fail or succeed on a pre-screener to ensure fair compensation. The role of pre-screeners in our studies was to realize two main criteria for data quality, namely, a) to be able to collect high-quality text descriptors, and b) to ensure that participants are able to inspect the target stimuli properly (in particular the audio component in prosody and videos). To do this, we implemented two pre-screening tasks, an English proficiency test and a standardized headphone test (used only for audio and video experiments). Table 1 provides details on which pre-screeners were used in each of the behavioral experiments. English proficiency test . To test participants\u2019 English proficiency, we used LexTALE, a lexical decision task developed in Lemh\u00f6fer & Broersma (2012). In each trial, participants were briefly presented (1 second) with either a real English word or a made up word that does not exist. Participants were instructed to guess whether the word was real or not. A total of 12 trials (half of them being real words) were presented, and 8 of them needed to be correct for the participant to pass. The presented words were: hasty, fray, stoutly, moonlit, scornful, unkempt, mensible, kilp, plaintively, crumper, plaudate, alberation. An example trial is shown in Figure 6. Headphone test . We used the headphone test developed by Wood et al. (Woods et al., 2017), which is used as a standard pre-screener for high-quality auditory psychophysics data-collection procedures (Milne et al., 2021). The test is designed to ensure that the participants are wearing headphones and are able to perceive subtle differences in volume. The task consists of a forced choice task, in which three consecutive tones are played, and the participant has to identify which of them is the quietest. Crucially, these tones are constructed to exhibit a phase cancellation effect when not using headphones, and therefore making it difficult for non-headphone users to identify the quietest tone. Participants had to answer 4 out of 6 trials correctly to pass this test. An example trial is shown in Figure 7. 15\n\nB.4 S IMILARITY JUDGMENTS\n\n In the present work, we collected similarity judgments across audio and video datasets. Each dataset comprised of 4,950 unique pairs corresponding to the number of unordered subsets that contain two distinct objects (i.e., excluding self-similarity), within a set of 100 stimuli. We did not collect similarity judgments over the three datasets of images, as these were provided in Peterson et al. (2018) (and used here with permission). The experiments proceeded as follows: upon completion of the consent form and the pre-screening tasks, participants received instructions regarding the main experiment: Audio. In this experiment we are studying how people perceive emotions. In each round you will be presented with two different recordings and your task will be to simply judge how similar are the emotions of the speakers. Video. In this experiment we are studying how people perceive activities. In each round you will be presented with two different videos and your task will be to simply judge how similar are the activities in them. The instructions then continued as follows: You will have seven response options, ranging from 0 (\u2019Completely Dissimilar\u2019) to 6 (\u2019Completely Similar\u2019). Choose the one you think is most appropriate. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer. The quality of your responses will be automatically monitored, and you will receive a bonus at the end of the experiment in proportion to your quality score. The best way to achieve a high score is to concentrate and give each round your best attempt. The experiment will begin now. You will take up to 85 rounds where you have to answer this question. Remember to pay careful attention in order to get the best bonus! 16 As described in the instructions, in each trial, participants rated the similarity between a pair of sounds (how similar are the emotions of the two speakers?) or videos (how similar are the activities in the following two videos?) on a scale ranging from 0 (completely dissimilar) to 6 (completely similar) (Figure 8). Overall, participants completed 85 trials on a random subset of the possible pairs. To further motivate participants to provide good responses, we gave them an additional performance bonus for providing consistent data. Among the 85 trials, 5 trials were repeated for consistency checking. The responses were converted into a performance score by computing the Spearman correlation between the original and repeat ratings. Perfect scores resulted in a 10 cent bonus.\n\nB.5 C APTIONS\n\n We collected free-text captions for the video and audio datasets. Captions for the image datasets were previously collected in Marjieh et al. (2022) and used here with permission. After completing the consent form and pre-screening tests, participants received the following instructions: Audio. In this experiment we are studying how people describe emotions. You will be presented with different recordings of speakers and your task will be to describe their emotions. In doing so, please keep in mind the following instructions \u2022 Describe all the important aspects of the recording. Video. In this experiment we are studying how people describe activities in videos. You will be presented with different videos of activities and your task will be to describe their content. In doing so, please keep in mind the following instructions \u2022 Describe all the important activities in the video. As well as the following guidelines adapted from Marjieh et al. (2022): \u2022 Do not start the sentences with \"There is\" or \"There are\". \u2022 Do not describe unimportant details. \u2022 You are not allowed to copy and paste descriptions. \u2022 Descriptions should contain at least 5 words. \u2022 Descriptions should contain at least 4 unique words. Note: No prior expertise is required to complete this task, just describe what you intuitively think is important as accurately as possible. The quality of your captions will be monitored automatically and provid- ing low quality and repetitive responses could result in early termination of the experiment and hence a lower bonus. You will describe up to 50 recordings. These guidelines were enforced to ensure that participants deliver sufficiently informative captions that are not repetitive. In each trial of the main experiment, participants described a single audio (please describe the emotions of the speaker) or video stimulus (please describe the activity in the video). Overall, participants described up to 50 randomly presented stimuli. To filter out bad participants that tend to deliver repeated responses, in each trial (excluding the first 4 trials) we computed the mean edit distance between their current response and all previous responses that they previously provided using the partial_ratio function in thefuzz 6 Python package for fuzzy string matching. This function returns for a pair of input strings a matching score between 0 and 100 (100 being identical strings). Early termination was enforced if the mean response matching score was above 80. The idea here was to prevent participants from copying and pasting the same response over and over again (or varying it only slightly).\n\nB.6 T AGS\n\n For the image, audio, and video datasets, we collected tag data, i.e., concise labels that describe the salient features of a stimulus. To do so, we developed a novel tag mining paradigm called STEP-Tag in 6 https://github.com/seatgeek/thefuzz 17 which each stimulus was treated as a separate \u201cchain\u201d (see Figure 2 in the paper). When the stimulus was presented for the first time, the participant was asked to provide at least one tag. For the following iterations, we sequenced participants so that each of them had to rate the tags provided by participants from the previous iterations within the same chain. The rating was either choosing between one (not very relevant) to five stars (very relevant), or marking the tag as completely irrelevant by using the flag icon (see Figure 9). Participants could optionally introduce new tags that will subsequently be presented to other participants assigned to the same chain. Participants could only provide tags that were not already present, and they had to be in lower-case letters. To discourage frequent use of long word combinations, a pop-up window appeared if participants used two or more white spaces (i.e., three or more words) to warn that long combinations should only be used when completely necessary. This process continued for at least 10 iterations, after which we checked at each consequent iteration whether the chain was \u201cfull\u201d. We considered a chain to be full if its latest iteration had at least 2 tags that were rated at least 3 times and had a mean rating of 3 stars. If a chain was not full after 20 iterations, we stopped collecting further iterations. Since each experimental batch lasted for a fixed duration of less than one day, in some cases we did not complete all chains, and a few chains had fewer iterations (3 for vegetables, 6 for animals and 2 for furniture, out of 120 chains each). Our experiment incentivized participants to provide new tags by paying them a performance bonus of 0.01 USD for every up-vote (i.e., not flagged) given by other participants. On the contrary, if two or more tags of the same participant were flagged by others, the participant was excluded (the participant received a warning after the first flag). We provide summary statistics on the number of collected tags in Table 2. After accepting the consent form and passing the pre-screening tasks, participants received introduc- tory instructions regarding the main experiment: Images . Rate & Tag animals/furniture/vegetables! Thanks for participating in this game! In this game you will: 18 \u2022 Watch images of animals/furniture/vegetables. \u2022 Rate tags that other players have given. \u2022 Add new tags that you think are missing. Audio . Rate & Tag emotions! Thanks for participating in this game! In this game you will: \u2022 Listen to a speech fragment and focus on the emotional content of the record- ing. \u2022 Rate tags that other players have given. \u2022 Add new tags that you think are missing. Video . Rate & Tag activities! In this game you will: \u2022 Watch a video and focus on the activities happening. \u2022 Rate tags that other players have given. \u2022 Add new tags that you think are missing. Participants then received further instructions regarding the rules of the game Images . After watching the animal/furniture/vegetable you will see tags given by other players that describe the animal/furniture/vegetable. You should rate the relevance of each tag by clicking the appropriate amount of stars (1 star not very relevant, 5 stars very relevant). If you think that the tag is a mistake or completely irrelevant, you should flag it by clicking the flag icon. If you are the first person seeing this animal/furniture/vegetable, you may see no previous tags. You can also add your own tag that is relevant to describe the animal/furniture/vegetable. Your tag will then be rated by other players who are playing the game simultaneously. Audio . After listening to the recording, you will see tags given by other players that describe the emotions in the speech fragment. You should rate the relevance of each tag by clicking the appropriate amount of stars (1 star not very relevant, 5 stars very relevant). If you think that the tag is a mistake or completely irrelevant, you should flag it by clicking the flag icon. If you are the first person listening to this speech sample, you may see no previous tags. You can also add your own tag that is relevant to describe the emotions in the speech fragment. Your tag will then be rated by other players who are playing the game simultaneously. Video . After watching the video, you will see tags given by other players that describe the activities in the video. You should rate the relevance of each tag by clicking the appropriate amount of stars (1 star not very relevant, 5 stars very relevant). If you think that the tag is a mistake or completely irrelevant, you should flag it by clicking the flag icon. If you are the first person watching this video, you may see no previous tags. You can also add your own tag that is relevant to describe the activities in the video. Your tag will then be rated by other players who are playing the game simultaneously. Finally, participants received the following guidelines regarding the tag input and the bonus scheme: Keep tags short. A word like \u201cgreen grass\" should rather be submitted as \u201cgreen\" and \u201cgrass\", whereas a compound word such as \u201cred wine\" cannot be separated, since \u201cred wine \" means something different than just \u201cred\" and \u201cwine\". Bonus rules. \u2022 If the tag you provide gets rated as a relevant tag (i.e., not flagged) by other players \u2022 If your tag is unique and have not been introduced by others 19 Note: Simply writing many and irrelevant tags is not a good idea because other players might flag your tag. Your experiment will terminate early if there are too many red flags! Please try to use a variety of words to describe the animal / furniture / vegetable / emotion in the speech fragment / activities in the video, and use the entire star rating scale for your responses.\n\nB.6.1 V ALIDATING STEP-T AG\n\n We conducted a small, exploratory ablation study to validate STEP-Tag as a procedure for collecting diverse, accurate, and informative tags. First, we compared using multiple tags from the last iteration of STEP-Tag to using just a single randomly-selected highly-rated tag from the last iteration. We found that using a single tag greatly decreased correlation with human similarity (i.e., for the video dataset, the best-performing method on multiple tags had a correlation of r = 0 . 74 while the best- performing method on single labels had a correlation of r = 0 . 35 ). Second, we compared tags from the first iteration of STEP-Tag (equivalent to collecting tags without an adaptive procedure) to tags from the last iteration. We found that using first iteration tags greatly decreased correlation with human similarity (i.e., for the video dataset, the \u2018Tags CNNB mean (no split)\u2019 method, the correlation from the last iteration was r = 0 . 74 and from the first iteration it was r = 0 . 44 ; for \u2018Tags overlap\u2019 it was r = 0 . 56 from the last iteration and r = 0 . 38 from the first iteration). Finally, we extracted the Kinetics-200 labels for each video to compare the tags from STEP-Tag against the kinds of labels typically collected for machine learning datasets. We found that using labels decreased the correlation with human similarity (i.e., the best-performing method on pipeline tags had a correlation of r = 0 . 74 while the best-performing method on dataset labels had a correlation of r = 0 . 64 ).\n\nC P REDICTION METHODS\n\n We used two main types of methods to predict human similarity judgments. The first class (\u201cDNN- based methods\", described in section C.1) make use of pre-trained embedding models. In the second class of models (\u201cWord Frequency Analysis methods\", described in the section C.2) simple feature extraction techniques are used instead of pre-trained deep learning models. Figure 1 depicts schematic overview of all prediction methods that we used.\n\nC.1 DNN- BASED METHODS\n\n The DNN-based methods use various embeddings and deep learning representations to predict human similarity judgments. These methods could be further split into three groups based on the kinds of input data they process, namely if they use a single sensory modality that is either image, audio or video (\u201cunimodal models\"; see subsection C.1.1), or use text that is either tag or captions (\u201ctext embeddings\"; see subsection C.1.2), or use both (\u201cmultimodal models\"). In addition, we also tested the performance of \u201cstacked\" representations, where the sensory and textual embedding of a select number of models were concatenated into a single long embedding. Overall, the computation time of embedding methods took about two weeks on an x1.16xlarge Amazon Web Services instance with 64 vCPUs and 976 GiB of memory.\n\nC.1.1 U NIMODAL DNN- BASED METHODS\n\n Image models We used 560 pre-trained models from the Pytorch Image Models ( timm ) repository (Wightman, 2019). We chose this repository as it contains an extensive and highly diverse set of pre-trained models in terms of architecture backbones, model sizes, and training sets. The repository includes models published from 2014 to 2022 that use various training sets (such as ImageNet1k, ImageNet21k, Instagram, etc.), training procedures objectives (e.g., pre-training, fine-tuning, self- supervision, weak supervision, etc.) and architectures (e.g., VGG, ResNet, Inception, Transformer, etc.). The repository also reports various evaluation metrics for each model (e.g., their ImageNet performance). For each model, we computed the embedding from the last layer (typically before the final softmax layer; see below and Figure 10 for a preliminary analysis for the effect of layer depth in audio 20 Note. Performance accuracy on ImageNet was based on Wightman (2019) and was not available for all models. models). We then computed the cosine similarity between pairs of embedding vectors to produce a similarity matrix. The entire list of the performance of all models is detailed in the OSF repository associated with this paper 7 . Table 3 presents additional details for the top 42 image baseline models in Figure 3A including their average score (correlation to human judgments) across the three image datasets, the standard deviation (SD) of this score (across datasets, repeated runs and available model parameters in Wightman (2019)), their ImageNet accuracy, and their number of trainable parameters. Figure 4A shows the correlation to human similarity as a function of the number of parameters for all 569 models. In general, we found that models that have more parameters perform better (Figure 4A). Plotting all the embedding technique correlations against the number of training parameters of their respective models showed statistically significant positive correlation ( r = 0 . 39 , p < 0 . 001 ). However, one possible explanation for this could be the improved performance of newer models, which typically have more parameters, on various computer vision tasks. To test this, we computed the performance (i.e., correlation with human similarity) of the various models as a function of their accuracy on ImageNet (Deng et al., 2009) - which was provided in Wightman (2019) for all models except for CLIP (whose implementation came from a different repository) as summarized in Figure 4B. We found a positive correlation between the two metrics ( r = 0 . 26 , p < 0 . 001 ), though with some clear exceptions. For example, the vision transformer BEiT (Bao et al., 2021) and the convolutional architecture EfficientNet (Tan & Le, 2019) achieved high accuracy on ImageNet but 7 https://osf.io/kzbr5/?view_only=3dea58e008ce41c290ef0f374bdbf444 21 performed poorly on human data. On the other hand, the vision transformer Swin (Liu et al., 2021) and the convolutional architecture ConvNext (Liu et al., 2022) both performed well on ImageNet and human similarity. This suggests that architecture and number of parameters are better predictors of similarity judgments than performance on ImageNet. Further analysis is required to determine what kind of architectural components actually contribute to more human-like performance (Langlois et al., 2021). superb, emotion) 0.34 95 21 wav2vec 2.0 base (superb, speaker) 0.34 95 22 wav2vec 2.0 base (100h) 0.32 95 23 HuBERT large (superb, emotion) 0.29 300 24 HuBERT large (superb, speaker) 0.29 300 25 wav2vec 2.0 large (100h) 0.32 317 26 wav2vec 2.0 large (superb, emotion) 0.31 317 27 wav2vec 2.0 large (superb, speaker) 0.31 317 28 wav2vec 2.0 large (960h) 0.31 317 29 wav2vec 2.0 large (10m) 0.31 317 30 data2vec audio large (960h) 0.31 313.28 31 data2vec audio base (100h) 0.23 313.28 32 data2vec audio large (100h) 0.23 313.28 33 data2vec audio large (10m) 0.21 313.28 34 wav2vec 2.0 (SpeechBrain, emotion) 0.11 95 35 data2vec audio base (960h) 0.16 93.16 36 data2vec audio base (10m) 0.15 93.16 Audio models We used all pre-trained wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) models available in torchaudio (Yang et al., 2021). We also extracted embeddings from WavLM (Chen et al., 2021) and data2vec audio models (Baevski et al., 2022). Furthermore, we used additional wav2vec 2.0 and HuBERT models that were either specialized on emotion recognition or speaker identification (wen Yang et al., 2021; Wagner et al., 2022; Ravanelli et al., 2021). The performance of HuBERT, wav2vec 2.0, and WavLM models is shown in Figure 3B. Additional details about the models are displayed in Table 4. In addition, we explored the correlation between the audio models and human similarity data as a function of the layer in the model. Earlier literature has suggested that similarity to human 22 representations may depend on the layer of the model (Kell et al., 2018; Yamins et al., 2014; Yamins, 2020). We expected that the layers closer to the input of the model (where the representation is more low-level) to be less predictive. In general, we found that this was the case (Figure 10). In some variants of wav2vec, however, intermediate representations performed better, possibly due to the misalignment of the training task of wav2vec with the emotion task. This analysis confirms the choice we made in the paper to mostly use the last two layers of the models. Preliminary analysis of the image and video models also explored different layers, but the results were similar to those we presented in audio, and are therefore not reported here. Video models We extracted embeddings from the \u2018Slow\u2019 (a 3D ResNet; see Feichtenhofer et al. (2019)), Slowfast (a 2-path model with one path capturing semantics and the other capturing fine details; see Feichtenhofer et al. (2019)), and X3d (a model that initially start as a simple 2D image classifier but is expanded in several axes; see Feichtenhofer (2020)) architectures implemented in pytorchvideo (Fan et al., 2021). All video models were pre-trained on the Kinetics-400 dataset (Kay et al., 2017). The performance of the models is displayed in Figure 3C. Numeric correlation values are detailed in Table 5 along with model accuracy (Top1 and Top5) on Kinetics-400, and the number of parameters in each model. The accuracies and parameter counts are listed as reported in Fan et al. (2021). As with previous modalities, the number of parameters appears to be positively correlated with correlation to human similarity.\n\nT EXT EMBEDDING METHODS\n\n Caption text embedding. Since there are multiple captions per stimulus, an aggregation procedure had to be applied to produce a single embedding vector for each stimulus. In our main analysis, 23 for each stimulus, we extracted the embedding for each associated caption and averaged these embeddings together before computing cosine similarity between the mean embeddings. We also tried an alternative approach of concatenating the captions together into a single paragraph, which we then passed through the LLMs to compute a single embedding per stimulus. We found that this did not consistently improve performance and in many cases even decreased it, though we note that we did not experiment with different permutations of the concatenated captions, nor did we extensively study other ways to combine them together. Future work could explore other techniques for pre-processing captions and aggregating representations from multiple captions in ways that would improve correlation with human similarity judgments. Tag text embedding. We experimented with several algorithms for computing similarity between sets (or multi-sets) of tags. The algorithms described in this section all involve using ConceptNet NumberBatch (CNNB) (Speer et al., 2017) as the embedding backbone for turning discrete tags into continuous vector representations. For each stimulus, we took the tags remaining in the final iteration, and tested whether they were found in the dictionary for our embedding model. If a tag was not found and if it contained no spaces, we tried to correct the spelling before trying to look it up in the dictionary again. If a tag contained spaces, we would split it into individual words, correct their spelling, and average together the embedded representations of those words that were found in the dictionary. Tags that were not found even after spelling correction and splitting were excluded from the set and did not contribute to the final representation. For the methods marked \u2018(no split)\u2019 we did not split multi-word tags, instead we just excluded multi-word tags that were not found in the embedding model dictionary. In the following, we describe the different techniques used to generate predictions based on tag embeddings. Tags CNNB overlap. For each pair of stimuli, we counted the number of \u2018almost identical\u2019 tag embeddings, defined as every respective element of the two embeddings being less than a certain threshold apart (in our case, this threshold was 0.1). We then set similarity for that pair of stimuli to be this count, i.e., the number of \u2018almost identical\u2019 tags, normalized by the total number of tags across the respective two sets. Tags CNNB quantized. This method involves quantizing tags using cosine similarity to find the number of unique tags. For each pair of stimuli, we counted the number of tags assigned to the first stimulus that had cosine similarity greater than a certain threshold (in our case, this threshold was 0.7) to at least one tag of the second stimulus (call this value N A ) and vice-versa ( N B ). The minimum of these two values is the number of unique, shared tags between the two sets ( min( N A , N B )). The total number of unique tags across the two sets is then the total number of tags in each set ( T A + T B ) minus the maximum number of shared tags ( max( N A , N B ) ). We compute similarity as the ratio of the number of unique, shared tags to the total number of unique tags, S AB = min( N A ,N B ) T A + T B \u2212 max( N A ,N B ) . For example, suppose the two sets of tags are A : { a, b, c, g } and B : { a, b, d, e } , so T A = T B = 4 , and that a, c have cosine similarity of 0.8. The number of tags from set A found in set B is N A = 3 , and those from B found in A is N B = 2 . The number of unique, shared tags is min( N A , N B ) = 2 (since { a, b, c } can be represented by { a, b } ), and the total number of unique tags is 4 + 4 \u2212 3 = 5 (since { a, b, c, g, a, b, d, e } can be represented by { a, b, d, e, g } ). The assigned similarity is then S AB = 25 . Tags CNNB mean. The set of tag embeddings for each stimulus were averaged together to form a single embedding assigned to the respective stimulus. We then computed cosine similarity on the embeddings of each pair of stimuli. Tags CNNB mean (no split). Same as above, but without splitting multi-word tags (i.e., ones that contain spaces) during the embedding process. All spelling corrections in these algorithms were performed using the Python package pyspellchecker 8 , taking the top corrected recommendation returned by the spell checker in each case.\n\nC.2 W ORD F REQUENCY A NALYSIS METHODS\n\n In this work, we also conducted an additional evaluation of prediction models beyond embedding- based techniques (described in the previous section). Specifically, we compared the predictions 8 https://pyspellchecker.readthedocs.io/en/latest/ 24 of embedding-based models, which utilize deep learning representations, with those of traditional methods of text mining. Before the word frequency analysis, we performed the following initial pre-processing steps \u2022 For caption data, we concatenated all the captions describing the same stimulus into a single long \u201cdocument.\u201d \u2022 For tag data, we wanted to prioritize tags that appeared earlier in the tag-mining chains and were rated higher. To that end, we gathered all tags from all iterations and duplicated tags from a given iteration based on the ratings they received. For example, if the tag \u201ctomato\u201d received three stars, then we would add the repeated tokens \u201ctomato, tomato, tomato\u201d to the aggregated list (\u201cdocument\u201d). In a given iteration, flagged tags are removed, but if they are rated later, then they are included. The total number of repetitions per token is equal to the sum of all the stars they received in all iterations. As a result, each token is repeated multiple times, which we take into consideration in consequent analysis. For the next steps, we used the Matlab text analytics toolbox 9 . Unless otherwise specified, we used default parameters for all functions. To generate similarity matrices, we applied the following methods: Co-occurrence method . In this approach, we simply counted the number of repeated pairs of words in documents i and j and normalized by the total number of pairs. Formally, we use w i to denote the word list of a document i . Let w i,k be the k -th word in the w i list of words, and let | w i | denote the length of the list. We denote by \u03b4 ( c, d ) the indicator function that returns 1 if and only if the word c is identical to the word d , and 0 otherwise. We computed the co-occurrence score S ( w i , w j ) according to the following formula: We suggest using this method only with tags and not with captions. Co-occurrence-rep . This method was applied only to tags. We used an identical procedure to the Co-occurrence method, except that we did not separate the words within a tag as separate tokens and instead treated the entire tag (that may include multiple words) as a single token. Rouge score . In this approach, similarity was estimated by computing the rouge score of the word lists associated with each pair of documents. The Rouge score was computed using rougeEvaluationScore (Rouge, 2004). We suggest using this method only with tags and not with captions. The following methods make use of tokenized data and a pre-processing procedure that we found effective. Pre-processing was applied to both tag and caption data and tokenization was performed as follows: \u2022 We separate all text into single words by applying the tokenizedDocument function. \u2022 We added part of speech information using the addPartOfSpeechDetails function. \u2022 We performed Lemmatization using the normalizeWords function. \u2022 We erased punctuation from the token using the erasePunctuation function. \u2022 We removed stopwords using the removeStopWords function. \u2022 We removed words with less than two characters or more than 15 characters. \u2022 We created a bag of words representation of each tokenized document using the bagOfWords function. \u2022 We also removed words that were not present in more than two documents using the InfrequentWords function. With the results of these pre-processing steps, we then computed similarity matrices based on the following methods: 9 https://mathworks.com/products/text-analytics.html 25 bm25S . We used bm25+ to compute similarity between documents (Barrios et al., 2016) using Matlab\u2019s bm25Similarity function. This function represents TF-IDF-like retrieval functions used in document retrieval. We used a variant that has a normalization function that properly handles documents with a long list of words. tf-idf-cosine . We computed pairwise cosine similarities between document pairs using the TF-IDF matrix derived from their word counts and Matlab\u2019s cosineSimilarity function. 26", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper compares various methods for efficiently approximating human similarity judgments at scale.", "topic_class_1": "Summary of topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors evaluate over 600 pre-trained deep neural network (DNN) models across images, audio, and video modalities to assess their alignment with human judgments.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They find a significant performance gap between these DNN methods and collecting exhaustive pairwise human judgments.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "To address this, they propose novel methods for approximating similarity using natural language, either via large language models (LLMs) applied to stimulus descriptions or classical word frequency analysis (WFA) on tags/captions.", "topic_class_1": "Summary of method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They also introduce a new adaptive tag collection pipeline called STEP-Tag.", "topic_class_1": "Summary of method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The language-based methods significantly improve over the DNN methods while requiring only O(N) human judgments compared to O(N^2).", "topic_class_1": "Summary of method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Combining the best DNN and language embeddings via \"stacking\" provides the closest approximations to human judgments overall.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Based on their results, the authors provide a best practices guide for collecting similarity data under different constraints.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Conducts the first large-scale, systematic evaluation of how well various DNN models approximate human similarity judgments across multiple modalities", "topic_class_1": "Strength on the experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Introduces an adaptive tag collection paradigm (STEP-Tag) to obtain informative tags", "topic_class_1": "Strength on the method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Compares unimodal, multimodal, and hybrid \"stacked\" approaches to integrating DNN and language information", "topic_class_1": "Strength on the experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Synthesizes findings into a practical flowchart to guide researchers in collecting similarity data", "topic_class_1": "Strength on the method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Evaluation is limited to three modalities (images, audio, video); extending to other domains like text or graphs would further demonstrate the generality of the language-based methods", "topic_class_1": "Concern on the limited modalities and domains", "topic_class_2": NaN, "reliability": "No", "explanation": "The modalities this paper have included is wide enough (video, image, audio). One can always claim that a \"generic\" method is desired, but that does not weaken the contributions they made in this work.", "error_type": "Out-of-scope"}, {"segment_text": "- Additional analysis of what factors enable certain DNN architectures to better capture human similarity could provide further insights", "topic_class_1": "Concern on the limited analysis", "topic_class_2": NaN, "reliability": "No", "explanation": "Not the focus of this work.", "error_type": "Out-of-scope"}, {"segment_text": "- Discussion of potential risks (e.g.", "topic_class_1": "Concern on the ethical issues", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "encoding human biases) is quite brief", "topic_class_1": "Concern on the ethical issues", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Strength on the writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are rigorous and comprehensive, evaluating a very large number of models and ablating key components of the proposed methods.", "topic_class_1": "Strength on the experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The language-based similarity approximation methods, especially the stacking approach, represent a novel contribution that significantly improves efficiency over prior work.", "topic_class_1": "Strength on the proposed method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By open-sourcing their code and data, the authors have made their results fully reproducible.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes valuable contributions to the important problem of efficiently collecting human similarity judgments at scale.", "topic_class_1": "Summary of topics", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The systematic evaluation of DNN methods reveals their limitations, while the proposed language-based approaches, combined with the useful best practices guide, provide an effective and practical way forward.", "topic_class_1": "Summary of strengths", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some additional evaluation, analysis and discussion of risks could further strengthen the work, but overall this is a significant and well-executed study.", "topic_class_1": "Summary of weaknesses & overall summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper evaluates the effectiveness of pre-trained deep neural networks (DNNs) in approximating human similarity judgments across images, audio, and video modalities, revealing a substantial gap between DNN and human performances.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "To bridge this gap, the authors introduce language-based methods for similarity approximation, leveraging an adaptive tag collection pipeline (STEP-Tag) for efficient data gathering.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They further enhance approximation accuracy by combining language embeddings with DNN embeddings (\"stacked\" methods), demonstrating superior performance across all tested modalities.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study provides a comprehensive guide for researchers on approximating human similarity judgments efficiently, supported by a large dataset of 206,339 human judgments and a detailed analysis of various modeling approaches.", "topic_class_1": "Summary of contributions", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive Evaluation: The evaluation of 611 pre-trained models across three distinct modalities presents a thorough and broad analysis, showcasing the extensive work undertaken.", "topic_class_1": "Strength on the experiment", "topic_class_2": "Strenghts, experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Innovative Language-based Methods: The introduction of language-based methods for similarity approximation, alongside the novel STEP-Tag pipeline, represents a significant contribution to the field, particularly in terms of efficiency and scalability.", "topic_class_1": "Strength on the method", "topic_class_2": "Strengths, mixing vision and language", "reliability": "No", "explanation": "This part is trivial from the NLP point of view, regardless of whether the authors call it \"novel.\" I don't see much novelty here (transform other modalities into language and use the language version to calculate similarity).", "error_type": "Misinterpret Novelty"}, {"segment_text": "High-Quality Dataset: The large dataset of human judgments collected and released provides a valuable resource for future research, enhancing reproducibility and further study.", "topic_class_1": "Strength on the resource", "topic_class_2": "Strengths, dataset", "reliability": "No", "explanation": "True, but there is no discussion of agreements. COllecting 5 annotations via crowdsourcing is not difficult, doing it properly could be.", "error_type": "Inexpert Statement"}, {"segment_text": "Practical Guide for Researchers: The concise guide based on experimental results is a practical tool for researchers facing limitations in judgment collection or model access, facilitating more effective research design.", "topic_class_1": "Strength on the findings", "topic_class_2": "Strengths, practical aspects", "reliability": "No", "explanation": "It could make sense, but the decisions seem unsupported and in any case a direct interpretation of their results.", "error_type": "Superficial Review"}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Model-Specific Analysis Limitations: The paper's focus on pre-trained models might limit its applicability to domains where such models are not available or suitable.", "topic_class_1": "Concern on the model availability", "topic_class_2": "Weaknesses.- requirements", "reliability": "No", "explanation": " it is obvious; it applies to any paper working with LLMs", "error_type": "Superficial Review"}, {"segment_text": "Additionally, the performance gap noted between human judgments and DNNs might be model-specific rather than a general limitation of DNNs.", "topic_class_1": "Concern on the generality of claims", "topic_class_2": "Weaknesses - experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Potential Bias in Language Data: The reliance on language-based methods might introduce biases related to language use and representation, which the paper does not fully address in terms of potential impacts on similarity judgments.", "topic_class_1": "Concern on potential biases introduced", "topic_class_2": "Weaknesses - bias", "reliability": "No", "explanation": " true of any paper using LLMs", "error_type": "Superficial Review"}, {"segment_text": "Ethical Considerations: While the paper briefly mentions the risk of embedding human biases in ML datasets, a deeper analysis of ethical implications, especially concerning the use of language data and the broader societal impacts of these biases, would strengthen the paper.", "topic_class_1": "Concern on the limited ethical discussion", "topic_class_2": "Weaknesses - bias", "reliability": "No", "explanation": "This is a stretch in my opinion. They may as well complain about the fact that the authors used electricity to fo their work.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity and Quality: The paper is well-structured and clear, with a strong emphasis on methodological rigor and comprehensive data analysis.", "topic_class_1": "Strength on the writing", "topic_class_2": "clarity / quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The quality of the research is evident in the extensive dataset and detailed experimental setup.", "topic_class_1": "Strength on the quality of the paper", "topic_class_2": "quality", "reliability": "No", "explanation": "Trying 611 in a brute force fashion looks overkill to me, but I guess others think it is great", "error_type": "Unstated statement"}, {"segment_text": "Novelty: The novel use of language-based methods and the STEP-Tag pipeline for approximating human similarity judgments is a significant and innovative contribution to the field.", "topic_class_1": "Strength on the novelty", "topic_class_2": "novelty", "reliability": "No", "explanation": "I see little novelty here. An iterative approach looks like the natural lway to refine, and the authors cite plenty of work that they borrow from. Perhpas the application to this specific task (collecting tags) could be novel, but saying \"we apply X\" rather that \"we propose the novel X\" would be accurate. Specially since the authors state that X comes from other papers.", "error_type": "Misunderstanding"}, {"segment_text": "Reproducibility: With the release of the dataset and detailed descriptions of the methodologies and models used, the study sets a high standard for reproducibility.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": "reproducibility", "reliability": "No", "explanation": "the experimetns are trivial. The important stuff is the dataset.", "error_type": "Experiment"}, {"segment_text": "Summary of the Review", "topic_class_1": "Section title", "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a comprehensive and innovative approach to approximating human similarity judgments using pre-trained DNNs and novel language-based methods.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The strengths lie in the extensive model evaluation, the innovative methodology for data collection, and the practical guide for researchers.", "topic_class_1": "Summary of strengths", "topic_class_2": "Summary - strenghts", "reliability": "No", "explanation": "Once again, I see little novelty", "error_type": "Duplication"}, {"segment_text": "However, the study could be improved by addressing potential biases introduced through language data and by expanding the discussion on the ethical implications of their findings.", "topic_class_1": "Summary of weaknesses", "topic_class_2": "Summary - biases", "reliability": "No", "explanation": "They don't get credit for creating any model, only for using them. Their framework is not about creating any models. According to the reviewer, this comments applies to any paper using any LLM (or anything pretrained fro that matter)", "error_type": "Unstated statement"}, {"segment_text": "Overall, the contributions are significant, providing valuable insights and resources for the NLP and broader AI research community.", "topic_class_1": "Summary of evaluations", "topic_class_2": "Summary - overall", "reliability": "No", "explanation": "I disagree with the content, but it is subjective.", "error_type": "Misunderstanding"}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " YES", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the use of language as a proxy for human similarity judgments, which are valuable for various machine learning applications but expensive to collect at scale.", "topic_class_1": "Summary of topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors compare several approaches, including using pre-trained deep neural networks (DNNs), large language models (LLMs) applied to textual descriptions, and hybrid \"stacked\" methods combining both.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They also introduce STEP-Tag, a novel adaptive pipeline for collecting high-quality word tags.", "topic_class_1": "Summary of method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Evaluating these methods across image, audio, and video datasets, the authors find that stacked representations best approximate human similarity, followed by text-based methods and then DNNs.", "topic_class_1": "Summary of experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They conclude by providing a guide for researchers to choose the most suitable method for their specific needs and resource constraints.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive comparison: The paper provides a valuable and extensive comparison of different methods for approximating human similarity judgments, encompassing DNNs, LLMs, and hybrid approaches.", "topic_class_1": "Strength on the experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel methods: The proposed language-based methods, particularly the use of LLMs and STEP-Tag, offer efficient and modality-agnostic alternatives for approximating similarity, especially when pre-trained DNNs are unavailable.", "topic_class_1": "Strength on the method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open data and code: Sharing the collected data and code promotes transparency and facilitates further research in this area.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited interpretability analysis: While interpretability is claimed as a key advantage of the proposed methods, the paper lacks a dedicated analysis to support this claim. Further investigation is needed to assess how interpretable the generated prompts and representations actually are.", "topic_class_1": "Concern on the limited analysis", "topic_class_2": NaN, "reliability": "No", "explanation": "The authors did not claim that interpretability is a key advantage of the methods, so no need to provide analysis on that. ", "error_type": "Unstated statement"}, {"segment_text": "Focus on relatively simple tasks: The experiments primarily focus on tasks with easily defined prompts, such as emotion recognition and activity recognition. Evaluating the methods on more complex NLP tasks would provide stronger evidence for their ability to generate and understand complex prompts.", "topic_class_1": "Concern on the complexity of tasks", "topic_class_2": NaN, "reliability": "No", "explanation": "This work focuses on simulating similarity predictions between video/image/audio pairs. They uses language as a medium to develop methods. Evaluating the methods on NLP tasks is kind of out of scope of this paper.", "error_type": "Out-of-scope"}, {"segment_text": "Lack of comparison with recent prompting methods: The paper does not compare with some recent methods for finding better discrete prompts, which would be helpful to gauge the relative performance of the proposed methods.", "topic_class_1": "Concern on the experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is generally well-written and clear, although some details about the experimental setup could be further clarified.", "topic_class_1": "Strength on the writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-designed and executed, with a comprehensive evaluation across different modalities and methods.", "topic_class_1": "Strength on the evaluation", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The proposed language-based methods and the STEP-Tag pipeline are novel and offer valuable contributions to the field.", "topic_class_1": "Strength on the novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors provide code and data, which facilitates the reproduction of their results.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a valuable contribution to the field of human similarity approximation.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed language-based methods and the STEP-Tag pipeline offer promising alternatives for efficiently collecting and approximating similarity judgments, particularly in situations where pre-trained DNNs are unavailable.", "topic_class_1": "Summary of methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, further investigation is needed to assess the interpretability of the generated representations and the performance on more complex NLP tasks.", "topic_class_1": "Summary of weaknesses", "topic_class_2": NaN, "reliability": "No", "explanation": "This statement repeats the weaknesses above. The focus of this work is not on the NLP side. Assessing the interpretability is not one of the major focuses.", "error_type": "Duplication"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Pretrained Language Model in Continual Learning: A Comparative Study", "body_text": "\n\nP RETRAINED L ANGUAGE M ODEL IN C ONTINUAL L EARNING : A C OMPARATIVE S TUDY\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n\n\nContinual learning (CL) is a setting in which a model learns from a stream of in- coming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many con- tinual learning methods and PLMs, understanding and disentangling their inter- actions become essential for continued improvement of continual learning perfor- mance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 veins of CL methods on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interest- ing, and sometimes surprising, performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs\u2019 performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques.\n\n\n\n1 I NTRODUCTION\n\n Continual Learning (CL) methods aim at training a model from a stream of non-i.i.d. samples, relieving catastrophic forgetting (CF) while limiting computational costs and memory footprint. Throughout the years, many methods have been proposed to address the CL problem in computer vision and robotics (Kirkpatrick et al., 2017; Serr ` a et al., 2018; Buzzega et al., 2020). CL in NLP is still a nascent topic, as re\ufb02ected by the relatively smaller number of proposed methods (Biesialska et al., 2020). However, it is not always easy to precisely measure the merits of these works. This is partly due to the subtle differences in the way methods are evaluated: many state-of-the-art ap- proaches only stand out in the setting where they were originally conceived. Moreover, pretrained language models (PLMs) have been widely applied in CL methods in NLP, and their addition further complicates the clear understanding of model performance (Han et al., 2020; Wang et al., 2019; Cao et al., 2020). Although many works that apply PLM in CL point out the CF problem present in PLMs, there remain three signi\ufb01cant issues that require further investigation. (1) Many existing CL works em- ploy speci\ufb01c PLMs such as BERT (Devlin et al., 2019), whereas more recent and sophisticated PLM structures, e.g. ALBERT (Lan et al., 2020) and XLNET (Yang et al., 2019), have been de- veloped afterwards. A natural question would be whether the later PLMs could better mitigate CF in continual learning (Mosbach et al., 2021; Lee et al., 2020a). (2) The existing works focus on input, output, and gradients information of PLMs while ignoring the effect of model architecture on CF (Biesialska et al., 2020). We believe opening up the black-box of PLMs may lead to a deep un- derstanding of their characteristics and thus better algorithmic design (Wallat et al., 2020). (3) Some existing works focus on alleviating forgetting in a trivial setting, during knowledge transfer between two tasks, which is not realistic in general continual learning with more tasks or even without task boundary (Jiang et al., 2020). In this paper, we conduct an in-depth exploration of continual learning through extensive empirical analysis of a number of PLMs and CL methods. We analyze not only the performance differences across the combinations of PLMs and CL methods, but also the idiosyncrasies of each PLM. 1 Our main contributions are as follows: \u2022 We design a rigorous benchmark for comprehensive study of continual learning in NLP. We conduct experiments over (1) two primary continual learning setting, including task- incremental learning and class-incremental learning; (2) three benchmark datasets with different data distributions and task de\ufb01nitions, including relation extraction, event clas- si\ufb01cation, and intent detection; (3) four veins of CL methods with six baseline methods implemented for systematic comparison; and (4) \ufb01ve pretrained language models. \u2022 We evaluate and contrast the performance of the above combinations of settings, PLMs and CL methods, providing a comprehensive comparative study from a number of perspectives. \u2022 We dissect the performance characteristics of different PLMs with a number of layer-wise probing analyses. \u2022 Our observations and insights give rise to a number of open research questions that can guide the design and optimization of better PLM-oriented continual learning methods and CL-oriented pretraining strategies. \u2022 To encourage more research on continual learning in NLP, we will release the code and dataset as an open-access resource.\n\n2 B ACKGROUND\n\n In this section, we provide the required background on pre-trained language models as well as continual-learning settings, methods and evaluation metrics.\n\n2.1 P RETRAINED L ANGUAGE M ODELS\n\n It now become the best practice to incorporate PLMs into NLP systems for many problems such as question answering, machine reading comprehension, summarization, to name a few. Usually, making use of PLMs in such systems leads to signi\ufb01cant performance gains in (weakly) supervised learning. In this work, we investigate the use of PLMs in continual learning. We now brie\ufb02y introduce \ufb01ve typical PLMs that are evaluated in our work. BERT (Devlin et al., 2019) is the most representative PLM, which uses bi-directional deep Trans- formers (Vaswani et al., 2017) as the backbone. BERT adopts the Masked Language Modeling (MLM) and the Next Sentence Prediction (NSP) as the self-supervision tasks for pre-training. ALBERT (Lan et al., 2020) ALBERT is a lite version of BERT which sets the parameters of all Transformer blocks shared across all layers and factorizes the embedding matrices into two small size of matrices. Instead of NSP, ALBERT predicts the order of two consecutive textual seg- ments. Although ALBERT utilizes signi\ufb01cantly less memory, \ufb01ne-tuning it on downstream tasks can achieve close performance to that of BERT. RoBERTa (Liu et al., 2019) RoBERTa has almost the same architecture as BERT, while it differs in terms of three training procedures. RoBERTa removes the NSP loss, trains a model with bigger size and longer sequences, and creates dynamic MLM masks vs static ones used in BERT. GPT2 (Radford et al.) Unlike the aforementioned PLMs which are all masked language models, GPT2 is an autoregressive language model predicting one token at a time from left to right. GPT2 is often used for natural language generation, while the aforementioned PLMs are mostly used for natural language understanding. XLNET (Yang et al., 2019) Unlike the masked language models, GPT2 can not utilize the context from the backward side. XLNET is an autoregressive language model as well, while it resolves this problem by adopting a new objective called Permutation Language Modeling, enabling the model to take advantage of both forward and backward contexts.\n\n2.2 C ONTINUAL L EARNING S ETTINGS\n\n Continual learning (CL) focuses on the development of learning algorithms able to accumulate knowledge on non-stationary data. CL approaches are benchmarked on their ability to learn a se- 2 quence of tasks without forgetting previously acquired knowledge. They are typically evaluated on incremental classi\ufb01cation settings. Some popular incremental learning scenarios in CL (van de Ven & Tolias, 2019; Zeno et al., 2018) include: class-incremental learning, domain-incremental learning, and task-incremental learning. In the training time of these incremental learning settings, a CL algorithm experiences each task sequentially (only once), and is informed about the distribution shift (aka the task boundary). They, however, differ in their assumptions about the evaluation. In task-incremental learning, the model relies on the task identity (or a task label) to make its prediction. Conversely, in class-incremental learning, the methods have to perform task inference, implicitly (Aljundi, 2019) or explicitly (Lee et al., 2020b). We will cover both settings in our study. A lesser known setting, namely Domain- incremental learning, also relaxes the task-ID dependence, but shares the same output head for each task. For more details on these settings, see (van de Ven & Tolias, 2019).\n\n2.3 C ONTINUAL L EARNING A PPROACHES\n\n Many continual-learning approaches have been proposed recently, eg see (Delange et al., 2021) for an overview, which can be categorised as follows: Rehearsal-based Approach The simplest way to reduce forgetting is to store samples from the past and reuse them to complement the learning of new tasks. In its simplest form, old samples are replayed with new data. This strategy, known as experience replay (ER), (Rolnick et al., 2019) is often a hard to beat baseline. Numerous rehearsal-based methods have been developed to increase ER\u2019s performance or ef\ufb01ciency (Aljundi et al., 2019a; Caccia et al., 2019; Hayes et al., 2018). Instead of replays, some methods use the old sample to perform constrained optimization to prevent increasing the loss on old tasks (Lopez-Paz & Ranzato, 2017; Aljundi et al., 2019b). Regularization-based Approach Also know as prior-based approaches, these methods prevent signi\ufb01cant updates to the parameters that are deemed important for the previous tasks. Their \ufb01rst in- stantiation appeared in the Elastic-Weight Consolidation (EWC) (Kirkpatrick et al., 2017), where the previously learned weights are restrained from drifting via an L2 regularization loss. Regularization- based strategy often rely on the task boundaries to consolidate their knowledge during training. They often fail on long tasks sequences or settings where the task identity is not observable (Farquhar & Gal, 2018; Lesort et al., 2019b; Chaudhry et al., 2018). Despite these \ufb01ndings, prior-focused meth- ods are actively researched (Zeno et al., 2018; Nguyen et al., 2018). Dynamic Architecture Approach Also know as parameter-isolation methods, this family of al- gorithm alleviates forgetting by using different subset of parameters for \ufb01tting different tasks. One popular approach, called Hard Attention to the Task (HAT) (Serr ` a et al., 2018), achieves parameter freezing through an attention mask that is learned concurrently at every tasks. Other similar strate- gies have also been proposed (Yoon et al., 2017; Schwarz et al., 2018). Similarly to regularization- based methods, dynamic architectures usually assume the availability of test-time task labels, and thus are not straightforwardly applicable in more realistic settings. Further approaches are detailed in Appendix A.\n\n3 B ENCHMARKING C ONTINUAL L EARNING OF PLM S\n\n In this section, we \ufb01rst compare the performance of different combinations of CL methods and PLMs on three benchmark datasets, and we explore the following three research questions: (1) Does the catastrophic forgetting problem exist in PLMs during continual learning? (2) Which continual learning approach is the most ef\ufb01cient for PLMs and why? (3) Which PLM is the most robust for continual learning and why?\n\n3.1 E XPERIMENTAL S ETUP\n\n Methods . We adopt 5 representative PLMs for evaluation, i.e., ALBERT (Lan et al., 2020), BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019), RoBERTa (Liu et al., 2019), and XL- NET (Yang et al., 2019). By combining a PLM and a linear classi\ufb01er as the backbone model, we 3 investigate the following 6 schemes for the comparative study: (1) Vanilla uses the model learned on previous tasks as initialization and then optimizes the parameters for the current task. This base- line greedily trains the model on each task without accessing data from previous tasks, and is thus severely prone to catastrophic forgetting. It serves as a weak lower bound in terms of the average accuracy. (2) Joint trains on the entire training set simultaneously in the conventional supervised learning setup. Therefore, Joint does not suffer from forgetting and represents the performance up- per bound. (3) EWC (Kirkpatrick et al., 2017) is a regularization-based methods, extending the loss function with a regularization term that slows down the updates of the important network weights. (4) HAT (Serr`a et al., 2018) is a dynamic architecture method, employing a heuristic strategy to pre- vent intransigence by allocating additional units to the network when needed. (5) ER is a rehearsal- based method, interleaving old samples with current data in training batches. (6) DERPP (Buzzega et al., 2020) is a hybrid method, combining the strategy of rehearsal and regularization which pre- vents the prediction logits of memorized samples from changing. Following DERPP (Buzzega et al., 2020), we simulate the sequence of tasks according to the class-incremental learning setting (i.e. Class-IL or CIL) and task-incremental learning setting (i.e. Task-IL or TIL), splitting the training dataset into partitions of classes/tasks. Training details and hyper-pearameters are in Appendix C Metrics . To measure the FWT and BWT abilities of the CL models, we assume access to a test set for each task. After the model learning on the training set of task t , we present the evaluation results on all T tasks. We adopt three evaluation metrics as in (Lopez-Paz & Ranzato, 2017): (1) Forward transfer FWT = 1 i =1 A T,i \u2212 A i,i ; And average accruacy Avg. ACC = 1 T (cid:80) Ti =1 A T,i , where A t,i is the accuracy of models on the test set of i th task after model learning on the t th task and \u02dc b i is the test accuracy for task i at random initialization. In addition to FWT and BWT, we consider average accuracy as a key performance measure. Datasets . We evaluate our methods on 3 datasets with distinct label distributions, covering the following domains. CLINC150 (Larson et al., 2019) is an intent classi\ufb01cation dataset with labels evenly distributed, including 150 classes and 100 instances per class. Maven (Wang et al., 2020) is a long-tailed event detection dataset with 163 classes, each of which is labelled at least 15 instances, reaching a total of 47,921 instances. WebRED (Ormandi et al., 2021) is a severely long-tailed relation classi\ufb01cation dataset with 243 classes, each of which is labelled with at least 15 instances, reaching a total of 31,441 instances. Adhering to (Wu et al., 2021), we randomly split each dataset into disjoint tasks, and each tasks contains 10 classes. For each class, we randomly split the dataset set into train, validation and test set by 10:2:3. To reduce the amount of computation, we cap the number of instances per class for both Maven and WebRED to 1000. The data distributions of the three datasets is visualized in Appendix B.\n\n3.2 R ESULTS AND D ISCUSSIONS\n\n The main evaluation results over all studied PLMs and CL methods, in terms of accuracy, are sum- marized in Table 1. For the Task-IL setting, Figure 1 shows a comparison between the PLMs (a\u2013d) and between CL methods (e\u2013h) in terms of accuracy, backward transfer, forward transfer, and train- ing time. Performance measure is averaged across the three datasets. From these results we can make the following observations on the three research questions that we posed previously. More analysis could be \ufb01nd in Appendix D and F. For PLMs, catastrophic forgetting is serious. For each PLM, the performance gap between Vanilla and Joint indicates the PLM\u2019s tendency to forgetting. For all PLMs on all three datasets, we can consistently observe signi\ufb01cant performance gaps between the two methods, where the gaps are even more pronounced in Class-IL. This observation indicates that, without specialised algorithms, a direct adoption of PLMs in a CL environment will result in severe performance penalty. Experience replay is the most robust method while regularization is not. Comparing the four dif- ferent CL methods, as shown in Figure 1(e\u2013h), we can observe that the methods adopting memory- replay (ER and DERPP) achieve much higher accuracy and higher backward transfer than the others (EWC and HAT). For dynamic architecture-based methods, HAT is the only method bene\ufb01ting from continual learning, as it shows the positive Forward Transfer (FWT) in Figure 2(c, g). However, the higher training time, the lower accuracy, and the requirement for task label (thus inapplicable for Class-IL) do limit the applicability of HAT. Counterintuitively, the regularization-based method 4 XLNET EWC shows the worst performance in our experimental setting. DERPP, which adopts both reg- ularization and experience replay, reach the highest accuracy on CLINC150. Surprisingly, it per- forms worse than ER on the other two imbalanced datasets (Figure 1(e\u2013f)), which indicates that regularization-based methods may not be as robust as those based on experience replay. BERT is still a good option for continual learning. Comparing different PLMs, there is no obvious difference on average accuracy and backward transfer, as shown in Figure 1(a, b). RoBERTa and GPT2 show the lowest FWT values in Figure 1(c), which means that their representative ability has been interfered the most during continual learning. For XLNET, Table 3 shows that it conducts the lowest computation per instance yet Figure 1(d) shows that it spends the longest time in training. In comparison, ALBERT has the least parameters and approx. equal time to obtain a competitive 5 performance. As shown in Figure 1(a-d), BERT is still a good choice for continual learning scenarios as it achieves the highest accuracy while being competitive on the other three metrics.\n\n3.3 N EW R ESEARCH Q UESTIONS\n\n The above results show that among all PLMs, BERT is the most robust, and that among all CL methods, those based on experience replay are the most performant.These observations lead to the following important research questions. (1) What happens inside the black box of BERT during continual learning? (2) What is the performance difference across PLMs and across the layers inside each PLM? (3) Why are replay-based methods more robust than regularization-based methods? (4) In which layers does replay make the most contributions? We further explore these research questions in Sections 4 and 5.\n\n4 M INING THE S ECRETS OF L AYERS IN BERT\n\n In this section, we dive into BERT to provide a detailed analysis on the severity of forgetting on each of its layers, addressing RQ (1) of Section 3.3. Performing such an analysis further allows us to gain an understanding of the possible reasons of the robustness of experience replay-based methods, addressing RQ (3) of Section 3.3. Generally speaking, the representative ability of a PLM increases with the increase of layers since such a bottom-up process can gradually gain more task-speci\ufb01c features. Hence, we put forward two possible reasons to catastrophic forgetting on BERT. (1) The continual learning problem interferes the inner representation ability of PLMs, which leads to catastrophic forgetting. (2) The continual learning of incremental tasks has minor in\ufb02uence on PLMs, but the classi\ufb01er layer is the bottleneck. To investigate the above assumptions, we designed the following probing experiments.\n\n4.1 P ROBING S ETTINGS\n\n To investigate the representation ability per layer of a PLM, we follow Saunshi et al. (2019) and pro- pose a prototype-based probing method (refer to E), which takes the layer-wise mean representation of instances belonging to the same class in the test set as the prototype representation of the class, and re-classify test instances by distance-based classi\ufb01cation for the layer. 6 Hence, we can use the mean classi\ufb01cation accuracy as a short-hand for the representative ability for each layer, and obtain T \u00d7 N results to track the performance per layer during continual learning, where T is the number of learned tasks, i.e., time steps, and N is the number of layers in a PLM, typically 12. To compare the the inner representative ability and the model output, we also track the prediction accuracy per task, which is denoted by clf . Note that, by comparing layer 12 and clf , we could learn the gap between the remaining representative ability of a PLM and the performance of a backbone model. To provide a fair comparison, we obtain the results for Joint by retraining a new backbone model from sketch at each time step, rather than retraining a model over seen data sequentially. Figure 2 shows the probing results, in which we probe BERT with Vanilla, Joint and two CL methods based on experience replay, namely ER and DERPP. The analysis is done in two settings, layer-wise and task-wise. Layer-wise Probing. In row 1, (a\u2013d), the x-axis is the layers of BERT, and each line represents the accuracy after the model is trained with task t , as measured by the testsets of tasks 1 up to t . Task-wise Probing. In row 2, (e\u2013h), the x-axis represents the tasks, and each line represents the representativeness of each inner layer as well as the backbone model ( clf ), as measured by the testsets of tasks 1 up to t after the model is trained with task t .\n\n4.2 R ESULTS AND D ISCUSSIONS\n\n Catastrophic forgetting occurs in both the last and middle layers. As shown in Figure 2(a\u2013d), com- paring the layer-wise performance of Vanilla to the other three methods, an obvious drop appears in Vanilla between layer 12 and the \ufb01nal classi\ufb01er layer for all tasks except task 1. This result in- dicates that although BERT still maintains a high representative ability at the last time step, yet the classi\ufb01er has already lost the ability to classify previously learned classes. Furthermore, as shown in Figure 2(e), the bottom (i.e. earlier) layers of BERT are consistently less representative than the top layers, and the drop is the most signi\ufb01cant in the 6th and the classi\ufb01er layers. Regularization of outputs cannot prevent the forgetting in the bottom layers. Besides experience replay, DERPP also adopts regularization on the \ufb01nal classi\ufb01er layer. Comparing ER and DERPP in Figure 2(f) and (g), DERPP can better constrain the clf layer to maintain the representativeness of BERT, but it is less effective than ER on constraining the drop of the middle layers. Such differences may partially explain why ER could be more robust than regularization-based methods. The layer-wise and task-wise probing experiments give rise to the following research questions that may shed lights on further improvements of CL methods. (1) The regularization of the middle layers may improve continual learning performance. (2) The classi\ufb01cation layer clf is typically the most fragile of BERT, where continual learning learning methods making sense.\n\n5 U NDERSTANDING THE E FFECTIVENESS OF E XPERIENCE R EPLAY\n\n In this section, we probe the possible reasons behind the effectiveness of experience replay. We do so by trying to understand why BERT is robust among all PLMs (RQ (2) in Section 3.3), and by analyzing the effect of replay on different layers of PLMs (RQ (4) in Section 3.3). See Appendix G for more results.\n\n5.1 P ROBING S ETTING\n\n We extend the probing method in the above section to the following two probing settings. Layer-wise Probing for Vanilla. As shown in Figure 3(a\u2013d), we evaluate the layer-wise represen- tativeness of different PLMs (except BERT) with Vanilla. Averaged Layer-wise Probing for ER with Various Buffer Size. Figure 3(e\u2013h) shows the mean accuracy per layer across time steps for ER with different buffer sizes. Doing so allows us to inves- tigate on which layers do experience replay make the most contributions. 7\n\n5.2 R ESULTS AND D ISCUSSION\n\n The layer-wise representativeness varies signi\ufb01cantly across PLMs. As shown in Figure 3(a-d), although the prediction of clf is similar across PLMs, the performance characteristic of each PLM on their inner layers varies a lot. For example, due to its parameter-sharing mechanism, the hidden layers of ALBERT are more fragile than BERT (Figure 2(a)) and RoBERTa (Figure 3(c)), with larger accuracy gaps across tasks. Figure 3(e-h) shows that which layers bene\ufb01t the most from the increase in buffer size differs across PLMs. ER improves the performance of layer 12 and clf layers in XLNET, but for RoBERTa and GPT2 it is mainly the clf layer. Compared with the classi\ufb01cation layer, the representation of some inner layers maintains a high performance. We notice a similar layer-wise performance curve in both BERT and RoBERTa, as shown in Figure 2(a) and Figure 3(c). In both cases, layers 7 to 12 seems to be robust during continual learning. For XLNET, the robust layers are 3 to 10, with a dramatic drop in layer 11. Moreover, some fragile layers can be observed for each PLM, such as layer 6 of BERT in Figure 2(e) and layer 12 for GPT2, and layer 11 for XLNET. The robust layers in pretrained model provide us the free lunch that we can extract features for metric-based classi\ufb01cation without extra computation and buffer memory. The probing over \ufb01ve pretrained language model shows the existence of robust layer in BERT, XLNET, and RoBERTa. However, the post-selection of pretrained model and con\ufb01guration may not always feasible in real- world applications, we believe it is an important direction to predict and dynamically detect the robust layer for better continual learning performance.\n\n6 C ONCLUSION\n\n In this paper, we conduct the \ufb01rst comprehensive comparative study that sheds light on the perfor- mance characteristics of continual learning across representative language models and CL methods, as well as a detailed layer-wise analysis within language models. The insights gained from this study open up new research questions that will inspire further research on continual learning based on pre-trained self-supervised models. Our comparative study on pretrained language models for continual learning provides more in- sights on different aspects of performance, uncovers new opportunities for the development of NLP- speci\ufb01c continual learning methods in which language models play a central role. Broadly speaking, the future impact of our work is threefold: (1) Criteria and strategies for selecting a PLM for con- tinual learning. (2) Techniques for utilizing the layer-wise insights of a speci\ufb01c PLM for continual learning. Given the fragile layer and robust layer of a PLM, novel dynamic adapter structures, replay 8\n\nR EFERENCES\n\n http nips 9 10 11\n\nA F URTHER R ELEVANT A PPROACHES TO C ONTINUAL L EARNING\n\n Instead of using previously-stored data for replay, some methods will train a generative model along- side their classi\ufb01er and perform the rehearsal on generated samples. These methods are often re- ferred to as generative replay Shin et al. (2017); Lesort et al. (2019a); Ostapenko et al. (2019). Some hybrid strategy from different strategy also exist. E g. Dark Experience Replay (DERPP) Buzzega et al. (2020) proposes a method at the intersection of rehearsals and regularization based methods. Furthermore, the \ufb01eld of meta-learning, now interrelating with continual learning, as provoked the emergence of new methods. In meta-continual learning Javed & White (2019); Vuorio et al. (2018), algorithms are learning how to continually learn. The hope is that the continual-learning problem can be solved in a data-driven way by the learning algorithm itself. In continual-meta learning, the task-agnostic CL setting, i.e., where the task boundaries aren\u2019t provided during training, is tackled. The algorithms perform fast adaptation Riemer et al. (2018) to adapt to the current task He et al. (2019); Caccia et al. (2020).\n\nB D ATASET D ISTRIBUTIONS\n\n The distribution of each dataset are summarized as Figure 4.\n\nC M ODEL T RAINING AND H YPER - PARAMETERS\n\n Hyperparameter selection . For each combination of PLM and CL scheme, we perform a com- bined grid-search for Class-IL and Task-IL, choosing the con\ufb01guration that achieves the highest \ufb01nal accuracy averaged on the two settings. Typically, we summarize the details of implementation as follows: Training . To provide a fair comparison among CL methods, we train all the networks using the AdamW Mosbach et al. (2021) optimizer, and select 10e-5 as the learning rate for all pretrained backbone models. We deliberately hold batch size and minibatch size out from the hyperparameter space, thus avoiding the \ufb02aw caused by the different numbers of update steps for different methods.\n\nD C OMPUTATION C OST\n\n To provide a perspetive on computational complexity, we also measure the giga multiply-accumulate operations (GMac) per instance (spans of 50 tokens randomly selected from Maven) for each PLM and its number of parameters, as shown in Table 3. 12\n\nE P ROBING M ETHOD\n\n As shown in Algorithm 1, we do layer-wise probing with the layer-wise prototype-based classi\ufb01ca- tion on validation dataset.\n\nF O VERALL P ERFORMANCE ON PLM S\n\n As shown in Figure 7 and Figure 5 pretrained language models show different task-wise robustness during continual learning 13\n\nG F INAL S TAGE P ROBING\n\n As shown in Figure 6 pretrained language models demonstrate different layer-wise representability at the last stage of continual learning 14 15 16", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a comprehensive comparative study on continual learning (CL) performance using different combinations of pre-trained language models (PLMs) and CL methods.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study evaluates 5 PLMs (ALBERT, BERT, GPT2, RoBERTa, XLNET) and 4 types of CL methods (experience replay, regularization-based, dynamic architecture, and hybrid) on 3 benchmarks (CLINC150, Maven, WebRED) under 2 incremental settings (task-incremental and class-incremental).", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Through extensive experiments and probing analyses, the paper reveals interesting performance differences across PLMs and CL methods.", "topic_class_1": "Summary", "topic_class_2": "personal understanding of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It dives into analyzing why certain PLMs (e.g. BERT) are more robust and why replay-based methods outperform regularization-based ones by examining the layer-wise characteristics of PLMs during CL.", "topic_class_1": "Summary", "topic_class_2": "paper summary personal understanding of contributions", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "The paper provides valuable insights and uncovers important open questions to guide future research on optimizing PLMs for CL and designing NLP-oriented CL methods.", "topic_class_1": "Summary", "topic_class_2": "personal understanding of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper conducts a thorough empirical study covering an extensive combination of representative PLMs, CL methods, benchmark datasets and CL settings. This helps provide a comprehensive understanding of the problem.", "topic_class_1": "Strengths - summary", "topic_class_2": "experiments insight for future research", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "- The layer-wise probing analyses offer an in-depth look into the idiosyncrasies of each PLM during CL, revealing useful insights on why certain PLMs and CL methods perform better.", "topic_class_1": "Strengths - results", "topic_class_2": "analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The observations and open questions discussed can serve as valuable guidance for future research on improving CL with PLMs and designing NLP-specific CL techniques.", "topic_class_1": "Strengths - potential for future work", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the paper experiments with an extensive combination of models and methods, the justification for choosing those particular PLMs, CL methods and datasets could be elaborated more.", "topic_class_1": "Weaknesses - model choices", "topic_class_2": "experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The layer-wise probing reveals useful insights, but the underlying reasons why different layers in a PLM demonstrate different remembering/forgetting behavior remains unclear.", "topic_class_1": "Weaknesses - analysis lack depth", "topic_class_2": "analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A more theoretical analysis here could strengthen the paper.", "topic_class_1": "Weaknesses - analysis lack depth", "topic_class_2": "analyses", "reliability": "No", "explanation": "Even though no theoretical analysis is provided, the findings here are informative enough.", "error_type": "Out-of-scope"}, {"segment_text": "- More details can be provided on the practical costs (computation, memory) of applying different CL methods to the large PLMs to better inform real-world deployment.", "topic_class_1": "Weaknesses - compute", "topic_class_2": "analyses", "reliability": "No", "explanation": "Partially addressed in the appendices, more details are outside the scope of the paper (?)", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is overall well-motivated, clearly organized, and easy to follow.", "topic_class_1": "Strenghts - flow / writting quality", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The work is of good quality, conducting extensive experiments and analyses that provide empirical and qualitative understandings.", "topic_class_1": "Strengths - general", "topic_class_2": "experiments, analyses", "reliability": "No", "explanation": "Vague, could apply to most good papers, no real information or insight; a weaknesses contradicts this poing (not enough or great qualitative analyses?)", "error_type": "Superficial Review"}, {"segment_text": "- The focus on benchmarking CL with modern PLMs and the layer-wise analysis of PLMs during CL are novel explorations that contribute new insights to the community.", "topic_class_1": "Strengths - general", "topic_class_2": "experiments, analyses", "reliability": "No", "explanation": "Nothing wrong but nothing new - already stated or at least heavily implied previously.", "error_type": "Duplication"}, {"segment_text": "- The paper provides many details of the experimental setup. Releasing the code and datasets will further facilitate reproducibility.", "topic_class_1": "Strenghts - details", "topic_class_2": "experiments reproducibility", "reliability": "No", "explanation": "Earlier a weakness was not enough details on practical costs. In any case, there is no useful information here... it is mostly boilerplate", "error_type": "Contradiction"}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper tackles the important problem of understanding continual learning with pre-trained language models through comprehensive experiments and insightful analyses.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "No", "explanation": "It repeats from the summary at the beginning but nice transition", "error_type": "Duplication"}, {"segment_text": "The extensive results reveal the performance differences across PLMs and CL methods, while the layer-wise probing provides a deep dive into a PLM's behavior during CL.", "topic_class_1": "Summary - strengths", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite some room for more theoretical grounding and cost analysis, the work is a valuable contribution that can guide future research on optimizing PLM-based CL.", "topic_class_1": "Summary - tradeoffs / overall strength", "topic_class_2": "suggestion for improvement", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "I believe it will foster meaningful discussions and developments in the community.", "topic_class_1": "Summary - overall opinion", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flags for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper conducts a comprehensive comparative study on the interaction between pre-trained language models (PLMs) and continual learning (CL) methods in NLP.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It evaluates five PLMs and four CL methods across three benchmarks in task- and class-incremental settings.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study uncovers performance differences across PLMs and CL methods and provides layer-wise and task-wise probing analyses to understand PLMs' susceptibility to forgetting.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It identifies the strengths and limitations of various combinations, leading to several research questions that guide future CL technique design.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper provides an extensive comparative analysis across multiple PLMs and CL methods, contributing significantly to understanding their interactions.", "topic_class_1": "Strengths - methods and results", "topic_class_2": "experiments, analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Layer-wise and task-wise probing analyses offer deep insights into how different layers of PLMs are affected by CL, revealing critical aspects of model susceptibility to forgetting.", "topic_class_1": "Strengths - methods and results", "topic_class_2": "experiments, analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology for benchmarking and analysis is rigorous and well-designed, covering a comprehensive range of settings and metrics.", "topic_class_1": "Strengths - methods and results", "topic_class_2": "experiments, analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper primarily focuses on quantitative analysis and could benefit from more qualitative examples to illustrate the practical impact of the findings.", "topic_class_1": "Weaknesses - lacks qualitative analyses", "topic_class_2": "suggestion for improvement", "reliability": "No", "explanation": NaN, "error_type": "Out-of-scope"}, {"segment_text": "While the study is extensive, it covers only a subset of available PLMs and CL methods, potentially limiting the generalizability of the conclusions.", "topic_class_1": "Weaknesses - narrow experiments", "topic_class_2": "suggestion for improvement", "reliability": "No", "explanation": NaN, "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and well-structured, with high-quality research and novel insights into the interaction between PLMs and CL methods in NLP.", "topic_class_1": "Strengths - Clarity", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The detailed methodology and provision of code and datasets support the potential for reproducibility and further research.", "topic_class_1": "Strengths - code and dataset", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper offers valuable contributions to the NLP and CL communities by thoroughly analyzing the performance of different PLMs and CL methods.", "topic_class_1": "Summary - summary", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Its rigorous benchmarking approach and novel layer-wise probing analysis provide deep insights and open up new research avenues.", "topic_class_1": "Summary - methodology / results", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While expanding the range of PLMs and CL methods studied could further enhance its impact, the paper's clarity, quality, and potential for reproducibility make it a significant contribution to the field.", "topic_class_1": "Summary - strengths and potential expansion", "topic_class_2": "personal understanding of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a comprehensive study of continual learning (CL) with pretrained language models (PLMs).", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It investigates the performance of five PLMs (ALBERT, BERT, GPT2, RoBERTa, and XLNET) combined with four CL methods (Vanilla, EWC, ER, and DERPP) on three benchmark datasets in two incremental learning settings (task-incremental and class-incremental).", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper analyzes the performance differences across PLMs and CL methods, conducts layer-wise probing to understand the forgetting behavior within PLMs, and identifies open research questions in this area.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive study: The paper covers a wide range of PLMs, CL methods, datasets, and settings, providing a valuable overview of the current state-of-the-art.", "topic_class_1": "Strengths - scope", "topic_class_2": "wide coverage", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Detailed analysis: The layer-wise probing analysis offers insights into the forgetting behavior within PLMs and the effectiveness of different CL methods on different layers.", "topic_class_1": "Strenghts - analysis / results", "topic_class_2": "experiments, analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Identifies open research questions: The paper highlights several important research directions for future work in PLM-based continual learning.", "topic_class_1": "Strenghts - future work", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open-source resources: The authors commit to releasing code and datasets, which will benefit the research community.", "topic_class_1": "Strenghts - opensouce code and datasets for future", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited novelty of CL methods: The paper mainly focuses on existing CL methods, and the proposed probing analysis, while insightful, is not a novel CL approach.", "topic_class_1": "Weaknesses - no new methods", "topic_class_2": "novelty", "reliability": "No", "explanation": "It is reliable but kind of pointless: analyzing existing methods is valuable by itself\nThis paper is more like an analysis paper and it does not aim to provide a new method.", "error_type": "Misinterpret Novelty"}, {"segment_text": "Evaluation concerns:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The pretraining corpus used differs from common choices in prior work, making comparisons difficult.", "topic_class_1": "Weaknesses - difficult to compare with previous work", "topic_class_2": "experiments", "reliability": "No", "explanation": "Paper conducts self-contained analyses; I can't tell what you would compare with. They are defining a framework.", "error_type": "Inexpert Statement"}, {"segment_text": "Evaluation on GLUE benchmark is missing, hindering comparison with previous PLMs.", "topic_class_1": "Weaknesses - missing GLUE", "topic_class_2": "experiments", "reliability": "No", "explanation": "Despite its popularity, I don't see any reason to include the tasks in GLUE to the study. The point in continual learning here.", "error_type": "Out-of-scope"}, {"segment_text": "Only best test set results are reported, instead of the standard practice of reporting median results across multiple runs on dev sets.", "topic_class_1": "Weaknesses - experiments", "topic_class_2": "experiments", "reliability": "No", "explanation": "It is fine as all models are under the same evaluation protocol. ", "error_type": "Inexpert Statement"}, {"segment_text": "Missing comparisons: The paper does not compare with contrastive learning-based sentence-level pretraining tasks, which have shown promising results in CL.", "topic_class_1": "Weaknesses - experiments", "topic_class_2": "experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additional Comments:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Address the concerns regarding the evaluation methodology and include comparisons with contrastive learning-based CL approaches.", "topic_class_1": "Suggestions", "topic_class_2": "suggestion for improvement", "reliability": "No", "explanation": "It is obvious that the weaknesses should be addressed.", "error_type": "Superficial Review"}, {"segment_text": "Consider exploring and proposing novel CL methods specifically designed for PLMs, based on the insights from the probing analysis.", "topic_class_1": "Suggestions", "topic_class_2": "suggestion for improvement", "reliability": "No", "explanation": "It is fair, but a suggestion for future work, improving continual learning is outside the scope of the paper.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is clearly written and well-organized.", "topic_class_1": "Strengths - writing", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-executed, but the concerns regarding evaluation and missing comparisons affect the overall quality.", "topic_class_1": "Strengths - quality", "topic_class_2": "experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The main contribution lies in the comparative study and probing analysis, which offer valuable insights but are not entirely novel approaches.", "topic_class_1": "Strengths - novelty", "topic_class_2": "novelty", "reliability": "No", "explanation": "Summary, review already states this\nNovelty is not the main goal in this paper, instead  interesting findings are.", "error_type": "Superficial Review"}, {"segment_text": "Reproducibility: The authors promise to release code and datasets, which will ensure reproducibility.", "topic_class_1": "Strengths - reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Heading", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper provides a valuable contribution to the field of continual learning with PLMs.", "topic_class_1": "Summary - closing", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive comparative study and detailed probing analysis offer important insights and identify promising research directions.", "topic_class_1": "Summary - strengths", "topic_class_2": "insight for future research", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, concerns regarding the evaluation methodology and missing comparisons with recent CL approaches need to be addressed to strengthen the paper's conclusions.", "topic_class_1": "Summary - weaknesses", "topic_class_2": "suggestion for improvement", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Exploring extreme parameter compression for pre-trained language models", "body_text": "\n\nE XPLORING EXTREME PARAMETER COMPRESSION FOR PRE - TRAINED LANGUAGE MODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language pro- cessing. This raises many concerns from various perspectives, e.g., \ufb01nancial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which ten- sor decomposition is a potential but under-investigated one. By comparing existing decomposition methods, Tucker decomposition is found to be parameter-ef\ufb01cient for compression. Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and ef\ufb01ciency of Tucker decomposition in parameter compression. Our compressed BERT with 1 / 7 parameters in Trans- former layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves 96.7% performance of BERT- base with 1 / 48 encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and 2 . 7 \u00d7 faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the bene\ufb01t of the proposed method on a distilled BERT.\n\n1 I NTRODUCTION\n\n Pre-trained Language Models such as BERT (Devlin et al., 2018) and ALBERT (Lan et al., 2019) have signi\ufb01cantly improved various NLP tasks with signi\ufb01cant improvement. Much recent work (Brown et al., 2020; Narayanan et al., 2021; Fedus et al., 2021) explores the potential of super large-scale PLMs. However, such large-scale PLMs are both economically and ecologically unfriendly (Bender et al., 2021; Patterson et al., 2021). Furthermore, deployment of large-scale PLMs is also challenging since (1) a model cannot be fully deployed or stored in a single GPU server, model parallelism would consume extra time for network communication among many servers; (2) edge devices may not have enough space for storing models; (3) the long inference time cannot support real-time feedback. Scaling down a model with negligible performance drop would facilitate the real-world applications of PLMs in a smaller size, faster inference time, and less network communication cost. For example, recent work explores quantization (Zhang et al., 2020; Bai et al., 2020), weights pruning (Hou et al., 2020), and knowledge distillation (Jiao et al., 2020; Sanh et al., 2020) for BERT (one of the most popular PLMs). We argue that existing methods cannot largely compress large-scale PLMs as stated in Sec. 2. In this paper, we aim to explore extreme parameter compression (i.e., bigger compression ratios) although they are by de\ufb01nition challenging. The parameter redundancy in PLMs was demonstrated by (Kovaleva et al., 2019; Michel et al., 2019; Voita et al., 2019; Cordonnier et al., 2021), for which we divide into two groups: intra- matrix redundancy and inter-matrix redundancy . The former happens in different heads that are calculated separately, e.g., attentions among heads act on a similar subspace and are therefore low- rank (Cordonnier et al., 2021) \u2013 we relate this phenomenon to the so-called \u2018decomposability\u2019 de\ufb01ned in this paper. Like self-attention layers, decomposability also holds in FFN layers \u2013 each FFN layer could be decomposed to many independent sub-FFNs (as explained in Appendix B). One example of inter-matrix redundancy happens across different layers , e.g., attention maps among layers might be similar (Clark et al., 2019; Vig, 2019; Rogers et al., 2020). 1 Exploration of main weight matrices in Transformer layers \ufb01nds that these weight matrices are possible to be approximated in a low-rank manner \u2013 evidencing the possible intra-matrix redundancy and inter-matrix redundancy . We comprehensively analyze and compare different decomposition methods for parameter compression including matrix decomposition (denoted as II), tensor train decomposition (Oseledets, 2011) (denoted as III) and Tucker decomposition (De Lathauwer et al., 2000) (denoted as IV). The fundamental difference between them is as below. II conducts matrix factorization (e.g., SVD) for each weight matrix thanks to intra-matrix redundancy . Regarding inter- matrix redundancy , III shares the head and tail matrices while keeping the core matrix individual; IV introduces \u2018matrix bank\u2019 to make parameter scale being nearly constant w.r.t. the number of layers. It is concluded that Tucker decomposition (IV) is more parameter-ef\ufb01cient than others in terms of compression ratios. ALBERT (Lan et al., 2019) can be considered as a special case of IV. The practical challenges of matrix/tensor decomposition for compression are twofold. First, the decomposition may result in a discrepancy between the raw weights and approximated weights, and exact decomposition is impossible with large compression ratios. Instead, Knowledge Distillation (KD) is used on the compressed model to simulate the predictions of the raw model in a loss-aware manner. Second, reconstruction may lead to additional computation costs. An ef\ufb01cient reconstruction protocol is implmented by reordering multiplication operations that also preserve the same results. The contributions of this work are (1) we propose a formal framework with standardized terminology to comprehensively discuss matrix/tensor decomposition methods to compress Transformer-based language models; (2) we adopt tensor decomposition for compressing PLMs which is also faster, while existing work (Ma et al., 2019; Liu et al., 2021) did not show the potential for speedup in PLMs; (3) our compressed BERT with 1 / 7 parameters in Transformer layers performs on-par with the original BERT in GLUE benchmark. Also, a tiny version achieves 96.7% performance of BERT-base with only 1 / 48 parameters in Transformer layers and 2 . 7 \u00d7 faster on inference. We directly use the proposed methods on TinyBERT (Jiao et al., 2020) that is purely based on KD, since our work is complementary to existing compression method like KD.\n\n2 R ELATED W ORK\n\n Compressing PLMs Although various work was proposed to design a new ef\ufb01cient Trans- former (Tay et al., 2020), e.g., (Ma et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Kitaev et al., 2020; Zaheer et al., 2020; Cao et al., 2020), in this paper, we are focusing on the compression of Transformer-based pre-trained language models. The difference is that the latter expects to reuse well-trained models, e.g., BERT and GPT (and even GPT3 (Radford et al., 2019) and PanGu- \u03b1 (Zeng et al., 2021)) with manageable computing resources, which typically does not change the original Transformer architecture. Taking BERT, one of the most commonly-used pre-trained language models, as an example. Existing work explored quantization (Zhang et al., 2020; Bai et al., 2020), weights pruning (Hou et al., 2020), knowledge distillation (Jiao et al., 2020; Sanh et al., 2020), progressive module replacing (Xu et al., 2020), and matrix decomposition (Noach & Goldberg, 2020). We argue that existing compression methods (see Tab. 1) may be inadequate for extreme parameter compression, which is under-investigated. The reasons are manifold, \ufb01rst, the knowledge distillation- based method generally learns a new student model from scratch, which cannot inherit too much knowledge from the teacher model before distillation. Second, some methods have upper bounds of compression ratio. For example, layer-sharing ALBERT (Lan et al., 2019) shares parameters in L layers with maximum L times compression. Quantization replaces existing 32-bit parameters with binary parameters with a maximum of 32 times reduction. Moreover, quantization needs further hardware support, which is usually ad hoc to speci\ufb01c platforms. Weight pruning arguably cannot achieve a big compression ratio (McCarley et al., 2019; Michel et al., 2019). Matrix/tensor decomposition for compression Tensor/matrix decomposition aims to approxi- mate a given tensor using a set of smaller tensors/matrices. It has been investigated to compress and speed up CNNs, RNNs, and Transformers for many years (Lebedev et al., 2014; Yang et al., 2017; Ye et al., 2018; Denton et al., 2014; Winata et al., 2019). Matrix decomposition (e.g., ALBERT (Lan et al., 2019) in the embedding layer and (Noach & Goldberg, 2020)) could decrease parameter scale with a linear factor depending on the selected rank. More advanced tensor decomposition approaches can be implemented by tensor network, which has recently been used to compress general neural 2 networks (Gao et al., 2020; Novikov et al., 2015), compress embedding layer (Khrulkov et al., 2019; Hrinchuk et al., 2020; Panahi et al., 2019). Recently, Ma et al. (2019) redesigned a new \u2018Self-Attention Network\u2019 (SAN) in Transformer architec- ture inspired by block-term tensor decomposition. The compression ratio is limited since the majority of parameters in Transformer comes from another module called \u2018Feed-Forward Network\u2019 (FFN) instead of SAN; moreover, the model does not have the potential for speedup regardless of compres- sion ratios since the time complexity is closed to vanilla Transformer. Noach & Goldberg (2020) reparameterized each weight matrix using matrix decomposition and further distill the compressed models, which nearly achieves a compression ratio of 1.7. Liu et al. (2021) adopted matrix product operators to reparameterize each group of weight matrices in embedding, SAN, and FFN, and only a small part of tensors of MPO (called \u2018auxiliary tensors\u2019) are \ufb01ne-tuned in downstream tasks. The compression ratio of the total parameter is negligible and the inference might be slow. Those works inspire us to explore in depth extreme parameter compression for large-scale PLMs. We argue that compression ratio of existing work using matrix/tensor decomposition (Ma et al., 2019; Liu et al., 2021; Noach & Goldberg, 2020) for PLMs is relatively-small; most of them do not have speedup effect, limiting their applications in large-scale PLMs. The potential to compress PLMs with matrix/tensor decomposition is under-investigated. In this work, we adopt tensor decomposition, especially Tucker decomposition, to cubically compress the parameters of PLMs.\n\n3 M OTIVATIONS FOR PARAMETER COMPRESSION\n\n Pre-trained language models are typically a stack of multiple Transformer (Vaswani et al., 2017) layers that consist of a Self-Attention Network (SAN) module and a Feed-Forward Network (FFN) module, see App. A for Transformer. Sec. 3.1 will introduce an important property called \u2018decomposability\u2019 for SAN and FFN, which indicates each sub-component in SAN or FFN is independently calculated without interactions between sub-components and they may therefore be redundant.\n\n3.1 D ECOMPOSABILITY IN T RANSFORMER\n\n Decomposability is de\ufb01ned as below, based on which we examine if SAN and FFN are decomposable . De\ufb01nition 1 Decomposability. One computing module f is decomposable if its sub-components { g 1 , g 2 , \u22ef g H } could be independently calculated without interactions. Usually, \u03b4 is a simple operation that has negligible computing cost compared to { g h } . Especially, backpropagation between sub-components is independent if \u03b4 is concatenation or addition. Sub- components in a decomposable component could be calculated in parallel without interactions. Decomposability in SAN Following (Hou et al., 2020), SAN could be decomposed as a sum of the output of every head. For the query/key/value/output transformations parameterized by W Q / W K / W V / W O , we divide them as N h heads: { W Qh } N h , { W Kh } N h , { W Vh } N h , { W Oh } N h . 3 \u0000\u0013 \u0000\u0019\u0000\u0017 \u0000\u0014\u0000\u0015\u0000\u001b \u0000\u0015\u0000\u0018\u0000\u0019 \u0000\u0016\u0000\u001b\u0000\u0017 \u0000\u001a\u0000\u0019\u0000\u001b \u00003\u0000&\u0000$\u0000\u0003\u0000F\u0000R\u0000P\u0000S\u0000R\u0000Q\u0000H\u0000Q\u0000W\u0000V \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u001c \u0000\u0014\u0000\u0011\u0000\u0013 \u0000F \u0000D\u0000S \u0000W \u0000X \u0000U \u0000H\u0000G \u0000\u0003 \u0000Y \u0000D \u0000U \u0000L \u0000D\u0000Q \u0000F \u0000H \u0000D\u0000Y\u0000H\u0000U\u0000D\u0000J\u0000H A single head is calculated as Then SAN ( X ) = \u2211 N h h = 1 Att h ( X ) , indicating SAN is decomposable. Cordonnier et al. (2021) argued that the attentions among heads learn redundant key/query projections due to independent calculation. Decomposability in FFN The two weight matrices in FFN, denoted as W In and W Out . As Geva et al. (2020) points out, FFN also operates as a key-value mechanism similar to SAN: one can consider the input of FFN as a query vector, and two linear layers of FFN as keys and values, respectively. There may exist some similar key-value pairs that may introduce redundancy. Remarks on decomposability Decomposability does not necessarily make sub-components being complementary, especially without orthogonality-like constraints between them. Sub-components may learn similar patterns, leading to some redundancy (Michel et al., 2019). For example, Hou et al. (2020) demonstrated that BERT could be pruned in both width and height without performance drop. Moreover, BERT is over-parameterized in the sense that the number of training examples in down- stream tasks is much fewer (393k training examples for the biggest task in GLUE, i.e., MNLI) comparing to the enormous number of parameters (110M parameters for BERT-base) \u2013 this was thought problematic according to the traditional machine learning theory (Vapnik, 2013). We believe that the pre-trained language models in downstream tasks could be compressed.\n\n3.2 E XPLORATION OF AN PRE - TRAINED T RANSFORMER\n\n Beyond the decomposability of Transformer, we conduct exploration on a widely-used Transformer- based PLM, i.e., BERT (Devlin et al., 2018). Technically, we calculate the captured variance ratio by Principal Component Analysis (PCA), as an indicator to measure the parameter redundancy. The main weight matrices of Transformer layers are { W Q , W K , W V , W O , W In , W Out } 1 . Sec. 3 and Appendix B show that FFN could be separately calculated in a multi-head fashion . We could sequentially split both { W In } and { W Out } into four groups like { W Inh } h = 4 and { W Outh } h = 4 1 We exclude embedding layer for compression, as (Ben Noach & Goldberg, 2020) did. Note that the lookup operation in embedding layers is fast; therefore, decomposing embedding will be more time-consuming since it involves additional computationally-expensive matrix multiplication. Moreover, this paper focuses on the core components in Transformer, namely SAN and FFN, which are the majority parameters that also increase linearly with network depth; while the parameter scale in embedding layer is constant w.r.t. network depth. 4 . By reconcile all be D Here, we could get 12 D \u00d7 D weight blocks for each Transformer layer, 4 for SAN, and 8 for In Fig. 1 we could \ufb01nd the intra-matrix and inter-matrix redundancy : Figure 1a shows that half dimensions could capture more than 90% variance of all weight matrices, this con\ufb01rms our statement in Sec. 3.1. Furthermore, we also study the redundancy between two matrices by conducting PCA on the concatenated matrix between two arbitrarily paired weight matrices. See Fig. 1b, half dimensions could capture nearly 80% variance, which suggests some possibility to compress inter- matrix redundancy . This inter-matrix redundancy may be twofold: (1) subFFNs are decomposable; (2) calculations (e.g., attention maps) in different layers may be similar. Regarding the latter, some existing works like RNNs (Goyal & Bengio, 2020), Neural Ordinary Differential Equations (ODE) (Chen et al., 2018), and cross-layer sharing ALBERT (Lan et al., 2019) show that it could work even with the parameter equivalence hypothesis among layers.\n\n4 A GENERAL FRAMEWORK FOR PARAMETER COMPRESSION\n\n The observation that the main weight blocks in BERT could be approximated in a low-rank manner (thanks to the intra-matrix and inter-matrix redundancy) inspires us to use decomposition. Here we introduce and compare some standard decomposition methods (see App. D) in compressing PLMs.\n\n4.1 E XPLORING PARAMETER COMPRESSION\n\n In principle, SANs and FFNs could be separately compressed; in this work, we additionally explore stacking SAN weights and FFN weights together as a uni\ufb01ed protocol since each weight block has an identical shape (i.e., D \u00d7 D ). The main weight matrices in the j -th Transformer layer are Weights of a L -layers Transformer are stacked as a 3-rd order tensor in R 12 L \u00d7 D \u00d7 D . The original non-decomposed weights is called I: W I = { W ( j ) } Lj = 1 \u2208 R 12 LD 2 . Each weight matrix in W I is W I i = W i \u2208 R D \u00d7 D . Here, we explore standard decomposition methods including matrix decomposition, tensor train decomposition (Oseledets, 2011) and Tucker decomposition (De Lathauwer et al., 2000). II: matrix decomposition Motivated by intra-matrix redundancy , one can adopt Matrix decom- position to factorize/approximate a matrix into some smaller ones. A typical example is singular value decomposition (SVD), called \u2018II- \u03b1 \u2019, for each D \u00d7 D matrix W i \u2208 W I , (5) One can also drop the diagonal \u03a3 i by decomposing it into two parts that are multiplied to U i and V i 2 , namely W i \u2248 U i V i , denoted as \u2018II- \u03b2 \u2019. U i \u2208 R D \u00d7 d and V i \u2208 R d \u00d7 D and usually d < D . Since the 2 By decomposing \u03a3 i into two diagonal matrices, each of which has diagonal elements that are the square root of \u03a3 i . By multiplying these two diagonal matrices to U i and V i . W i \u2248 U i V i \u2208 R D \u00d7 D . 5 - Methods Space complexity Difference Reference I - 12LD 2 raw (Devlin et al., 2018) II- \u03b1 Matrix decomposition 24LDd + 12 Ld 2 w/ low-rank approximation - II- \u03b2 Matrix decomposition 24LDd w/t diagonal matrices (Ben Noach & Goldberg, 2020) III Tensor-train decomposition 12Ld 2 + 2 Dd w/ parameter sharing - IV Tucker decomposition ld 2 + 12 Ll + 2 Dd w/ matrix bank - compression ratio is D 2 2 Dd with reducing the rank from D to d , the preserved rank approximated III: tensor train decomposition Inspired by the inter-matrix redundancy , one could expect to share weights among matrices. The biggest terms in Eq. 5 are U i and V i while { \u03a3 i } is relatively small since \u03a3 i \u2208 R d \u00d7 d and d is relatively small compared to D. We could share { U i } and { V i } among matrices to save parameters. This results in (6) Here, { \u03a3 i } are not necessarily diagonal. This results in a tensor-train (TT) decomposition (Oseledets, 2011) 3 . One can also consider higher-order TT decomposition (i.e., a longer chain for tensor multiplications) which could be more parameter-ef\ufb01cient; this often needs to reshape the raw tensor into a higher-order tensor with heuristics. However, it is more time-consuming and costs more GPU memory during training, which we leave as future work. IV: Tucker decomposition In Eq. 6, the biggest term is the { \u03a3 i } \u2208 R 12 L \u00d7 d 2 , especially the number of layers may be large in practice (e.g., L = 24 for BERT-large). To this end, we propose a \ufb01xed-sized matrix bank such that a weight matrix is considered as a linear combination of these matrices inside the bank, making the parameter scale become nearly a constant with respect to the number of layers. Namely, where C \u2208 R l \u00d7 d 2 is a matrix bank with a size of l , each matrix is assigned with a weight vector P i \u2208 R 1 \u00d7 l . ALBERT (Lan et al., 2019) could be considered as a special case of IV, see App. E.\n\n4.2 C OMPARISON BETWEEN I,II, III, AND IV\n\n The comparison in parameter scale between these decomposition methods is in Tab. 2. Since D > d and L > l , we could generally conclude that the parameter scales decrease from I, II, III to IV. We can observe that marginal parameter cost to add a new layer in IV is nearly 12 l , which is negligible compared to the other parameters. During the inference phase, the terms that do not involve batch size b or sequence length n could be calculated in an of\ufb02ine way only once before starting inference, which costs more storage but gets slightly acceleration \u2013 since the main purpose of this work is to compress models, we ignore it in this work but encourage doing it in speed-sensitive scenarios.\n\n5 E XTREMELY COMPRESSING BERT USING T UCKER D ECOMPOSITION 5.1 D ECOMPOSITION PROTOCOL\n\n IV reduces space complexity from O ( 12 LD 2 ) to O ( ld 2 + 12 Ll + 2 Dd ) where d < D and l < 12 L . l determines to which degree we want to share Transformer parameters among all modules, a \ufb02exible 3 A tensor-train decomposition is to approximate a high-order tensor with a product of many smaller three- order tensors \u2013 except for the \ufb01rst and last ones being matrices. Here, for a three-order tensor W \u2208 R 12 L \u00d7 D \u00d7 D , it is approximated by W \u2248 U G V and shape transpose, where U \u2208 R D \u00d7 r 1 , G \u2208 R r 1 \u00d7 12 L \u00d7 r 2 , and V \u2208 R r 2 \u00d7 D . For a speci\ufb01c slice of W , W i \u2248 U G \u22c5 ,i, \u22c5 V . r 1 and r 2 are the \u2018TT ranks\u2019. 6 - Multiplication order Computing complexity IV-1 X ( U ( P i C ) V ) O ( bnD 2 + Dd 2 + D 2 d + ld 2 ) IV-2 ( X U )( P i C ) V O ( 2 bnDd + bnd 2 + ld 2 ) IV-3 ( X U )(( P i C ) V ) O ( 2 bnDd + Dd 2 + ld 2 ) factor to smoothly transform vanilla BERT to layer-shared BERT (or called \u2018ALBERT\u2019 (Lan et al., 2019)). d determines the expressive power (rank) of each linear transformation (originally D \u00d7 D ). The decomposition protocol does not change the raw architecture of BERT, alternatively, it introduces a new reparameterization of the existing weights. However, the approximated weights W IV usually are not exactly equal to the the raw weights W I . Moreover, the tiny decomposition discrepancy of weight matrices in low-layer may lead to an accumulated difference in the \ufb01nal output due to the multiple-layer neural network architecture 4 . In this work, we propose to use knowledge distillation to simulate the \ufb01nal output of raw models. (8) f W I is the raw BERT model and f W IV is the compressed one. We argue that approximation in prediction (like knowledge distillation in Eq. 8) is more important than approximation in weights. Such a loss-aware strategy in compression could be found in quantization (Hou et al., 2018).\n\n5.2 R ECONSTRUCTION PROTOCOL\n\n A slice of D \u00d7 D parameter block is represented as a matrix product, W IV i \u2248 U ( P i C ) V \u2208 R D \u00d7 D . For an input X \u2208 R b \u00d7 n \u00d7 D where b is the batch size and n is the sequence length, an output of linear transformation between X and a D \u00d7 D parameter block will be Y = XW i ;; = X U ( P i C ) V . Note that matrix multiplication is associative 5 , thus different order of matrix multiplication will not affect the \ufb01nal result but their computational complexity is different 6 . One can see the computational complexity for multiplication order in Tab. 3. In practice, the batch size b will be set as big as possible to increase data throughput and make training more stable, we could conclude that IV-3 is more ef\ufb01cient than IV-2. Since D is typically much bigger than d ; IV-3 is more ef\ufb01cient than IV-1 when d < D / 2 . In conclusion, setting IV-3 is most ef\ufb01cient in this scenario.\n\n6 E XPERIMENTS 6.1 S ETTINGS\n\n Decomposition For a given pre-trained BERT (i.e., 12-layer BERT-base in this paper), its well- trained weights W with 144 D 2 parameters are decomposed into a core tensor with a size of ld 2 and three factor matrices with 12 L \u00d7 l , D \u00d7 d , and D \u00d7 d respectively (see Fig. 2). We use Tensorly (Kossai\ufb01 et al., 2016) for Tucker decomposition with random initialization. Knowledge distillation As (Jiao et al., 2020; Zhang et al., 2020; Bai et al., 2020) did, we use two-stage knowledge distillation for the compressed model. At General Distillation (GD) stage, we adopt Knowledge Distillation (KD) for the compressed model to simulate the last-layer hidden states and last-layer attention maps of the general teacher model (BERT-base). At the second stage, we adopt Task-speci\ufb01c Distillation (TD) to simulate the logits of a task-speci\ufb01c BERT model (e.g., \ufb01ne-tuned on MNLI task). In GD, compressed models are trained with two epochs. In TD, we also augment training data by randomly replacing a random word with a similar word according to either 4 Our experiments also show that a direct decomposition results in very low performance, see Tab. 6. 5 For a sequence of matrices (e.g., [ A , B , C ]), matrix multiplication with different calculation orders results in a identical result, i.e., ( AB ) C = A ( BC ) 6 In this paper, we de\ufb01ne the computational complexity of a matrix multiplication between a n \u00d7 m matrix and a 7 word vector similarity using Glove (Pennington et al., 2014) or the predicted logistics of BERT when masking the target word, see more details in (Jiao et al., 2020). We follow the experimental settings like hyperparameters in KD from (Jiao et al., 2020).\n\nGLUE evaluation General Language Understanding Evaluation (GLUE) (Wang et al., 2018)\n\n benchmark includes datasets for single document classi\ufb01cation and sentence pair classi\ufb01cation. Our \ufb01ne-tuning on GLUE follows the settings from the Huggingface website (Wolf et al., 2019). The numbers of training examples are listed in Appendix F. The evaluation metrics follow the of\ufb01cial GLUE benchmark (Wang et al., 2018). The best-performed model is selected according to the dev set, where we select the learning rate in [ 1 e -5, 2 e -5] and batch size in [ 16 , 32 ] .\n\n6.2 R ESULTS\n\n As shown in Tab. 4, our decomposed BERT with layer rank 144 and dimension rank 384, called \u2018TuckerBERT-144-384\u2019, outperforms the BERT-base, with only 1 / 7 parameters in Transformer layers and slightly bigger throughout. TuckerBERT-72-384 performs on-par with raw BERT, which is slightly worse than TuckerBERT-144-384 due to the smaller size. We could observe that a bigger rank (both for layer mode and dimension mode) usually consistently leads to better performance. TuckerBERT-144-64 achieves 96.7% performance (82.7 vs. 80.0) with only 1 / 48 parameters of Transformer layers and 2 . 7 \u00d7 inference speedup. Tab. 4 shows the compression ratios of TuckerBERT are bigger than existing parameter sharing method (Lan et al., 2019) and decomposition method (Noach & Goldberg, 2020; Mao et al., 2020). TuckerBERT-144-384/TuckerBERT-72-384 achieve comparable or even slightly better results than (Noach & Goldberg, 2020). TuckerBERT outperforms (Mao et al., 2020) with a large margin. TuckerBERT (except for TuckerBERT-144-64) outperforms ALBERT \u2013 the latter needs training from scratch (1M training steps or more) while the latter does not (usually less than 0.2M steps). 8 To contextualize TuckerBERT with other compression methods like knowledge distillation, Tab. 5 shows that TuckerBERT (in Tab. 4) achieves comparable performances with knowledge distillation methods (Sun et al., 2019; Xu et al., 2020; Jiao et al., 2020) while with fewer parameters. To show the proposed method is orthogonal to existing compression methods like pure knowledge distillation , we further explore the proposed method in TinyBERT (Jiao et al., 2020), called \u2018Tuck- erTinyBERT\u2019. Tab. 5 shows performance loss to compress TinyBERT (degrading from 83.0 to 81.6 in TuckerTinyBERT-72-384) is bigger than compressing raw BERT (degrading from 82.7 to 82.6 in TuckerBERT-72-384). This is probably due to smaller redundancy in TinyBERT compared to BERT.\n\n6.3 A NALYSIS\n\n Ablation on the necessity of knowledge distillation Tab. 6 shows that both GD and TD are essential for an effective decomposed BERT. In particular, the overall performance decreases from 82.6 to 80.0 by removing TD. Note that the model will collapse if we directly take decomposed BERT for \ufb01ne-tuning without knowledge distillation. Decomposition on FFNs or SANs For FFNs and SANs, we use the half size of matrix bank (i.e., 24 for SANs and 48 for FFNs) and half dimension rank (i.e., 384) respectively. The two settings are called \u2018FFN-48-384\u2019 and \u2018SAN-24-384\u2019. Tab. 7 shows that solely compressing SANs or FFNs could nearly achieve on par with the raw model since smaller compression ratios are achieved. In detail, FFNs are slightly easier to be compressed even with a big compression ratio comparing to SANs. It is intriguing to notice that TuckerBERT-72-384 outperforms SAN-24-384, although the former additionally compresses FFNs and has much fewer parameters. The reason may be that the size of the matrix bank in TuckerBERT-72-384 (i.e., 72), which is shared between FFNs and SANs, is bigger than its counterpart in SAN-24-384 (i.e., 24). This can shed some light on the bene\ufb01t to stacking FFNs and SANs together as introduced in Sec. 3.2.\n\n7 C ONCLUSION AND F UTURE W ORK\n\n To largely compress PLMs, we also comprehensively compare many matrix/tensor decomposition methods and conclude Tucker decomposition has the potential for extreme parameter compression. We therefore ef\ufb01ciently implemented Tucker decomposition inspired compression for BERT, thanks to an ef\ufb01cient reconstruction protocol. To compensate for the decomposition discrepancy, knowledge distillation is used to simulate the output of raw BERT, which is demonstrated to be essential in ablation study. Our method with 1 / 7 parameters could achieve comparable with the original model, with slight speedup. A tiny version achieves more than 96.7% performance of BERT-base with 1 / 48 parameters in Transformer layers and 2 . 7 \u00d7 faster on inference. In the future, we expect compression of PLMs to shift purely encoder-based language models (e.g., BERT) to decoder language models, since the latter has been designed as big as we could afford, e.g. GPT3 (Radford et al., 2019). Furthermore, hybrid methods by mixing knowledge distillation, quantization, parameter sharing, weight pruning, and matrix/tensor decomposition together are potential in practice. 9\n\nR EFERENCES\n\n 10 11 12\n\nA B ACKGROUND OF T RANSFORMER AND BERT A.1 T RANSFORMER\n\n A Transformer layer (see Fig. 3) consists of a self-attention (SAN) module and a feed-forward network (FFN) module. An input X for SAN will be linearly transformed into query, key, value and output space { Q, K, V } as below 7 : (9) 7 For all linear transformation in this paper, the bias term is in default omitted 13 The self-attention mechanism (a.k.a Scaled Dot-Product Attention) is calculated as For a multi-head version of the self-attention mechanism, it linearly projects Q, K, V with h times using individual linear projections to smaller dimensions (e.g. d k = d model h ), instead of performing a single attention function with d model -dimensional keys, values and queries. Finally, the output of SAN is where Q = [ Q 1 ; \u22ef Q ] , K = [ K ; \u22ef K h ] , and V = [ V 1 ; \u22ef V h ] . The individual attention heads are independently calculated, Cordonnier et al. (2021) claims that there is some redundancy in multi-head attention. Since the output of SAN is a linear transformation (using W O ) of V , which is a weighted sum of V . A stack of many purely SAN layers is not expressive (Dong et al., 2021), since it is equivalent to a single linear transformation. To this end, a feed-forward network with non-linear activation is alternately used with each SAN layer, Since some neurons after the activation function (e.g., \u03b4 is ReLU or GELU (Hendrycks & Gimpel, 2016)) become inactivated (zero), d in is usually bigger than d model to avoid the low-rank bottleneck, typically, d in = 4 \u00d7 d model = d out . Other tricks, such as layer normalization, residual connection, dropout, and weight decay are also adopted to relieve the optimization and over\ufb01tting problems when it goes deeper.\n\nA.2 BERT\n\n BERT is a Transformer architecture-based pre-trained language model trained on plain corpora by using a masked language model pre-training objective, thanks to the capacity and scalability of the Transformer (Devlin et al., 2018). BERT signi\ufb01cantly improved the SOTA performance of many downstream tasks, including classi\ufb01cation benchmark GLUE (Wang et al., 2018). Note that the parameters of SAN and FAN linearly increase with the number of layers while the embedding layers keeps constant with respect to the layer number.\n\nB \u2018M ULT - HEAD \u2019 F EED F ORWARD N EURAL N ETWORK\n\n In the multi-head attention mechanism, individual attention heads are separately calculated, however the calculation is low-rank due to the redundancy among heads (Cordonnier et al., 2021). In 14 this paper, we argue that such redundancy may also appear in the feed-forward neural networks. In the feed-forward neural networks, element-wise activation functions are usually adopted, e.g., GELU (Hendrycks & Gimpel, 2016) and Relu (Agarap, 2018); that is, each activation can be independently calculated. By partitioning the original W in into H column groups and W out into H row groups, one can revise a feed-forward neural layer (i.e. FFN ( X ) = \u03b4 ( X W in ) W out ) as a sum of H independent \u2018thin\u2019 sub-FFNs with a dimension of D H = 4 D / H as below: Where W in h \u2208 R D \u00d7 D h and W out h \u2208 R D h \u00d7 D , W in = [ W in 1 \u2295 \u22ef \u2295 W in H ] \u2208 R D \u00d7 4 D and W out = [ W out 1 \u2295 \u22ef \u2295 W out H ] \u2208 R 4 D \u00d7 D . In this paper, we set H = 4 , since W in h , W out h \u2208 R D \u00d7 D and each transformation in sub-FFNs layer are full-rank transformations. See Fig 4 for graphical illustration. One can refer to block partitioned matrix multiplication to understand the equivalence between Fig. 4a and 4b.\n\nC CONCATENATION OVER ROWS OR COLUMNS\n\n PCA on columnly-stacked and rowly-stacked matrices are shown in Fig. 5a and Fig. 5b. Observe that there is no too much difference between them.\n\nD M ATRIX AND TENSOR DECOMPOSITION\n\n Tensor is a generalized \u2018matrix\u2019 with typically a dimension of 3 or more; sometimes, one can also call a matrix as a bi-dimensional \u2018tensor\u2019 and a vector as a one-dimensional \u2018tensor\u2019. Formally, an n -dimensional tensor is an array with n indexes as An entry in X is accessed by selecting n ordered index ( i 1 , \u22ef , i n ) , and { 0 \u2264 i k < I n , i k \u2208 N } . In this section, we mainly discuss order-3 tensors for simplicity while higher-order tensors also hold. Typical tensor decomposition includes Canonical Polyadic (CP) decomposition (Carroll & Chang, 1970; Harshman, 1972) and Tucker decomposition (Tucker, 1963). CP decomposition approximates 15 a high-dimension tensor with as a sum of many rank-one tensors. In the case of three-dimensional tensor decomposition, Where A r \u2208 R I 1 , B r \u2208 R I 2 , C r \u2208 R I 3 and A \u2208 R R \u00d7 I 1 , B \u2208 R R \u00d7 I 2 and C \u2208 R R \u00d7 I 3 . \u2297 is the tensor product 8 . Tucker decomposition decomposes a tensor into a set of factor matrices and one small low-rank core tensor, 16 where A \u2208 R R 1 \u00d7 I 1 , B \u2208 R R 2 \u00d7 I 2 , C \u2208 R R 3 \u00d7 I 3 and G \u2208 R R 1 \u00d7 R 2 \u00d7 R 3 . \u00d7 1 , \u00d7 2 , and \u00d7 3 are mode- k products 9 . { R 1 , R 2 , R 3 } is sometimes called Tucker ranks . An entry with index ( i 1 , i 2 , i 3 ) is calculated as Tucker decomposition will degrade to CP decomposition,where the core tensor G is constrained to be super-diagonal 10 and R = R 1 = R 2 = R 3 . A k -slice in the \ufb01rst mode, i.e., a matrix with a size of I 2 \u00d7 I 3 , would be 18 All slices in a speci\ufb01c mode share factor matrices (i.e., B , C ) in other modes.Therefore, there exists inter-correlation between these slices. These shared factor matrices not only make the compression ratio of tensor decomposition being much bigger than multiple independent matrix decomposi- tion (Noach & Goldberg, 2020), but also can be bene\ufb01cial especially when there is redundancy among these parameter matrices and factor matrices can be utilized common features. 8 Here we give an example to calculate the tensor product of three 2-dimensional vectors, x , y , z \u2208 R 2 , resulting in a tensor of R 2 \u00d7 2 \u00d7 2 . R is the CP rank shared in all modes. \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 9 Given a tensor G \u2208 R R 1 \u00d7 R 2 \u22ef R k \u22ef R n and a matrix M \u2208 R R k \u00d7 r , a mode- k between G and M results in G \u00d7 m M \u2208 R R 1 \u00d7 R 2 \u22ef R k \u2212 1 \u00d7 r \u00d7 R k + 1 , \u22ef ,R n . 10 Namely, G i 1 ,i 2 ,i 3 equals 1 if i 1 = i 2 = i 3 , and 0 otherwise 16\n\nE L INKING IV TO ALBERT\n\n For IV, we have W IV = U ( P C ) V . Considering a speci\ufb01c case when U = V = I ( D ) , l = 12 (i.e., C \u2208 R 12 \u00d7 D 2 ) and P = \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 I ( 12 ) I ( 12 ) \u22ef I ( 12 ) \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 \u2208 R 12 L \u00d7 12 . Where I ( i ) is an i \u00d7 i identity matrix. Namely (19) Then (20) Eq. 20 results in a layer-shared BERT like ALBERT (Lan et al., 2019). (Lan et al., 2019) additionally compresses the embedding layer using matrix factorization.\n\nF GLUE BENCHMARK\n\n The data statistics in GLUE is shown in Tab. 8.\n\nG I NFERENCE TIME\n\n As shown in Fig. 6, the raw BERT base is the slowest one. TuckerBERT with dimension rank d = 384 is slight faster than raw. The inference time consistently decreases when d becomes smaller. Especially, TuckerBERT-144-64 is 2.6 \u00d7 faster than BERT, which is consistent to RPS shown in Tab. 4. Note that TuckerBERT-144-384 and TuckerBERT-72-384 are faster Than BERT in terms of inference time and RPS (see RPS in Tab. 4), although they have the identical FLOPS. The similar observation could be found in ALBERT (Lan et al., 2019) \u2013 layer-shared BERT has the same FLOPS with the raw BERT while the former is faster than the latter. The better inference time and RPS with same FLOPS may be due to the relatively smaller model consume less in memory bandwidth inside GPU, see https://openreview.net/forum?id=H1eA7AEtvS . This speedup effect in this matter depends on speci\ufb01c platforms and hardware.\n\nH C OMPARISON TO METHODS THAT USES NON - STANDARD METRICS IN GLUE\n\n (Noach & Goldberg, 2020) uses non-standard setting for GLUE tasks. Metrics in (Noach & Goldberg, 2020) are Accuracy (MNLI (average of MNLI match and MNLI mis-match), QNLI, RTE, SST- 2), Avg of Accuracy and F1 (MRPC, QQP), Matthew\u2019s correlation (CoLA), Avg of Pearson and 17 Spearman correlations (STS-B). Tab. 9 tailored our result in their evaluation protocol. In this work, we do not include CoLA dataset since training samples in CoLA is too few to have stable evaluations. 18", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper explores extreme parameter compression for pre-trained language models (PLMs) like BERT using tensor decomposition methods.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors compare matrix decomposition, tensor train decomposition, and Tucker decomposition, concluding that Tucker decomposition is the most parameter-efficient for compression.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They propose decomposition and reconstruction protocols to improve the effectiveness and efficiency of Tucker decomposition.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Their compressed BERT models achieve comparable or slightly better performance than the original BERT on the GLUE benchmark, with significantly fewer parameters.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "For example, one model performs on par with only 1/7 of the parameters in the Transformer layers, while a tiny version achieves 96.7% of BERT-base performance with 1/48 parameters and 2.7x faster inference.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method is shown to be orthogonal and complementary to other compression techniques like knowledge distillation.", "topic_class_1": "Complementing existing methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Comprehensively compares different tensor decomposition methods for compressing PLMs, providing useful insights", "topic_class_1": "Comprehensive Experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Achieves very high compression ratios (e.g.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1/48 parameters) while maintaining most of the performance, demonstrating the effectiveness of the Tucker decomposition approach", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Proposes practical decomposition and reconstruction protocols that enable efficient compression and inference", "topic_class_1": "Efficiency for standard protocols", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Shows the method is complementary to knowledge distillation, indicating potential for combining with other compression techniques", "topic_class_1": "Complementing existing methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Evaluation is primarily on the GLUE benchmark; performance on other NLP tasks is not shown", "topic_class_1": "Limited Benchmarks", "topic_class_2": NaN, "reliability": "No", "explanation": "Glue is enough since it contain enough sub-tasks", "error_type": "Out-of-scope"}, {"segment_text": "- Theoretical analysis of why Tucker decomposition works well for compressing Transformers is somewhat limited", "topic_class_1": "Missing Discussion", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Does not explore the limits of compression - even smaller models may be possible with some performance trade-off", "topic_class_1": "Missing Discussion", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Impact on pre-training efficiency and resources is not analyzed, though this is very relevant for large-scale PLMs", "topic_class_1": "Missing Discussion", "topic_class_2": NaN, "reliability": "No", "explanation": "This is a little bit out-of-scope and actually this critique can be applied to many papers.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology is explained in detail and the experimental setup is reproducible.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The comprehensive comparison of different decomposition methods for PLMs is novel, and the extreme compression results achieved with Tucker decomposition are a significant contribution.", "topic_class_1": "Comprehensive Experiment", "topic_class_2": NaN, "reliability": "No", "explanation": "The comparison is comprehensive but that is not a point for novalty", "error_type": "Misinterpret Novelty"}, {"segment_text": "The overall quality of research and presentation is high.", "topic_class_1": "Soundness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a valuable contribution by demonstrating the effectiveness of Tucker tensor decomposition for extreme compression of PLMs like BERT.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The results showing comparable performance with 1/7 to 1/48 of the parameters are impressive and insightful.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The comparison between decomposition methods is rigorous and novel.", "topic_class_1": "Soundness and Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A few areas that could be further explored are evaluating on an even wider range of NLP tasks, providing more theoretical justification, analyzing impacts on pre-training, and pushing compression to the limit.", "topic_class_1": "Missing Discussion", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, these do not take away from the strong technical contributions and clear presentation of the work.", "topic_class_1": "Strength & Weakness Tradeoff", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a high-quality paper that advances the important problem of making large language models more efficient.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes a novel approach to achieve larger compression ratios for pre-trained language models (PLMs), specifically focusing on BERT.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It identifies tensor decomposition, particularly Tucker decomposition, as a parameter-efficient method for compression.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study introduces two decomposition and reconstruction protocols to enhance the effectiveness and efficiency of Tucker decomposition in parameter compression.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The compressed BERT models, with significantly reduced parameters, achieve comparable or slightly better performance than the original BERT on the GLUE benchmark.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, a tiny version of BERT, with less than 2M parameters (excluding the embedding layer) and 2.7 times faster inference, achieves 96.7% of BERT-base's performance.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also examines the synergy of the proposed method with knowledge distillation, indicating its orthogonality to existing compression techniques.", "topic_class_1": "Complementing existing methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper tackles the significant issue of PLM size, making a compelling case for the need for more efficient, smaller models without compromising performance.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It introduces an innovative application of Tucker decomposition for PLM compression, which is both novel and technically sound.", "topic_class_1": "Novelty and Soundness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The empirical results are impressive, especially the achievement of a tiny BERT variant that retains most of the performance of the full model while being significantly smaller and faster.", "topic_class_1": "Strong Experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The discussion on the scalability of the proposed approach as the sets produced by the decomposition grow larger is somewhat lacking.", "topic_class_1": "Missing discussion", "topic_class_2": NaN, "reliability": "No", "explanation": "Such scalablibity costs resources. The demonstration on Bert type model is evident enough", "error_type": "Out-of-scope"}, {"segment_text": "There might be concerns about the reliability of probabilistic inference in such cases.", "topic_class_1": "Missing discussion", "topic_class_2": NaN, "reliability": "No", "explanation": "The reviewer should specify what the concers are.", "error_type": "Vague Critique"}, {"segment_text": "While the paper provides a good comparison with existing methods, it could benefit from a deeper theoretical discussion on why Tucker decomposition outperforms other techniques in this specific application.", "topic_class_1": "Missing discussion", "topic_class_2": NaN, "reliability": "No", "explanation": "Another generic and out-of-scope suggestion.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written and clear, making it accessible to readers familiar with PLMs and tensor decomposition techniques.", "topic_class_1": "Calrity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, employing rigorous experimental methodologies and providing a comprehensive comparison with baseline models.", "topic_class_1": "Soundness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty lies in applying Tucker decomposition to compress PLMs efficiently, which is a relatively unexplored area.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments appear reproducible, with sufficient details on the implementation and evaluation settings.", "topic_class_1": "Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a significant contribution to the field of NLP by proposing an effective method for compressing PLMs using Tucker decomposition.", "topic_class_1": "Impact and Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Its approach not only preserves the performance of the original models on benchmark tasks but also improves inference speed, addressing a critical need for more efficient NLP models.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the paper is strong in its empirical evaluation, further exploration into the theoretical underpinnings of its method and its scalability would enhance its contribution.", "topic_class_1": "Missing discussion", "topic_class_2": NaN, "reliability": "No", "explanation": "Another generic and out-of-scope suggestion.", "error_type": "Out-of-scope"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates extreme parameter compression for pre-trained language models (PLMs) like BERT.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It focuses on tensor decomposition, specifically Tucker decomposition, as a promising yet under-explored approach.", "topic_class_1": "Topic Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors argue that existing compression methods like knowledge distillation and quantization have limitations in achieving high compression ratios.", "topic_class_1": "Motivation", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They introduce the concept of \"decomposability\" in Transformer layers and demonstrate its presence in both Self-Attention Networks (SANs) and Feed-Forward Networks (FFNs), suggesting redundancy that can be exploited for compression.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes two protocols: a decomposition protocol that leverages Tucker decomposition to represent weight matrices efficiently, and a reconstruction protocol that optimizes the order of matrix multiplication for faster inference.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The compressed models, called TuckerBERT, are evaluated on the GLUE benchmark and compared with other compression methods.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive framework: The paper provides a well-structured framework for discussing and comparing different matrix/tensor decomposition methods for PLM compression.", "topic_class_1": "Comprehensive Discussion", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "High compression ratios: Tucker decomposition enables significant parameter reduction, achieving up to 48x compression in Transformer layers with minimal performance loss.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Faster inference: The reconstruction protocol leads to faster inference speeds even with similar FLOPS compared to the original model.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Orthogonal to existing methods: The approach is complementary to existing compression methods like knowledge distillation, allowing for further compression of distilled models.", "topic_class_1": "Complementing existing methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Knowledge distillation dependence: The success of the compression relies heavily on knowledge distillation to compensate for the discrepancy introduced by weight approximation. This raises questions about the portability of the method to other PLMs or tasks where distillation might not be effective.", "topic_class_1": "Method Reliant Adaptability Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "The reviewer is not clear about this. What other models? Why distillation does not work there?", "error_type": "Vague Critique"}, {"segment_text": "Limited novelty of findings: While the application of Tucker decomposition to PLMs is interesting, the general idea of using tensor decomposition for neural network compression is not entirely novel.", "topic_class_1": "Limited Novelty", "topic_class_2": NaN, "reliability": "No", "explanation": "Using tensor decomposition for compression is a large task, and this work is very noval about using Tucker decomposition on it", "error_type": "Misinterpret Novelty"}, {"segment_text": "Impact on generation capabilities: The paper focuses on compression and inference speed, but it would be beneficial to analyze how the proposed method affects the model's generation capabilities and performance on tasks beyond GLUE.", "topic_class_1": "Limited Benchmarks", "topic_class_2": NaN, "reliability": "No", "explanation": "GLUE was the well-recognized benchmark.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and well-organized.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology is sound and follows established practices in tensor decomposition and knowledge distillation.", "topic_class_1": "sound method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experimental results are convincing and demonstrate the effectiveness of the proposed approach.", "topic_class_1": "Strong Experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the novelty of the findings is somewhat limited, as tensor decomposition has been previously explored for neural network compression.", "topic_class_1": "Limited Novelty", "topic_class_2": NaN, "reliability": "No", "explanation": "Using tensor decomposition for compression is a large task, and this work is very noval about using Tucker decomposition on it", "error_type": "Inexpert Statement"}, {"segment_text": "The paper provides sufficient details for reproducibility, although the specific implementation details of the knowledge distillation process could be further elaborated.", "topic_class_1": "Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a promising approach for extreme parameter compression of PLMs using Tucker decomposition.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The method achieves impressive compression ratios and even improves inference speed while maintaining good performance on the GLUE benchmark.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the reliance on knowledge distillation and the limited novelty of the findings are potential drawbacks.", "topic_class_1": "Adaptability Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further investigation into the impact on generation capabilities and exploration of alternative methods to address the decomposition discrepancy would be valuable.", "topic_class_1": "Limited Benchmarks", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additional Feedback:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It would be helpful to discuss the limitations of the proposed method and potential future research directions to address them.", "topic_class_1": "Missing Discussion", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Exploring the impact of the compression on different downstream tasks, including generation tasks, would provide a more comprehensive understanding of the method's impact.", "topic_class_1": "Limited Benchmarks", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Investigating alternative approaches to compensate for the decomposition discrepancy, such as fine-tuning strategies or regularization techniques, could be beneficial.", "topic_class_1": "Future Work", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper presents a valuable contribution to the field of PLM compression and is worthy of acceptance with minor revisions and clarifications.", "topic_class_1": "Overall contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Recitation-Augmented Language Models", "body_text": "\n\nR ECITATION -A UGMENTED L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n We propose a new paradigm to help Large Language Models (LLMs) gener- ate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval- augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE \ufb01rst recites one or several relevant pas- sages from LLMs\u2019 own memory via sampling, and then produces the \ufb01nal an- swers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Speci\ufb01cally, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA).\n\n1 I NTRODUCTION\n\n Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowd- hery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demon- strated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs\u2019 performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by \ufb01rst reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded Question: who wrote the song i hate you i love you Answer: Gnash \u2026 Question: who wrote the school for good and evil Direct Generation (e.g., PaLM) Answer: Soman Chainani LM Input LM Output Question: who wrote the school for good and evil Retrieval-augmented Generation (e.g., Atlas) Answer: Soman Chainani LM Input LM Output The School for Good and Evil is a fantasy fairytale hexalogy of books by Soman Chainani\u2026 Retriever Results Question: who wrote the song i hate you i love you Recitation: \"I Hate U, I Love U\" (stylized in all lowercase) is a song by American singer and rapper Gnash featuring American singer Olivia O'Brien. Answer: Gnash \u2026 Question: who wrote the school for good and evil Recitation-augmented Generation (ours) Recitation: The School for Good and Evil was first published on May 14, 2013 by Soman Chainani\u2026 Answer: Soman Chainani LM Input 1 as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the \ufb01nal outputs. The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute speci\ufb01c NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, \u201cwhat is the tenth decimal of \u03c0 ?\u201d . They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read \u201cthe tenth decimal of \u03c0 is 5\u201d. However, there can be some sentences like \u201cthe \ufb01rst N digits of \u03c0 are 3.14159 26535...\u201d existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: \u201cThe \ufb01rst 10 digits of \u03c0 are 3.14159 26535. So the answer is 5\u201d . Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge. We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Ques- tion Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, \ufb01ne-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on three large language models (In-house LM, UL2 (Tay et al., 2022a), and OPT (Zhang et al., 2022)) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-based single-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (Hot- potQA, Yang et al. 2018).\n\n2 R ELATED W ORK 2.1 O PEN - DOMAIN QUESTION ANSWERING\n\n Open-domain question answering (Prager et al., 2007) refers to the task of generating answers for arbitrary context-free questions. In the open-book setting, it is typically assumed that the QA model can \ufb01nd the answer in an external corpus, e.g., Wikipedia (Chen et al., 2017; Izacard & Grave, 2021) or web pages (Lazaridou et al., 2022). This is in analogy as taking an open-book exam where students can search over an external knowledge corpus. The standard pipeline (Chen et al., 2017; Izacard & Grave, 2021; 2020) usually consists of a learnable or non-learnable document retriever module and a learnable neural network-based reader module. In the closed-book setting, the QA model is not allowed to access any external knowledge, and needs to store all the knowledge in its parameters. It has been recently observed that large-scale pre-trained language models (Devlin et al., 2019; Radford et al., a; Yang et al., 2019b) can internalize a sort of implicit \u201cknowledge base\u201d after pre-training (Petroni et al., 2019; Jiang et al., 2020; Talmor et al., 2020). Roberts et al. (2020) show that after \ufb01ne-tuning on open-book question-answer pairs, T5 (Raffel et al., 2020) can answer a large portion of knowledge-intensive questions. This is similar as taking a closed-book exam. However, Lewis et al. (2021) found that the high performance is mainly due to training set question memorization. Wang et al. (2021) also found that it is still challenging for relatively small-scale pre-trained language models like RoBERTa (Liu et al., 2019) or GPT-2 (Radford et al., b) to answer closed-book questions. In this work, we focus on evaluating the CBQA performance of large language models (LLMs) in the few-shot setting, which ideally minimizes the bias of train-test overlapping (Liu et al., 2021). We propose a recite-and-answer scheme, which is similar to a student \ufb01rst recite the factoid knowledge about the question, and then answer the question. 2\n\n2.2 I N - CONTEXT FEW - SHOT LEARNING\n\n Large language models (LLMs) such as GPT-3 (Brown et al., 2020) have the surprising ability to do in-context learning, where the model learns to do new tasks simply by being prompted a few exemplars. The LLMs learn from these exemplars without being explicitly pre-trained for in-context learning and without any gradient updates or \ufb01ne-tuning. Recent study showed that such ability improves with the scaling of both model size (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) and number of tokens for training (Hoffmann et al., 2022). When evaluated on knowledge- intensive question answering tasks, these models are usually evaluated in the closed-book setting, where the factoid knowledge are completely stored in the model parameters of dense LLMs. Recently, Atlas (Izacard et al., 2022) shows that for knowledge-intensive NLP tasks, a relatively lite model with retrieval augmentations can achieve similar or even better performance through few- shot \ufb01ne-tuning, which proves that memorization can be decoupled from generalization in LLMs. In contrast, we show that still a large underestimated amount of knowledge can be retrieved from LLMs\u2019 model weights through better-designed prompting.\n\n2.3 R ATIONALE - AUGMENTED REASONING\n\n Ling et al. (2017) pioneer the work of solving math word problems by generating step-by-step human-readable solutions described by natural language and math equations before the \ufb01nal answer. That is fundamentally different from other works which directly generate the \ufb01nal answers or use formal languages. e.g. equations only, to illustrate the intermediate solving steps (Roy et al., 2016; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend (Ling et al., 2017) by constructing a much larger dataset to \ufb01netune a pre-trained large language model to solve math word problems and a parameterized ranker is trained to rank candidate solutions to improve the solving rate. Wei et al. (2022) propose chain-of-thought prompting which combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021) with few-shot prompting (Brown et al., 2020). In this work, instead of generating a chain of thought for multi-step reasoning questions, we de- compose the process of answering a knowledge-intensive question into two steps: recite the relevant knowledge stored in the model parameters, and then answer the question.\n\n2.4 M EMORIZATION IN LARGE LANGUAGE MODELS\n\n Recent study shows that large language models can memorize its training data, and generate texts from training data given certain prompts (Carlini et al., 2021; 2022; Zhang et al., 2021; Kharitonov et al., 2021; Thakkar et al., 2020; Carlini et al., 2019; Tirumala et al., 2022). Most related to our work, Carlini et al. (2022) found that the memorization ability of LLMs signi\ufb01cantly grows as the model capacity increases, the number of times an example has been duplicated, and the number of tokens of context used to prompt the model. While these works mainly analyze the fundamental properties of memorization in the exact setting, where exactly N tokens are used as the prompt to reproduce the suf\ufb01x of the prompt, our work relies on \u201cfuzzy memorizaiton\u201d, where the prompts tend to not be exactly the same as the training data, but still improve the memorization accuracy. The proposed recitation-augmented generation idea is also related to the line of work on utilizing Transformer memory as an information retrieval model (Tay et al., 2022b). Zhuang et al. (2022); Wang et al. (2022c); Zhou et al. (2022) proposed to augment documents at indexing time with a number of generated queries. Bevilacqua et al. (2022) proposed to directly generate n-grams grounded in one or multiple documents with constrained decoding.\n\n3 L EARNING TO R ECITE FOR C LOSED - BOOK Q UESTION A NSWERING\n\n The goal of this paper is to mimic a human\u2019s ability to recite relevant factoid knowledge before an- swering knowledge-intensive questions, such that these questions can be answered more accurately. In the following we describe our recite-and-answer scheme for few-shot closed-book question an- swering (CBQA), which consists of two components: (1) a evidence-recitation module for reciting relevant passages, and (2) a question-answering module for generating answers given the recited evidence. Notice that in this paper, we focus on few-shot setting, in which we assume only a few 3 question-answer demonstrations are provided. In Natural Questions (Kwiatkowski et al., 2019) benchmark, since the questions are from queries issued to the Google search engine by multiple users, and thus can be regarded as unannotated data, we further assume that we have top-retrieved Wikipedia pages for these questions. The paragraphs in these top-retrieved Wikipedia pages will be used to generate synthetic paired question-recitation data for \ufb01ne-tuning the LM (described in Section 3.2).\n\n3.1 P ROMPT - BASED RECITE - AND - ANSWER FOR QUESTION - ANSWERING\n\n Recitation-augmented question answering We start with single-hop question answering (Kwiatkowski et al., 2019; Joshi et al., 2017), where the answers are usually supported by a spe- ci\ufb01c document in the corpus, which is sometimes referred as evidence (Joshi et al., 2017). Different from chain-of-thought prompting (Wei et al., 2022) where a rationale is directly generated to explain the generated answer (Joshi et al., 2017; Narang et al., 2020; Lampinen et al., 2022), we propose to \ufb01rst recite a passage about the question, and then answer the question based on the recitation. We propose a prompt-based learning-to-recite scheme by leveraging the LLM\u2019s in-context learning ability (Brown et al., 2020). We prompt the LLM with paired exemplars of questions and recited evidences, and the LLM can learn in an in-context manner to generate a recitation for an arbitrary question. To perform recitation-conditioned few-shot question answering, we append the recited passages at the beginning of the original question-answer exemplars as a single prompt, and then generate the \ufb01nal answer (Step 1 & 2 in Figure 2). Self-consistency ensemble The factual knowledge about a question can appear in several places in the language model\u2019s training corpora. For example, the fact of \u201cQueen Elizabeth II opened the London Bridge on 17 March 1973\u201d can appear in both Wikipedia page \u201cLondon Bridge\u201d and page \u201cMarch 1973\u201d, so it is highly likely that there exists knowledge from different articles that could lead to the same, correct answer. With this motivation, we argue that similar to multi-step reasoning in chain-of-thought, recitation-augmented question answering can also bene\ufb01t from the self-consistency technique with multiple-path decoding (Wang et al., 2022b). Speci\ufb01cally, given an arbitrary question, we \ufb01rst use top- k sampling to independently generate a few recitations, and then 4 greedy decode the answer of the question based on the sampled recitations. Finally, we determine the optimal answer by taking a plurality/majority vote (Step 3 in Figure 2). Multiple-recite-and-answer for multi-hop question-answering Multi-hop question answering requires the QA system to \ufb01nd and reason over multiple supporting documents. However, the nature of recitation restricts us to recite passages from one article at a time. In order to apply the recite-and- answer scheme to solve multi-hop questions, we introduce multiple-recite-and-answer scheme (Step 4 in Figure 2), that is, given the multiple-hop question, we use the prompt words such as \u201cRecitation 1\u201d and \u201cRecitation 2\u201d to elicit the LLM to generate recitation passages on different topics. Since the multiple recited passages are generated in one-pass from the LLM decoding sequentially, the generation of later passages can effectively utilize the information both in the original question and the previous recited ones. Our multiple-recite-and-answer scheme for multi-hop question-answering is also compatible with the self-consistency technique, by applying top- k sampling when generating multiple recitations and performing majority voting for the \ufb01nal answers.\n\n3.2 P ASSAGE HINT - BASED DIVERSIFIED RECITATION WITH FINE - TUNING\n\n Passage hint-based diversi\ufb01ed recitation While the sampling-based recitation and self- consistency improves the robustness of recite-and-answer method, one argument for its inef\ufb01ciency is that if the evidence-recitation module samples the wrong facts about the question, the question- answering module will not be able to \ufb01gure it out and tend to generate the wrong answer. Therefore, on the one hand, we need to use a low sampling temperature to avoid generating recitations with wrong facts, on the other hand, we want to make sure the sampled recitations have enough diversity. To tackle such a dilemma, we propose passage hint-based diversi\ufb01ed recitation . We observe that in well-formed text knowledge bases, such as Wikipedia, we can usually \ufb01nd a unique passage hint for each passage, by concatenating the section titles and the in-section order of each passage. For example, the passage hint of the second passage in Section 5.2 \u201cEnforcement\u201d of Wikipedia page \u201cChild support\u201d would be \u201cChild support \u2014 Compliance and enforcement issues \u2014 Enforcement \u2014 Paragraph #2\u201d. In passage hint-based diversi\ufb01ed recitation, we \ufb01rst use sampling to generate a diverse set of passage hints, and then use greedy decoding to ensure the factual accuracy of the contents in each passage. Since each passage hint corresponds to a unique passage, we can \ufb01rst de-duplicate the passage hints and then generate the full passages to get more diverse recitation passages. Furthermore, as the recited passages are less likely to be similar due to unique passage hints, inspired by recent progress 5 on question-answering with multiple retrieved passages (Izacard & Grave, 2021), we use aggregated diverse recitations as a single context, and generate the answer with a few more question-answer pair demonstrations. Figure 3 (lower) illustrates the recite-and-answer scheme with passage hint-based diversi\ufb01ed recitation. Fine-tuning on few-shot generated questions We found that although the training data of many existing LLMs (Devlin et al., 2019; Chowdhery et al., 2022) contains the Wikipedia corpora, which are usually regarded as the factoid documents for knowledge-intensive question answering tasks (Joshi et al., 2017; Kwiatkowski et al., 2019), the section titles are usually not explicitly included in training. This makes the pre-trained LLM hard to discover the mapping from the question to the passage hint, and to the full passage merely by few-shot prompting. To address this issue, we propose an additional \ufb01ne-tuning stage to adapt LLMs to learn such map- pings. Assuming we have access to not only a few question-answer pairs, but also the top-retrieved Wikipedia pages for queries issued to the Google search engine by multiple users (Kwiatkowski et al., 2019), we can use few-shot prompting to generated synthetic question-hint-passage pairs and then \ufb01netune the LLMs on the generated data. Speci\ufb01cally, we use the ground-truth evidence and question pairs as the prompt, and generate new questions with in-context learning for randomly sampled passages from Wikipedia pages. Next, based on the few-shot generated questions, we train the LLM to predict the original passage hint, as well as the passage content. Figure 3 (upper) illustrates the whole process of passage hint \ufb01ne- tuning.\n\n4 E XPERIMENTS\n\n In this section, we report empirical evaluations of our proposed RECITE with recite-and-answer schemes on a diverse set of few-shot closed-book question answering tasks and different language models with varying scales\n\n. 4.1 E XPERIMENTAL SETUP 4.1.1 E VALUATION D ATASETS\n\n Natural Questions Natural Questions (Kwiatkowski et al., 2019) consists of questions aggregated from the Google search engine and the answers from the Wikipedia page in the top 5 search results. We treat it as a single-hop question answering task. Since Natural Questions contains the so-called \u201clong answer\u201d annotations, which is a whole HTML bounding box containing enough information to infer the answer, we directly utilize the \u201clong answer\u201d as the ground-truth recitation exemplars in our prompt (Sec. 3.1). In order to make a direct comparison with recent LLMs (Chowdhery et al., 2022; Izacard et al., 2022), we evaluate our methods in 5-shot and 64-shot settings. TriviaQA TriviaQA dataset (Joshi et al., 2017) is constructed by collecting Trivia enthusiast au- thored question-answer pairs and their retrospectively collected evidence. Since there is no obvious way to collect a \u201clong answer\u201d in the retrospective evidence documents (the exact appearances of the answer may contain enough information to infer the answer), we evaluate TriviaQA in the single- hop 5-shot setting, and manually compose the recitation passage from Wikipedia for 5 randomly sampled training questions. The concrete prompt can be found in the appendix. HotpotQA HotpotQA (Yang et al., 2018) is designed to explicitly test QA systems\u2019 ability to perform multi-hop reasoning. It is collected by explicitly composing questions requiring reasoning about multiple supporting context documents. Following Wang et al. (2022a), we evaluate Hot- potQA as a multi-hop question answering task in the 4-shot setting. But instead of chain-of-thought prompting as in (Wang et al., 2022a), we use multiple-recite-and-answer (Sec. 3.1) to achieve multi- step reasoning. We also provide the concrete prompt in the appendix. Metrics We calculate the Exact Matching (EM) and F1 scores for the normalized answers, while the speci\ufb01c text normalization applied on each dataset can be slightly different. 6\n\n4.1\n\n\n\nP RE - TRAINED LANGUAGE MODELS\n\n In-house LM In-house LM is a family of densely activated decoder trained on the language mod- eling objective. It has strong capabilities in in-context few-shot learning, multilingual, as well as reasoning tasks. In this paper, we use the In-house LM model with 62B parameters. UL2 UL2 (Unifying Language Learning, Tay et al. 2022a) is an encoder-decoder model trained on a mixture of denoising tasks in a uni\ufb01ed framework. In this paper, since we mainly focus on the in-context learning ability of language models, we use UL2-20B in the S-Denoiser mode (i.e., pre-trained with the pre\ufb01x language modeling) 1 . OPT OPT (Open Pre-trained Transformer language model, Zhang et al. 2022) is a family of re- cently released open-source densely activated language model that aims to re-reproduce comparable results as GPT-3 (Brown et al., 2020). We use the 30B one 2 in this paper.\n\n4.2 E XPERIMENTS\n\n We use the test split for all tasks if the test split is available and has labels for evaluation, otherwise we use the dev split. In addition, TriviaQA and HotpotQA are too large to run large language models on, so we used the \ufb01rst 1,024 data points for evaluation.\n\n4.2.1 P ROMPT - BASED RESULTS\n\n We report the single-hop closed-book question answering (CBQA) evaluation results on Natural Questions (NQ) and TriviaQA and the multi-hop CBQA evaluation results on HotpotQA. In Tab. 1, we report the results with prompt-based in-context learning and self-consistency. From the tables, we can see that the proposed recite-and-answer scheme can signi\ufb01cantly improve the CBQA performance on both datasets with various pre-trained language models. While the per- formance improvements on NQ is more consistent across different language models, we \ufb01nd that the improvements from recite-and-answer is more signi\ufb01cant on smaller language models on TriviaQA. Our hypothesis is that the Trivia-style question usually contains more contextual information in the question, thus weakened the effectiveness of recitation for strong LLMs like In-house LM. Besides, we can see that the recite-and-answer scheme can outperform the chain-of-thought prompt- ing performance on the multi-hop reasoning task. Interestingly, we also \ufb01nd that for LLMs that have large gains from chain-of-thought (i.e., In-house LM), they also have large improvements from recite-and-answer. 1 This can be achieved by append the \u201c[NLG]\u201d and \u201c[extra id 0]\u201d token to the beginning and the end of the prompt. 2 https://github.com/facebookresearch/metaseq 7\n\n4.2.2 R ESULTS OF PASSAGE HINT - BASED DIVERSIFIED RECITATION\n\n For Natural Questions dataset, since it has the collection of top-retrieved Wikipeida pages corre- sponding to the unannotated queries issued to the Google search engine we additionally report the diversi\ufb01ed recitation results of \ufb01ne-tuned In-house LM model in Tab. 2. From the table we \ufb01nd that diversi\ufb01ed recitation can further improve the performance of In-house LM on the NQ dataset.\n\n4.3 A NALYSIS\n\n\n\n4.3.1 O N THE NUMBER OF SELF - CONSISTENCY PATHS\n\n We analyze how the number of passages recited would affect the performance of recite-and-answer under the self-consistency setting. Due to the costly inference of LLMs, we \ufb01rst sample up to k = 20 recitation passages, and then apply self-consistency to a randomly selected subset of recitations to simulate less paths. For each number of self-consistency paths, we evaluate the randomly selected subsets \ufb01ve times and report the mean and standard deviation. We conduct an analysis on OPT-30B and UL2-20B on the TriviaQA dataset and report the results in Fig. 4. We can see that sampling more recitation passages tends to improve the recite-and-answer performance, while less randomness is observed with more self-consistency paths.\n\n4.3.2 O N THE ROBUSTNESS OF FEW - SHOT EXEMPLARS\n\n A well-known problem of in-context few-shot learning is its instability to the choices of exemplars and their orders (Zhao et al., 2021). We evaluate the robustness of standard prompting and our recite- and-answer method with 5 random seeds and report the mean and standard deviation of UL2 model running on the TriviaQA dataset in Tab. 3. The 5-shot exemplars are randomly sampled and shuf\ufb02ed for each seed. From the table, we can see that with recitation sampling, recite-and-answer exhibits similar robustness (in terms of small performance deviation) as standard prompting under different 8 random seeds and numbers of self-consistency paths. The overall gains by recite-and-answer are signi\ufb01cant compared to standard prompting regardless of the choice of few-shot exemplars.\n\n4.3.3 R ECITATION V . S . R ETRIEVAL V . S . G ROUND - TRUTH\n\n One may ask without the external corpus, whether the quality of recited passages with LLMs is better than simple retrieval models, e g., BM25 (Robertson et al., 2009) 3 . To answer this question, we evaluate the few-shot question-answering performance of UL2 on three kinds of context pas- sages: retrieval, recitation and ground-truth. We report the results on \ufb01rst 1024 validation examples in Natural Questions (NQ) dataset, since it is the only dataset that contains the \u201clong answer\u201d an- notation that can be regarded as ground-truth context passage. From Tab. 4, we can see that the classic retrieval model, i e., BM25 is still a very strong baseline for collecting information from the corpus. Nonetheless, compared to BM25, our recite-and-answer still achieves a quite competitive performance via generation only and without using any external corpus. As expected, question an- swering with either retrieved passages or recited passages signi\ufb01cantly underperform the model with ground-truth passages, meaning there is still headroom in both approaches to improve the retrieval or recitation quality.\n\n4.3.4 E RROR ANALYSIS\n\n We perform an error analysis on the 1024 evaluation examples in the TriviaQA dataset. We classify the errors into three categories: 1) Not Recit., i.e., the correct answer is not recited in any of the 20 recited passages in self-consistency. 2) Hits@20-Recit., i.e., the correct answer can be found in one of the recited passage, but does not appear in the QA module\u2019s outputs. 3) Hits@20-Path, i.e., the correct answer is one of the \ufb01nal outputs of the 20 self-consistency paths, but it does not have the majority votes. We report the results of UL2-20B and OPT-30B in Tab. 5. We can see that \u201cNo Recit\u201d and \u201cHits@20-Path\u201d account for the majority of the errors, meaning that the QA module performs quite well (if the correct answer appears in one of the recitation passages, it will be extracted by the QA module in most of the cases), and the main bottleneck still lies in the recitation quality and answer aggregation strategies.\n\n5 C ONCLUSION & D ISCUSSION\n\n In this paper, we propose a novel recitation-augmented generation framework to improve lan- guage models\u2019 performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a speci\ufb01c knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be bene\ufb01cial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large lan- guage models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach. One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or \ufb01ne-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge-intensive NLP tasks in the closed-book setting, such as fact checking. 3 We use the pre-indexed \u201cenwiki-paragraphs\u201d corpus in the pyserini package ( https://github.com/ castorini/pyserini ), which is originally designed for BERTserini (Yang et al., 2019a). 9\n\nE THICS S TATEMENT\n\n The goal of this paper is to use recitation as an intermediate step to generate more accurate factual knowledge in the model\u2019s outputs. Therefore, our method should in principle improve the faith- fulness of the LLM systems. However, unlike retrieval-augmented generation (RAG) models that can use external trustworthy corpus, all the intermediate steps in RECITE is generated by the LLM itself, RECITE may further enhance the existing biases in the LLMs\u2019 model weights compared to RAG.\n\nR EPRODUCIBILITY S TATEMENT\n\n Model weights The model weights of two LLMs used in our experiments, i.e., UL2-20B (Tay et al., 2022a) and OPT-30B (Zhang et al., 2022), are publicly released through GCP bucket ( gs: //scenic-bucket/ul2 ) and Github ( https://github.com/facebookresearch/ metaseq ), respectively. Evaluation datasets The three evaluation datasets used in our experiments (Natural Questions 4 , TriviaQA 5 , and HotpotQA 6 ) are all publicly accessible. Prompt We provide all the used prompts in the appendix. Source code Though the prompt examples in the appendix should be enough to reproduce all the results in our paper, we plan to open-source all the evaluation code after the acceptance of the manuscript.\n\nR EFERENCES\n\n , , 4 https://github.com/google-research-datasets/natural-questions 5 https://nlp.cs.washington.edu/triviaqa/ 6 https://github.com/hotpotqa/hotpot 10 11 12 13 14\n\nA I LLUSTRATIONS OF P ROMPTS AND L ANGUAGE M ODEL O UTPUTS\n\n Fig. 5-13 illustrate the evidence-recitation, question-answering prompt that we used for Natural Questions, TriviaQA, and HotpotQA dataset. We also provide the example sampled recitations for these datasets. Notice that for Natural Questions, we use the \u201clong answer\u201d annotation as the recitation in the prompt, while for the other two datasets, we manually compose a few recitation passages based on web search.\n\nB P RINCIPLES OF PROMPT DESIGNS\n\n We mainly follow Chowdhery et al. (2022) and use two new line symbols \u201c \\ n \\ n \u201d as the separator between different components within exemplars, and use three new line symbol \u201c \\ n \\ n \\ n \u201d as the separator between different exemplars. For the UL2 (Tay et al., 2022a) model, since its original SentencePiece (Kudo & Richardson, 2018) vocabulary does not encode the new line symbol \u201c \\ n \u201d, we instead use \u201c ; \u201d to replace \u201c \\ n \u201d as the separator in all the prompts.\n\nC D ETAILS OF P ASSAGE H INT - BASED F INE - TUNING\n\n For Natural Questions (Kwiatkowski et al., 2019) dataset, we assume that we have top-retrieved Wikipedia pages for the unannotated queries issued to the Google search engine by multiple user. We collect the passages in these pages as a corpus, and use the rule to annotate the hints of these passages. To make a fair comparison with prompting-based models in both 5-shot and 64-shot, we only use 5 paired \u201clong answer\u201d-question exemplars as the prompt to generate the synthetic question for the sampled passages from the Wikipedia hint-passage corpus, and thus construct the synthetic question- hint-passage paired \ufb01ne-tuning data. We train In-house LM in the constructed corpus for 10,000 steps with a batch size of 64, which takes approximately 1 day in 64 TPUv4 chips 7 . The \ufb01ne-tuned model can be used for passage hint-diversi\ufb01ed recitation without any further prompts.\n\nD L ANGUAGE M ODEL H YPERPARAMETERS\n\n We report the hyperparameters of the LLMs we used in Tab. 6. 7 https://cloud.google.com/tpu/docs/v4-users-guide 15 Question: where did united states drop the atomic bomb The answer to the above question can be found in the following Wikipedia page, section, and paragraph or table: Answer: During the final stage of World War II, the United States detonated two nuclear weapons over the Japanese cities of Hiroshima and Nagasaki on August 6 and 9, 1945, respectively. The United States dropped the bombs after obtaining the consent of the United Kingdom, as required by the Quebec Agreement. The two bombings killed at least 129,000 people, most of whom were civilians. They remain the only use of nuclear weapons in the history of warfare. Question: who sells the most oil in the world The answer to the above question can be found in the following Wikipedia page, section, and paragraph or table: Answer: The top three producers have in recent history been (alphabetically) Russia, Saudi Arabia, and the United States. Each of these countries experienced major production declines at different times in the past, but since 2014 all three have been producing near their peak rates of 9 to 11 million barrels per day. Saudi Arabia and Russia also top the list of oil exporting countries, while Russia is not a member of OPEC. The monthly U.S. oil production reached 10.07 million b/d in November 2017, the highest monthly level of crude oil production in U.S. history. In early 2018, the U.S. government forecast the country will become a net oil and gas exporter by 2022, the first time since 1953. Question: when did day light savings start in the us Recitation: In the UK, the first Daylight Saving Time was first proposed by William Willett, an English builder and Liberal Party politician, in 1907, and after a public consultation his proposal was accepted in 1908. The first Daylight Saving in the UK was introduced on 25 March 1916, and the second on 27 March the following year. It was repealed in May 1919, but reintroduced in May 1919. Recitation: Daylight saving time, also known as daylight savings, and several other variants, is the practice of advancing clocks during summer months by one hour so that evening daylight lasts one . 16 Question: Which physicist's principle asserts that the momentum & position of a particle cannot both be precisely determined at the same time? Recitation: In 1935, Heisenberg (along with his assistants Walther Gerlach and Walther von Braun) published a paper in which they described a \"principle of uncertainty\" that would later be dubbed the Heisenberg uncertainty principle. The principle asserts that the momentum and position of a particle cannot both be precisely determined. Recitation: One of the consequences of the principle of special relativity is that the position and momentum of a moving object cannot both be measured simultaneously with high precision. Recitation: The uncertainty principle states that it is impossible to know both the exact momentum and the exact position of an object (particle) at the same time, that is, the momentum and position are incompatible and, therefore, complementary. The uncertainty principle is one of the fundamental results of quantum mechanics, and is often regarded as one of its most surprising consequences. Recitation: It states that the following two statements cannot both be true: the position of a particle cannot be both precisely determined and known exactly, and the momentum of a particle cannot be both precisely measured and known exactly. Question : Triggered by Rosa Parks' refusal to give up her seat, the public transportation system in what US city was devastated by a year long boycott of their busses? The answer to the above question can be found in the following Wikipedia page, section, and paragraph: Answer : On December 1, 1955, in Montgomery, Alabama, Parks rejected bus driver James F. Blake's order to vacate a row of four seats in the \"colored\" section in favor of a White passenger, once the \"White\" section was filled. Question : What was Beethoven's last symphony? The answer to the above question can be found in the following Wikipedia page, section, and paragraph: Answer : The year 1823 saw the completion of three notable works, all of which had occupied Beethoven for some years, namely the Missa solemnis, the Ninth Symphony and the Diabelli Variations. Question : In which 1972 John Boorman film is a leading character, played by Ned Beatty, raped by a 'Hillbilly'? The answer to the above question can be found in the following Wikipedia page, section, and paragraph: Answer : In 1972, Beatty made his film debut as Bobby Trippe in Deliverance, starring Jon Voight and Burt Reynolds, and set in northern Georgia. Beatty's character is forced to strip at gunpoint by two mountain men who humiliate and rape him, a scene so shocking that it is still referred to as a screen milestone. Question : Which bridge crossing The River Thames did Queen Elizabeth II open on 17th March 1973? The answer to the above question can be found in the following Wikipedia page, section, and paragraph: Answer : Queen Elizabeth II of the United Kingdom opens the new London Bridge. Question : \"The song \"\"My Kind Of Town\"\", written by Sammy Cahn and Jimmy Van Heusen in 1964, was about which city?\" The answer to the above question can be found in the following Wikipedia page, section, and paragraph: Answer : \"My Kind of Town\" made a minor appearance on the U.S. pop charts, reaching #110 in 1964. It was the second of two charting songs about Chicago recorded by Sinatra. The other was \"Chicago (That Toddlin' Town)\" from 1957, which reached U.S. #84. 17 Q: Triggered by Rosa Parks' refusal to give up her seat, the public transportation system in what US city was devastated by a year long boycott of their busses? A: Montgomery, Al Q: What was Beethoven's last symphony? A: 9th Q: In which 1972 John Boorman film is a leading character, played by Ned Beatty, raped by a 'Hillbilly'? A: Deliverance Q: Which bridge crossing The River Thames did Queen Elizabeth II open on 17th March 1973? A: London Bridge Q: \"The song \"\"My Kind Of Town\"\", written by Sammy Cahn and Jimmy Van Heusen in 1964, was about which city?\" A: Chicago Question: Which magazine was started first Arthur\u2019s Magazine or First for Women? The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs: Answer 1: Arthur magazine was a bi-monthly periodical that was founded in October 2002, by publisher Laris Kreslins and editor Jay Babcock. Answer 2: First for Women is a woman's magazine published by Bauer Media Group in the USA. The magazine was started in 1989. Question: The Oberoi family is part of a hotel company that has a head office in what city? The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs: Answer 1: P.R.S. Oberoi is the current chairman of The Oberoi Group. Answer 2: The Oberoi Group is an award-winning luxury hotel group with its head office in New Delhi, India. Question: What nationality was James Henry Miller\u2019s wife? The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs: Answer 1: In 1967, Miller married his fifth wife, Japanese born singer Hoki Tokuda. Answer 2: Hoki Tokuda is an Japanese actress, known for Blind Woman's Curse (1970), The Abalone Girls (1965) and Nippon Paradise (1964). Question: The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year? The answer to the above question can be found in the following two Wikipedia page, section, and paragraphs: Answer 1: House of Anubis is a mystery television series developed for Nickelodeon based on the Dutch\u2013Belgian television series Het Huis Anubis. Answer 2: Het Huis Anubis (English: The House of Anubis) is a Dutch-Belgian children's television mystery drama. It first aired in September 2006. 18 Question: Are Local H and For Against both from the United States? Answer 1: Local H is an American band from San Diego, California. Answer 2: For Against is an American punk band from Los Angeles, California. Answer 1: Local H is a rock band from Los Angeles, California. Answer 2: For Against is an American rock band from Phoenix, Arizona. Answer 1: Local H is an American rock band from Austin, Texas. Answer 2: For Against is a rock band from Nashville, Tennessee. Answer 1: Local H is an American rock band from Chicago, Illinois. Answer 2: For Against is an indie rock band from New York City, New York. 19 20", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes a novel recitation-augmented generation (RECITE) paradigm to improve language models' performance on closed-book question answering (CBQA) tasks.", "topic_class_1": "summary of the paper", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RECITE first recites relevant passages from the language model's own memory via sampling before producing the final answers.", "topic_class_1": "summary of the approach", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors show that utilizing recitation as an intermediate step in a recite-and-answer scheme can achieve state-of-the-art performance on various CBQA tasks.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA datasets (Natural Questions, TriviaQA, and HotpotQA).", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Fine-tuning the pre-trained LMs on synthetic question-passage pairs can further improve recitation and downstream QA accuracy.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Novel recitation-augmented generation paradigm for closed-book QA that improves performance by reciting relevant passages from the LM's memory before answering.", "topic_class_1": "strength on novelty", "topic_class_2": "Opinion on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Experiments on multiple LMs and CBQA datasets demonstrate broad effectiveness of the approach.", "topic_class_1": "strength on strong performance", "topic_class_2": "Opinion on the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Additional fine-tuning on synthetic data further improves recitation and QA performance.", "topic_class_1": "strength on strong performance", "topic_class_2": "Opinion on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Analysis of number of recitations, robustness to few-shot exemplars, and comparison to retrieval provides useful insights.", "topic_class_1": "strength on detailed analysis", "topic_class_2": "Opinion on the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Lacks comparisons to very recent state-of-the-art CBQA methods to contextualize the results.", "topic_class_1": "weakness on missing comparisons with SOTA methods", "topic_class_2": "Concerns on baselines", "reliability": "No", "explanation": "The paper compares the necessary baselines within the scope of in-context few-shot learning.", "error_type": "Out-of-scope"}, {"segment_text": "- Evaluation is limited to CBQA datasets; broader applicability to other knowledge-intensive NLP tasks is not explored.", "topic_class_1": "weakness on experiment coverage", "topic_class_2": "Concerns on the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Updating the LM's knowledge requires costly re-training/fine-tuning, limiting flexibility compared to retrieval-augmented methods.", "topic_class_1": "weakness on flexibility", "topic_class_2": "Concerns on the method", "reliability": "No", "explanation": "It is unfair to consider only the training cost; each has its own advantages.", "error_type": "Inexpert Statement"}, {"segment_text": "- Some key implementation details are missing, such as specifics of the synthetic data generation process.", "topic_class_1": "weakness on implementation details", "topic_class_2": "Concerns on the data", "reliability": "No", "explanation": "The paper explains how to generate the data in section 3.2.", "error_type": "Neglect"}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is generally well-written and easy to follow.", "topic_class_1": "strength on writting", "topic_class_2": "Opinion on writing", "reliability": "No", "explanation": "The figures are a little confusing", "error_type": "Writing"}, {"segment_text": "The proposed recitation-augmented generation approach is novel and the extensive experiments demonstrate the effectiveness and robustness of the method.", "topic_class_1": "strength on novelty and experiment design and analysis", "topic_class_2": "Opinion on novelty and the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Key details are provided to understand the methodology.", "topic_class_1": "comments of the experiment  settings", "topic_class_2": "Opinion on clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, some specifics are lacking that would be needed to fully reproduce the results, such as details on the synthetic data generation process for fine-tuning.", "topic_class_1": "weakness on reproducibility", "topic_class_2": "Concerns on reproducibility", "reliability": "No", "explanation": "The paper plans to provide code.", "error_type": "Unstated statement"}, {"segment_text": "The overall quality of the research appears high, though comparisons to very recent state-of-the-art CBQA methods are missing to fully contextualize the significance of the results.", "topic_class_1": "comments on the overall quality", "topic_class_2": "Concerns on experiments", "reliability": "No", "explanation": "The paper compares the necessary baselines within the scope of in-context few-shot learning.", "error_type": "Out-of-scope"}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and effective recitation-augmented generation approach for closed-book question answering with large language models.", "topic_class_1": "summary of the paper", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on multiple models and datasets provide convincing evidence of the benefits of the recite-and-answer paradigm.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The analysis of various aspects of the method offers useful insights.", "topic_class_1": "summary of the experiments", "topic_class_2": "Opinion on the analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some weaknesses include the lack of comparisons to recent SOTA methods and full reproducibility details.", "topic_class_1": "weakness on missing comparisons with SOTA methods", "topic_class_2": "Concerns on the experiments and reproducibility", "reliability": "No", "explanation": "The paper compares the necessary baselines within the scope of in-context few-shot learning. The paper also plans to provide code.\n", "error_type": "Out-of-scope"}, {"segment_text": "Overall, this work makes a valuable contribution to improving CBQA with language models, though further work is needed to explore broader applicability and knowledge updating.", "topic_class_1": "comments on the overall quality", "topic_class_2": "Suggested paper decision", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces RECITation-augmented gEneration (RECITE), a novel paradigm aimed at enhancing Large Language Models' (LLMs) ability to generate accurate factual knowledge without relying on external document retrieval.", "topic_class_1": "summary of the paper", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Unlike traditional retrieval-augmented models, RECITE generates answers by first reciting relevant passages from its internal knowledge base before producing final answers.", "topic_class_1": "summary of the approach", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study demonstrates RECITE's superior performance over existing methods in closed-book question answering (CBQA) tasks across various LLMs and datasets.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths include the innovative approach to leveraging internal knowledge for generating answers, the comprehensive evaluation across different models and datasets, and the clear demonstration of RECITE's effectiveness in improving CBQA task performance.", "topic_class_1": "strength on the strong performance and writing", "topic_class_2": "Opinion on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses might include the potential for enhanced biases due to the reliance on internal model knowledge and the lack of consideration for time-sensitive information updating.", "topic_class_1": "weakness on potential bias and experiment coverage", "topic_class_2": "Concerns on the method", "reliability": "No", "explanation": "The review is not professional with very uncertain tones. It is also too generic. ", "error_type": "Superficial Review"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written, with a well-structured methodology and detailed experimental setup that facilitates reproducibility.", "topic_class_1": "strength on writting and reproducibility", "topic_class_2": "Opinion on writing", "reliability": "No", "explanation": "The writing is not that clear, some figures are confusing", "error_type": "Writing"}, {"segment_text": "The concept of recitation-augmented generation is novel, addressing the limitations of both direct generation and retrieval-augmented methods in LLMs.", "topic_class_1": "strength on novelty", "topic_class_2": "Opinion on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive experiments and comparisons underline the quality and novelty of the research.", "topic_class_1": "strength on extensive experiments", "topic_class_2": "Opinion on the exeperiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RECITE represents a significant advancement in leveraging LLMs for knowledge-intensive tasks, offering a promising direction for enhancing factual accuracy without external data reliance.", "topic_class_1": "comments on the significance of the paper", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach is innovative, well-executed, and backed by compelling experimental evidence.", "topic_class_1": "comments on the overall quality", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite potential limitations, the paper's contributions to the field are substantial.", "topic_class_1": "comments on the overall quality", "topic_class_2": "Suggested paper decision ", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes RECITE (RECITation-augmented gEneration), a novel approach for improving the factual accuracy of Large Language Models (LLMs) on knowledge-intensive tasks.", "topic_class_1": "summary of the paper", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Unlike retrieval-augmented methods that rely on external corpora, RECITE leverages the LLM's own memory by first reciting relevant passages and then generating the final output based on the recited information.", "topic_class_1": "summary of the approach", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper focuses on closed-book question answering (CBQA) and demonstrates that a recite-and-answer scheme achieves state-of-the-art performance on various CBQA tasks and across different LLM scales.", "topic_class_1": "summary of the experiments", "topic_class_2": "Summary of the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, the paper explores techniques like self-consistency ensembling and passage hint-based diversified recitation to further enhance performance and robustness.", "topic_class_1": "summary of the approach", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel approach: RECITE introduces a new paradigm for knowledge-intensive NLP tasks, utilizing the LLM's internal knowledge without relying on external corpora.", "topic_class_1": "strength on novelty", "topic_class_2": "Opinion on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strong empirical results: The recite-and-answer scheme achieves state-of-the-art performance on multiple CBQA tasks and across different LLM scales.", "topic_class_1": "strength on strong performance", "topic_class_2": "Opinion on the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Detailed analysis: The paper provides extensive analysis on the impact of self-consistency paths, robustness to exemplar choice, and comparison with retrieval and ground-truth passages.", "topic_class_1": "strength on detailed analysis", "topic_class_2": "Opinion on the experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Transparency and reproducibility: The paper includes detailed prompts and plans to open-source the evaluation code, facilitating reproducibility.", "topic_class_1": "strength on reproduciblity", "topic_class_2": "Opinion on code", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited task focus: The paper primarily focuses on CBQA tasks.", "topic_class_1": "weakness on limited evaluation benchmarks", "topic_class_2": "Concerns on the experiments", "reliability": "No", "explanation": "The tasks here are sufficient for the scope of this paper.", "error_type": "Out-of-scope"}, {"segment_text": "Exploring the effectiveness of RECITE on other knowledge-intensive tasks would broaden its impact.", "topic_class_1": "suggestion on experiments", "topic_class_2": "Concerns on the experiments", "reliability": "No", "explanation": "Too generic and out of scope.", "error_type": "Out-of-scope"}, {"segment_text": "Potential bias amplification: As acknowledged in the ethics statement, RECITE might amplify existing biases within the LLM's memory compared to retrieval-based methods that utilize curated external corpora.", "topic_class_1": "weakness on potential bias", "topic_class_2": "Concerns on bias", "reliability": "No", "explanation": "It can apply to many papers.", "error_type": "Superficial Review"}, {"segment_text": "Computational cost: Sampling multiple recitations and employing self-consistency can be computationally expensive, especially for large LLMs.", "topic_class_1": "weakness on computational cost", "topic_class_2": "Concerns on computational cost", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and well-organized.", "topic_class_1": "strength on writting", "topic_class_2": "Opinion on writing", "reliability": "No", "explanation": "The figures are a little confusing", "error_type": "Writing"}, {"segment_text": "The research is of high quality, with well-designed experiments and thorough analysis.", "topic_class_1": "strength on experiment design and analysis", "topic_class_2": "Opinion on quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The RECITE framework and its components are novel and represent a significant contribution to the field of knowledge-intensive NLP.", "topic_class_1": "comments on this approach", "topic_class_2": "Opinion on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors' commitment to sharing prompts and code enhances the paper's reproducibility.", "topic_class_1": "strength on reproduciblity", "topic_class_2": "Opinion on code and reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and effective approach for improving the factual accuracy of LLMs on knowledge-intensive tasks.", "topic_class_1": "summary of the paper", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RECITE leverages the LLM's internal knowledge through a recite-and-answer scheme, achieving state-of-the-art performance on CBQA tasks.", "topic_class_1": "summary of the approach", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While potential limitations exist, the paper's strengths outweigh them, making it a valuable contribution to the NLP community.", "topic_class_1": "comments on overall quality", "topic_class_2": "Suggested paper decision", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "accepted", "title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts", "body_text": "\n\nP-A DAPTERS : R OBUSTLY E XTRACTING F ACTUAL I N - FORMATION FROM L ANGUAGE M ODELS WITH D I - VERSE P ROMPTS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because differ- ent users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters : lightweight models that sit between the embedding layer and \ufb01rst attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the \u201cexperts\u201d) and select one to query the LLM. They require a separate classi\ufb01er trained on human-annotated data to map nat- ural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes a P-Adapter successful and conclude that access to the LLM\u2019s embeddings of the original natural language prompt, particularly the subject of the entity pair being asked about, is a signi\ufb01cant factor.\n\n1 I NTRODUCTION\n\n Recently, there has been an interest in meeting users\u2019 factual information needs using large language models (LLMs) as knowledge bases in place of traditional, index-based IR methods (Petroni et al., 2019; Metzler et al., 2021). LLMs do have some similarities to traditional IR methods. Both process large sets of documents: IR methods to create an index, and LLMs during pretraining. Both also can be queried by users with natural language: IR methods returning a ranked list of relevant docu- ments leveraging their index, and LLMs returning a ranked list of vocabulary items leveraging their pretraining. However, LLM-knowledge bases have the bene\ufb01t that they can interactively generate language to respond to users needs, rather than merely pointing them in to a resource. In this conception of LLM-based knowledge bases, LLM parameters are frozen and end users can query them using their own handwritten, natural language prompts. For example, if one user wants to know what the capital of Canada is, they might query a model with the prompt, \u201cThe capital of Canada is [MASK] .\u201d, while another might use different wording: \u201cCanada, which has the capital city [MASK] .\u201d In order for LLMs to be effective knowledge bases, they have to be robust to different queries users could provide. Unfortunately, prior work has shown that LLMs are not robust: the queries they are prompted with greatly impact the model\u2019s predictions, and semantically equivalent prompts may lead to inconsistent predictions (Jiang et al., 2020; Elazar et al., 2021). In the above example, the \ufb01rst prompt does extract the correct answer \u201cOttawa\u201d from BERT Base, but the second one extracts the incorrect \u201cWinnipeg\u201d. This observation has led many works to try to \ufb01nd the optimal prompt or set of prompts for a given relation: the one(s) that will allow models to extract factual information the best (e.g., for the relation capital of ). These works have proposed ensembling sets of prompts (Jiang et al., 2020), optimizing which tokens are included in the prompt (Haviv et al., 2021; Shin et al., 2020), or forgoing the dif\ufb01cult discrete optimization problem entirely, and instead 1 . optimizing the continuous embeddings that are input to the LLM (Li & Liang, 2021; Qin & Eisner, 2021; Zhong et al., 2021). In our work, rather than focus on \ufb01nding a single optimal prompt, we aim to help LLMs overcome this variability by adapting natural language prompts into continuous representations that allow LLMs to accurately predict factual information. Because we are motivated by a user-focused set- ting, we want our adaptation method to require only a natural language prompt (e.g. \u201cThe capital of Canada is [MASK] \u201d) at inference time with no additional annotation for the relation ( capital of ) between the subject (\u201cCanada\u201d) and object (\u201cOttawa\u201d) of the entity pair in the prompt or even the identity of the subject separate from the prompt. At training time, our ideal method would only rely on (prompt, object) pairs (e.g. ( \u201cThe capital of Canada is [MASK] \u201d, \u201cOttawa\u201d ) ), so collecting new data would not require additional annotation. We introduce a class of models called P-Adapters for performing this adaptation, which satisfy our inference and training desiderata. P-Adapters sit between the embedding layer and the \ufb01rst attention layer of the frozen LLM, and modify the LLM\u2019s embeddings so the factual information can be more effectively predicted (Figure 1). They are opti- mized end-to-end, only requiring (prompt, object) pairs at training time. We also investigate other models that could increase the consistency of natural language prompts: Mixture of Experts (MoE) models. Rather than adapt a natural language prompt, a MoE model maps it to a canonical contin- uous prompt (an \u201cexpert\u201d) based on the relation between the entities in the prompt, and queries the LLM with that expert. These methods require additional annotations during training and inference (namely the relation between the entities and the identity of the entity subject), and therefore do not conform to our desiderata. However, we include them in our discussion because they are use- ful for comparing to previous work and for understanding why P-Adapters are successful. We \ufb01nd that P-Adapter and MoE methods both strongly outperform a baseline of using the natural language prompts themselves by between 12-28% on precision and 36-86% on consistency. While MoE mod- els usually perform better, the small increase comes at the cost of requiring relation annotations at training time and specifying the subject of the entity at inference time. To simulate the variable kinds of natural language prompts we expect our methods to handle, we evaluate them in three out-of-distribution (OOD) settings: on different set of natural language prompts, on a different distribution of entities (uniform rather than the distribution of Wikidata), and on natural language prompts with typographical errors. These OOD evaluations are also impor- tant because recent work has found that models that appear to have knowledge of factual information are often relying on surface-level feature associations. For example, models ignore negation, utilize stereotypical associations when making predictions (Kassner & Sch\u00a8utze, 2020; Poerner et al., 2020), and pick up on the distribution of objects from their training corpus rather than learn how to access arbitrary facts (Zhong et al., 2021; Cao et al., 2021). We \ufb01nd that the OOD distribution of entities is the most challenging setting, and while typographic errors reduce model performance, they do so by less than might be expected. Finally, we take a deep dive into what makes a P-Adapter effective, \ufb01nding that it is important to keep some of the original natural language prompt embeddings available to the LLM. In particular, 2 the availability of the subject of the prompt is the most important factor, in contrast with some other work suggesting the subject matters less (Cao et al., 2021). Overall, we conclude that our method is helpful in reducing the impact of variable prompts, and is able to do so without requiring additional annotations. We hope P-Adapters are taken up by the community as effective ways to extract factual information from LLMs.\n\n2 R ELATED W ORK\n\n Prompting Language Models for Factual Knowledge. Prompting has taken off as a low- parameter alternative to \ufb01netuning LLMs. In prompting, the LLMs are usually frozen and their masked language model pre-training task is used to \ufb01ll in the relevant information, which is usually a token (Trinh & Le, 2018; Davison et al., 2019; Petroni et al., 2019). Cao et al. (2021) outlines three main approaches for prompting LLMs for factual information. The \ufb01rst involves querying them zero-shot with a prompt that contains the single entity of interest (like in Figure 1) (Petroni et al., 2019). The second consists of querying them in a few-shot setting, where a few completed ex- ample prompts are provided in addition to the query of interest (e.g. \u201cThe capital of France is Paris. The capital of Canada is [MASK] \u201d) (Lu et al., 2021; Gao et al., 2021). The third involves querying LLMs with additional context that likely contains the answer to the query (e.g. \u201cOttawa sits on the Ottawa River at the border between Quebec and Ontario. The capital of Canada is [MASK] .\u201d) (Petroni et al., 2020; Bian et al., 2021). In this work, we focus on the \ufb01rst approach because it is similar to how IR systems are currently used in practice. Much previous work in this setting endeavors to understand how much factual information LLMs store. (Petroni et al., 2019) use one natural language prompt for each relation that they evaluate in the LAMA dataset. Subsequent work argues that this approach of using a single natural language prompt per relation underestimates the ability of language models to predict factual information given language models are very sensitive to the input prompts. To this end, Jiang et al. (2020) and Elazar et al. (2021) generate paraphrases of prompts in the LAMA dataset and ensemble them to get a score for each relation. Other methods argue that natural language prompts might not be the best, and instead optimize a discrete (Shin et al., 2020; Haviv et al., 2021) or continuous (Qin & Eisner, 2021; Zhong et al., 2021; Liu et al., 2021b) prompt for each relation. Challenges with Prompting. While the idea of querying LLMs for factual information has re- ceived recent interest, there are notable shortcomings such as source attribution, updating missing or outdated knowledge, and even the consistency of model predictions (Metzler et al., 2021). We focus on the issue posed by consistency. Elazar et al. (2021) thoroughly investigated this issue, \ufb01nding models often predict different entities for semantically equivalent prompts. Further, Elazar et al. (2021) proposed to continue training LLMs with a consistency loss function to improve their robustness. In this work, however, we focus on a user-centric and lightweight approach that does not require updating LLM parameters. Robustness and Extracting Facts. Poerner et al. (2020) and Kassner & Sch\u00a8utze (2020) argue that LLMs exploit surface form regularities when making predictions. Zhong et al. (2021) and Dufter et al. (2021) make a related observation, concluding that simpler models like randomly initialized LLMs, static embeddings, and even a Naive Bayes model can achieve an accuracy better than a majority baseline. Cao et al. (2021) argue that prompts, that were found to do well in previous work, over\ufb01t the distribution of objects in the training data rather than enabling knowledge extrac- tion. They show that prompting with different sets of entities leads to very similar language model predictions. Therefore, we adopt an out-of-distribution evaluation setting to mitigate these concerns. Low parameter \ufb01netuning methods (Houlsby et al., 2019; Ben Zaken et al., 2021; Liu et al., 2021a) have also been shown to have better OOD performance for generation and classi\ufb01cation tasks (Li & Liang, 2021; Lester et al., 2021), which we hope to leverage for factual extraction here.\n\n3 T ERMINOLOGY\n\n Taking inspiration from Liu et al. (2021a), we use the following terminology throughout this work. Each fact we want to extract consists of two parts: a pair of entities (a subject ( x ) and an object ( y ) \u02d9) 3 and the relation ( r ) between them. We use the 41 relations proposed by the LAMA dataset (Petroni et al., 2019). Each relation is expressed through a number of templates (e.g., one might be \u201cThe capital of [X] is [Y].\u201d). To query a language model for factual information, we substitute the subject into its place in the template, and a [MASK] token into the object\u2019s place to form a prompt ( x (cid:48) ). We refer to this as a natural language prompt to emphasize that it consists of natural language tokens. Our prediction problem then consists of mapping x (cid:48) to y . They way LLMs perform this mapping is by using their embedding layer, e , to embed x (cid:48) into a sequence of continuous vectors, and then input this sequence to the \ufb01rst attention layer of the LLM. We refer to this sequence of continuous vectors that are input to the LLM as a continuous prompt ( x (cid:48) cont ). We make this distinction because our P-Adapters sit between the embedding layer and the \ufb01rst attention layer of the LLM, so they take as input e ( x (cid:48) ) and output a continuous prompt x (cid:48) cont that is different from e ( x (cid:48) ) . After passing x (cid:48) cont into the rest of the LLM, we take the LLM\u2019s prediction ( y (cid:48) ) to be the token assigned the highest probability in the location of the [MASK] token in x (cid:48) , and compare it to y to assess if the model knows the fact.\n\n4 M ETHODS 4.1 D ATA\n\n We use the entity pairs and relations from the T-Rex split of the LAMA work (Elsahar et al., 2018; Petroni et al., 2019) in our experiments. This data includes 41 relations and approximately 1000 entity pairs for each relation. We pool the templates from the LAMA, LPAQA, and ParaRel datasets (Jiang et al., 2020; Elazar et al., 2021). LPAQA includes templates created automatically with a paraphrase model, mined from Wikipedia, and written by annotators, and ParaRel contains high- quality human-written prompts. Additionally, we use the BERT lexical substitution system from Lee et al. (2021) to augment our pool of ParaRel templates. This gives us on average 81.4 prompt templates per relation (More statistics available in Figure 6 in the Appendix). For training and validation, we use the entity pairs from the training and validation splits of Shin et al. (2020) respectively. We randomly divide the templates into two groups: one for training and one for OOD natural language prompt evaluation. To test extracting arbitrary factual knowledge (rather than over\ufb01tting to training templates and entity pairs distributions), we perform evaluations in four different settings: The \ufb01rst setting is on ID templates and objects. We use the entity pairs from the LAMA dataset and the templates that were seen during training time. The entity pairs are different from what was seen at training-time, but they still follow the distribution of objects in Wikidata\u2014the source of the training entity pairs. Second, we evaluate on OOD natural language prompts ( OOD Prompts ). This tests whether our P-Adapters generate effective continuous prompts for novel templates. We use the same entity pairs as in the ID evaluation, but use the templates we did not train with. The next evaluation setting is on an OOD distribution of objects ( OOD Objects ). This tests whether our P-Adapter models learn to match the distribution of objects in the training set rather than predict arbitrary object entities. To evaluate this, we use the uniform-wikidata dataset from Cao et al. (2021). This dataset was constructed so that each object appears the same number of times for each relation, in contrast to the ID evaluation set which contains a more uneven distribution of objects from Wikidata. Finally, we evaluate on templates with typographic errors ( OOD Keyboard Errors ). This assesses the robustness of our P-Adapters to potential typos in the natural language prompts. We use the entity pairs and the templates from the ID evaluation, except we introduce at least one typographic error in each template using the nlpaug package (Ma, 2019).\n\n4.2 M ODELS\n\n First we introduce our P-Adapter models, and then we introduce our MoE models, and we \ufb01nish up with our oracle and baseline models. Recall that P-Adapters intercede between the LLM\u2019s embed- ding layer, e , and the \ufb01rst attention layer (Figure 1). The LLMs that we extract factual information 4 (cid:37)(cid:50)(cid:53)(cid:49)(cid:66)(cid:44)(cid:49) (cid:17)(cid:17)(cid:17)(cid:3) (cid:8)(cid:42)(cid:39)(cid:31)(cid:1)(cid:19)(cid:32)(cid:39)(cid:28)(cid:47)(cid:36)(cid:42)(cid:41) (cid:16)(cid:45)(cid:28)(cid:30)(cid:39)(cid:32) (cid:44)(cid:54)(cid:66)(cid:38)(cid:36)(cid:51)(cid:44)(cid:55)(cid:36)(cid:47) (cid:37)(cid:50)(cid:53)(cid:49)(cid:66)(cid:44)(cid:49) (cid:17)(cid:17)(cid:17)(cid:3) from are BERT Base Cased, BERT Large Cased, and RoBERTa Large (Devlin et al., 2018; Liu et al., 2019). P-Adapter. Our P-Adapter models take as input the embeddings of the natural language prompt, e ( x (cid:48) ) , and output a new sequence of continuous embeddings that are used as input to the LLM in place of the original natural language prompt, x (cid:48) cont . Formally, a P-Adapter is a function f prompt : e ( x (cid:48) ) \u2192 x (cid:48) cont trained to maximize P LM ( y | x (cid:48) cont ) . The LLM\u2019s prediction would then be: where V is the LLM vocabulary. There are many different possible parameterizations of f prompt , and we describe the three here. Two of these (Rewrite P-Adapter and Pre\ufb01x P-Adapter) \ufb01t the training and inference data criteria put forth in the introduction. The third (P-Tuning P-Adapter) does not, but we include it because it provides insight into what makes a P-Adapter effective (See Figure 2a for an illustration of each.) The simplest adapter we investigate is the Rewrite P-Adapter . The Rewrite P-Adapter is parame- terized by a bidirectional LSTM followed by an MLP layer. The output of the MLP is x (cid:48) cont , and is input to the LLM. We use LSTMs to better compare our results with those of Liu et al. (2021b) who use also use them, and because we want to use only a small number of parameters. Additionally, the prompts tend to be short and we do not do any pre-training, so LSTMs are effective. Next we look at the Pre\ufb01x P-Adapter . The Pre\ufb01x P-Adapter is parameterized similarly to the Rewrite P-Adapter; however, it prepends the output of its MLP to e ( x (cid:48) ) . Additionally, the pre\ufb01x is set to be a \ufb01xed length, as is common in many works that learn a prompt pre\ufb01x (Li & Liang, 2021; Lester et al., 2021). We choose a pre\ufb01x length of nine to compare with later models. Formally, using the Pre\ufb01x P-Adapter, the input to the LLM is [ g ( e ( x (cid:48) )); e ( x (cid:48) )] , where g is the application of a Bi-LSTM, a max-pool, and an MLP. Unlike the Rewrite P-Adapter, the Pre\ufb01x P-Adapter keeps all of the embeddings e ( x (cid:48) ) accessible to the LLM\u2019s \ufb01rst attention layer. Finally, we investigate the P-Tuning P-Adapter . This P-Adapter is based on work by Liu et al. (2021b) in which they present a method called P-Tuning that learns a continuous prompt for each relation in the LAMA dataset. They learn a function g : r \u2192 R d \u00d7 9 , where d is the hidden dimension of the LLM. The x (cid:48) cont consists of the output of g , the embedding of the subject, and the embedding 5 of the mask token. It has the following form: where bracket notation represents python-style indexing. Our P-Tuning P-Adapter is implemented in a similar fashion, except our g takes as input x (cid:48) , and is parameterized similarly to the Pre\ufb01x P-Adapter. Like the Pre\ufb01x P-Adapter, a \ufb01xed number of embeddings are output; however, unlike the Pre\ufb01x P-Adapter, only the embeddings for the subject and [MASK] token are kept unchanged, rather than all of e ( x (cid:48) ) . Note that this P-Adapter uses more annotations during training and inference than the other two: speci\ufb01cally, it requires knowing the identity of the subject. This means that it does not \ufb01t our desiderata laid out in the introduction, but we include because it allows us to investigate what makes a P-Adapter successful. All of these adapters were trained using the hyperparameters from Liu et al. (2021b): Adam opti- mizer with a learning rater of 0 . 00001 , weight decay of 0 . 0005 , a batch size of 128 , and an expo- nential learning rate decay schedule with a decay rate of 0 . 98 (Kingma & Ba, 2015). Enforcing consistency does not require modifying a natural language prompt like the P-Adapters do. Another option is to map each prompt to a canonical prompt, and query the LLM with this prompt. We explore this next with our MoE and Oracle methods. In these, the canonical prompt is a continuous prompt for the relation between the entities in the natural prompt learned with P-Tuning. In contrast to the P-Adapter methods, these require additional annotations with the relation of the prompt (See Figure 2b). Mixture of Experts. Our Mixture of Experts (MoE) model consists of two components. The \ufb01rst is a classi\ufb01er that predicts the relation between the entities of a natural language prompt. This classi\ufb01er is a BERT Base Cased model fully \ufb01ne-tuned on a 41-way classi\ufb01cation task using an AdamW optimizer with a learning rate of 0 . 001 and linear learning rate decay (Loshchilov & Hutter, 2018). We use HuggingFace Transformers to train the model for 3 epochs on the same training data used to train the P-Adapter models (Wolf et al., 2020). The second component is a look-up table to map the predicted relations to the cannonical continuous prompts. We train these using the same hyperparamters reported in Liu et al. (2021b) with one prompt for each of the 41 relations. Note that this is similar to a traditional mixture of experts model approach except that we do not use a weighted combination of prompts from different relations and instead just use the single prompt from the predicted relation. Oracle. Our oracle method is similar to the MoE approach, except rather than using a classi\ufb01er to predict the relation, we use the gold relation at test time. We call this method the oracle because it achieves perfect consistency (not precision). The oracle also serves as a direct comparison to P-Tuning because P-Tuning assumes access to the gold relation. Baseline. Our baseline is a model that takes as input the natural language prompt without any pre\ufb01xes or optimization. Any effective and useful P-Adapter network would have to perform better than just using the natural language text itself.\n\n4.3 M ETRICS .\n\n We report two metrics: precision@1 (P@1) and consistency. P@1 is a stringent measure of whether a prompt can extract a fact. It is de\ufb01ned as the proportion of prompts where the correct object is the LLM\u2019s top prediction. Consistency measures whether a model\u2019s predictions for a given entity pair match across the different prompts x (cid:48) \u2019s that contain the entities. We calculated con- sistency using the method from Elazar et al. (2021): the consistency of knowledge of a fact is the proportion of pairs of prompts for the fact where the model makes the same prediction. For- mally, given a set of unordered pairs of prompts, X (cid:48) , where there are n unique prompts, and letting v ( x (cid:48) ) = arg max v \u2208V P LM ( v | f prompt ( e ( x (cid:48) ))) return the model\u2019s top prediction given a natural lan- guage prompt x (cid:48) , the consistency is de\ufb01ned as: 6 Note a model can be consistent but is not accurate (e.g., by always predicting the same incorrect object).\n\n5 R ESULTS\n\n whether they require additional training/inference data. Note that the Baseline does not have any standard deviation because there is no optimization across runs, and that the consistency of the Oracle is 1.0. (OOD KE: OOD Keyboard Errors) We present our main results for BERT Base in Table 1. The results for BERT Large and RoBERTA Large show similar trends and are available in the Appendix (In Tables 2, 3 as well as Figures 4 and 5). Comparing the different evaluation sets, we observe the following. First, the OOD Objects was the most challenging setting, on average 20.41% lower precision than the ID setting, even for the oracle. However, the models performed similarly on the OOD Prompts as they did on the in domain ones. At \ufb01rst this might seem to con\ufb02ict with previous work that \ufb01nds that the prompt has a large impact on performance (Jiang et al., 2020), but these results make claims about average prompt performance rather than individual prompt performance. That said, precision is still higher ID than OOD, particularly for the MoE model. While this suggests some over\ufb01tting, our methods still outperform the baseline, so what they learn is still useful for the OOD natural language prompts. Finally, when evaluated on the OOD Keyboard Errors, we \ufb01nd that the models still do well compared to the baseline. However, precision on the corrupted prompts is lower than the uncorrupted ones. Across all evaluation sets, we \ufb01nd that optimized prompts lead to higher precision than natural language ones, as the low performance of the baseline indicates. Additionally, across all variation types we can see that optimized prompts lead to more consistency among the models\u2019 predictions. The consistencies dramatically increase from less than 0 . 2 for the baseline to over 0 . 4 (Figure 5). Models were the least consistent on the OOD Keyboard Errors, and interestingly, were similarly consistent between the OOD Prompts and OOD Objects evaluations despite having a much higher precision for the OOD Prompts. The oracle has a perfect consistency of 1 . 0 , because it uses the same continuous prompt for all facts with the same relation, so the predictions for that fact are all the same. The MoE model has a high consistency for a similar reason: as long as the classi\ufb01er predicts two prompts for the same entity pair come from the same relation, the predictions will be the same, and therefore consistent. The P-Tuning P-Adapter often has a a high consistency as well, sometimes even higher than the MoE. This suggests that in P- Tuning P-Adapter, the main factor driving the model\u2019s prediction is the subject of the entity pair, because the unmodi\ufb01ed embedding of the subject is shared among all of the prompts for the same object. This last points contributes to a more general observation: we found that giving the LLM access to the embeddings of the natural language prompt was helpful. The Pre\ufb01x P-Adapter and the P-Tuning P-Adapter, which have access to some unmodi\ufb01ed LLM embeddings, achieve higher precisions 7 than the Rewrite P-Adapter, which alters all of the LLM embeddings. Because of this, we perform additional experiments to see what part of natural language prompt was important to keep. One hypothesis is that the [MASK] token embedding has to remain unmodi\ufb01ed, and the Rewrite P- Adapter performs poorly because it corrupts this embedding. We investigate this in two ways. First, we train a version of the Rewrite P-Adapter that replaces the P-Adapter\u2019s output in the [MASK] to- ken position with the LLM\u2019s [MASK] token embedding. Second, we train a version of the P-Tuning P-Adapter whose output only has the LLM\u2019s [MASK] embedding, not the subject embedding. How- ever, we \ufb01nd these do not increase performance much compared to the Rewrite P-Adapter. Another hypothesis is that the subject embeddings must remain unmodi\ufb01ed, and that the Rewrite P-Adapter corrupts the subject, leading to lower precision. To address this, we train a version of the Rewrite P-Adapter model that replaces the embeddings for the subject in the P-Adapter output with the LLM\u2019s embeddings for the subject. We \ufb01nd this is much more effective, increasing preci- sion by 13% . For comparison with the P-Tuning P-Adapter, which keeps the subject and [MASK] embeddings, we also train a version of the Rewrite P-Adapter that replaces the embeddings at both the subject and [MASK] token positions with the LLM\u2019s embeddings and \ufb01nd that this performs only 4% worse than the P-Tuning P-Adapter. The results are visible in Figure 3. From these ex- periments, we conclude that the most important aspect of the prompt is the subject token. This also helps explain why the Pre\ufb01x P-Adapter does well: it has access to the LLM\u2019s embeddings for the entire natural language prompt, which includes the subject. Unfortunately, while these changes to the Rewrite P-Adapter models show positive results, they require knowing the index of the subject tokens , which contradicts our requirement to not use extra annotations. Fortunately, there is another way to incorporate the unmodi\ufb01ed LLM\u2019s embeddings of the natural language prompt into the output of the P-Adapter: we can interpolate between the P-Adapter output and the unmodi\ufb01ed embeddings. To do this, we train a third type of Rewrite P-Adapter. If f rewrite-prompt is the original Rewrite P-Adapter, the equation for the new Rewrite P- Adapter is: where \u03b1 is a hyperparameter. When \u03b1 = 0 , this P-Adapter is equivalent to the original Rewrite P-Adapter, and when \u03b1 = 1 it is equivalent to the baseline. We test \u03b1 \u2208 { 0 . 1 , 0 . 25 , 0 . 5 , 0 . 75 , 0 . 9 } , and \ufb01nd \u03b1 = 0 . 5 to perform the best. It outperforms the Rewrite P-Adapter when the subject and [MASK] tokens are substituted in, though barely underperforms compared to the P-Tuning P-Adapter (See Figure 3).\n\n6 D ISCUSSION\n\n Our goal was to create models to adapt arbitrary user queries so predictions from frozen LLMs were accurate and consistent. One of our desiderata was to train with only (prompt, object) pairs, so that a user\u2019s experience would match current IR system experiences. Despite this requirement, some of the methods we introduce do use additional information\u2014for comparison to other methods and understanding our own. For example, the P-Tuning P-Adapter and the MoE models both need the identity of the subject of the user\u2019s prompt, and requiring a user to provide this is feels cumbersome and redundant. One could obviate this need using an off-the shelf NER system as well, and we leave this to future work. Additionally, the MoE models train their classi\ufb01ers using annotations for the relation between the entities in the prompts. We use these annotations to compare to previous work: previous work as- sumes that relation is the most useful intermediate variable for factual extraction, but this does not have to be the case. For example, good prompts across different relations like \u201cOttawa is the capital of [MASK] \u201d and \u201cThe second largest country by land area is [MASK] \u201d might share components even though they do not share a relation. Our P-Adapters allow for greater \ufb02exibility by condition- ing on the natural language prompt itself rather than the relation. Importantly, while using extra information led to better results in some cases, the Pre\ufb01x P-Adapter was usually able to achieve comparable precision. And while the Rewrite P-Adapter \ufb02oundered, ensembling its outputs with the LLM embeddings led to greater success. There are two likely reasons for the P-Adapters\u2019 success. First, not updating the LLM parameters likely maintains all of factual knowledge learned during pretraining. Second, the natural language 8 N one [MASK ] [MASK ] S ub j e c t S ub j e c t + [ MASK] S ub + [ MASK] 0.0 0.1 0.2 0.3 0.4 P @ 1 Unmodi\ufb01ed Embeddings Rewrite P-Tuning Precision vs \u03b1 prompts are likely close to being effective, so they do not require large changes to elicit the correct objects. The pushes -Adapters are suf\ufb01cient It is also worth noting that while the Pre\ufb01x P-Adapter was able to approach the performance of the techniques with more information, this performance is still quite modest. The oracle was able to achieve only a precision of 50% in the cases with ID objects, missing half of the facts. For the OOD Objects evaluation set, it performed much worse, only correctly predicting about 25% of the facts. While the P@1 metric is quite strict, these numbers do not bode well for this approach. Cao et al. (2021) blame this poor performance on the optimized prompts over\ufb01tting to the training object distribution, and they observe that when the subjects are changed, the LLM\u2019s distributions do not change all that much. They even observe that the LLM\u2019s distributions when prompts have subjects are similar to those when the subjects are completely omitted. Our \ufb01ndings point to a more nuanced result. While it might be true that the LLM\u2019s distributions in these cases are correlated, the subject does matter when considering the top prediction. We \ufb01nd that without the LLM\u2019s embeddings of the subject accessible, the P-Adapters performs even more poorly.\n\n7 C ONCLUSION\n\n LLMs implicitly learn factual information during pretraining, but accurately and consistently ex- tracting this information presents a challenge. Our work complements previous work on extracting factual information from LLMs by introducing a more user-oriented setting: we want our models to only take in varied natural language prompts, and return the objects that meet information needs. To do this, we propose P-Adapter models, which sit between the embedding layer and the \ufb01rst attention layer of LLMs. These are effective compared to other methods that require additional annotations (e.g., the relation between the entities in the prompt), and we perform ablations determining that their success is due to keeping the LLM\u2019s embeddings of the subject available to the LLM. While we focus on the task of extracting factual information from LLMs, P-Adapters potentially provide a general framework for adapting to variable inputs, for example in reading comprehension or dia- logue. While there is still room for the further improvement of using LLM\u2019s as knowledge bases, and we see P-Adapters as an important part of the future of this endeavor. 9\n\n8 E THICS S TATEMENT\n\n While our work is inspired by how users interact with current IR systems we do not perform any experiments with human subjects. Additionally, our method improves the ability to extract factual information from Pretraining corpora, so this means it may be possible to exploit our method to elicit the biased or harmful from these corpora as well, though we do not study this. Finally, improving the ability to accurately and consistently extract information from text sources has privacy concerns if LLMs are pretrained on private or otherwise sensitive data. For the LLMs we study, this is not a concern as they were trained on Wikipedia and Books Corpus data rather than Web data.\n\n9 R EPRODUCIBILITY S TATEMENT\n\n The descriptions of our models and training procedure in Section 4 and the code uploaded in the supplementary material is suf\ufb01cient for reproducing our results. The data we use is available in the Github repositories of the work we cite that creates these resources. For running experiments, see the README.md \ufb01le. We will upload our code to Github and place a link in this section upon acceptance.\n\nR EFERENCES\n\n /L18-1544 . 10 11 . learn- ing to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5017\u20135033, On- line, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021 . naacl -main. 398. URL https://aclanthology.org/2021.naacl-main.398 .\n\nA A LL R ESULTS B T EMPLATES\n\n 12 13 OOD OOD Keyboard OOD OOD OOD 14 subject Canada object Ottawa relation IS CAPITAL template The capital of [X] is [Y]. natural language prompt The capital of Canada is [MASK] . prediction Toronto 15", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes P-Adapters, lightweight models that sit between the embedding layer and first attention layer of large language models (LLMs).", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "P-Adapters take the LLM embeddings of natural language prompts as input and output continuous prompts that are fed into the LLM to extract factual information.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The goal is to make the LLM's factual predictions more accurate and consistent across diverse phrasings of the input prompt.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors also investigate mixture of experts (MoE) models that map the prompt to a learned continuous prompt, but these require additional annotations.", "topic_class_1": "Baseline Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on the LAMA dataset show that P-Adapters substantially improve the precision and consistency of factual extraction compared to using the natural language prompts directly.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Ablation studies demonstrate that preserving access to the LLM's original embeddings of the subject in the prompt is important for P-Adapter performance.", "topic_class_1": "Analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Addresses an important problem of extracting factual knowledge consistently from LLMs", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "No", "explanation": "Not address, it is more like alleviate accordinge to the performance", "error_type": "Misunderstanding"}, {"segment_text": "- P-Adapters are lightweight and require no additional prompt annotations at inference time", "topic_class_1": "Efficient", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Substantial improvements in precision and consistency over the baseline", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Insightful ablation studies on what makes P-Adapters effective", "topic_class_1": "Analysis Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Overall precision is still modest, with the best models only correctly predicting 50% of facts", "topic_class_1": "Effectiveness Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Analysis of what embeddings to keep focuses on subject/object, overlooking other aspects of the prompt that could be important", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "Well, this comment is very blurred. The reviewer should clearly point out which aspect s/he think is missed, instead of the blurred term \"other aspects.\"", "error_type": "Vague Critique"}, {"segment_text": "- No human evaluation of the interpretability of the continuous prompts", "topic_class_1": "Missing Experiment", "topic_class_2": NaN, "reliability": "No", "explanation": "This is not the focus of this paper while it is good to have, but not must.", "error_type": "out-of-scope"}, {"segment_text": "- More complex tasks beyond factual extraction are not explored", "topic_class_1": "Missing Experiment", "topic_class_2": NaN, "reliability": "No", "explanation": "The focus of this paper is to improve the consistency of factual knowledge extraction from pre-trained LMs. Of course there are no more complex tasks beyond factual knowledge extraction.\nThe scope of this paper is about factual knowledge extraction, so it is not appropriate to list this as a weakness", "error_type": "out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and the proposed P-Adapter approach is novel in aiming to adapt diverse natural language prompts into effective continuous prompts for factual extraction.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments cover multiple out-of-distribution settings to test robustness.", "topic_class_1": "Comprehensive Experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Key implementation details, hyperparameters, and datasets are specified.", "topic_class_1": "Comprehensive Details", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Code is provided to aid reproducibility.", "topic_class_1": "Reporducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents P-Adapters, a novel lightweight approach to adapt natural language prompts into continuous prompts for more accurate and consistent factual extraction from LLMs.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "P-Adapters demonstrate substantial improvements over baselines while requiring no additional annotations at inference time.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Insightful ablations reveal the importance of preserving access to the original subject embeddings.", "topic_class_1": "Ablation Strength", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While overall precision remains modest, this work is a promising step towards practical use of LLMs as knowledge bases.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clear and technically sound.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces P-Adapters, a novel approach for extracting factual information from Large Language Models (LLMs) more consistently across varied natural language prompts.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By positioning lightweight models between the LLM's embedding layer and its first attention layer, P-Adapters transform embeddings based on the natural language prompts into continuous prompts that the LLM can interpret more effectively.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This technique is compared with Mixture of Experts (MoE) models, which also aim to improve the consistency of factual extraction but require additional annotations.", "topic_class_1": "Baseline Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study demonstrates that P-Adapters achieve a significant improvement in precision and consistency over baseline methods that directly use natural language prompts, without necessitating extra annotation efforts for mapping natural language prompts to their continuous counterparts.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The methodology presents a significant innovation in handling variability in natural language prompts for querying LLMs, offering a practical solution to improve the robustness of factual information extraction without additional annotation costs.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Experimental results show clear benefits in terms of precision and consistency, with P-Adapters outperforming baseline approaches by a notable margin across different settings, including out-of-distribution scenarios.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is well-organized, articulating the problem, proposed solution, and results in a coherent and comprehensible manner, contributing positively to its clarity and overall quality of research.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the paper shows the effectiveness of P-Adapters in improving precision and consistency, there is limited discussion on the potential scalability and adaptability of the proposed method to LLMs beyond BERT and RoBERTa or to languages other than English.", "topic_class_1": "Limited scope", "topic_class_2": NaN, "reliability": "No", "explanation": "While BERT and RoBERTa are the most famous encoder-only pre-trained models, the authors do need to test other structures, like decoder-only models (e.g., GPT2) and seq2seq models (e.g., T5). This part of the comment is correct. However, I don't think involving langauges other than English is needed.\nEvaluation on BERT structure and on English provide enough validation for the methods. Although more is better, but that should not be a weakness", "error_type": NaN}, {"segment_text": "- The empirical evaluation primarily focuses on the quantitative benefits of P-Adapters, with less emphasis on the qualitative analysis of why certain prompts work better and how P-Adapters manage to improve upon them.", "topic_class_1": "Lacking Case Study", "topic_class_2": NaN, "reliability": "No", "explanation": "Suggesting out-of-scope analysis. ", "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written, detailing the methodology, experimental setup, and results with sufficient clarity to enable reproducibility.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach is novel, addressing the inconsistency problem in querying LLMs with natural language prompts in a unique and effective way.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors provide a comprehensive comparison with existing methods, demonstrating the quality and novelty of their contributions.", "topic_class_1": "Comprehensive Experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, they mention that the code and data used for experiments will be made available, supporting the paper's reproducibility.", "topic_class_1": "Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper introduces a promising approach to enhance the consistency and precision of factual information extraction from LLMs using P-Adapters.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By innovatively transforming natural language prompts into continuous prompts, the authors address a significant challenge in NLP, making a notable contribution to the field.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite some limitations in the scope of evaluation and qualitative analysis, the paper stands out for its clear presentation, novelty, and potential impact on the usage of LLMs as knowledge bases.", "topic_class_1": "Strength & Weakness Tradeoff", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes P-Adapters, lightweight models that improve the robustness and consistency of factual information extraction from Large Language Models (LLMs).", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "P-Adapters sit between the embedding layer and the first attention layer of an LLM, taking LLM embeddings as input and outputting continuous prompts used to query the LLM.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also investigates Mixture of Experts (MoE) models, which learn a set of continuous prompts and select one based on the relation between entities in the prompt.", "topic_class_1": "Baseline Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "P-Adapters are shown to perform comparably to MoE models while requiring less training data and annotation.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They demonstrate significant improvements in both precision and consistency compared to using natural language queries alone, even in out-of-distribution settings.", "topic_class_1": "Experiment Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addresses an important problem: P-Adapters tackle the issue of inconsistency in factual information extraction from LLMs, which is crucial for reliable knowledge base applications.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "No", "explanation": "Not address, it is more like alleviate accordinge to the performance", "error_type": "Misunderstanding"}, {"segment_text": "Lightweight and efficient: P-Adapters require minimal additional parameters and training data compared to other approaches like MoE models.", "topic_class_1": "Efficiency", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Improved robustness and consistency: P-Adapters demonstrate significant improvements in both precision and consistency across various evaluation settings, including out-of-distribution scenarios.", "topic_class_1": "Effectiveness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "User-friendly: P-Adapters only require natural language prompts at inference time, aligning with current user expectations for IR systems.", "topic_class_1": "Easy to use", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Modest overall performance: While P-Adapters improve upon the baseline, the absolute precision achieved by even the best models remains relatively low, particularly in out-of-distribution settings.", "topic_class_1": "Effectiveness Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited interpretability analysis: The paper claims improved interpretability of generated prompts but lacks a dedicated study or human evaluation to support this claim.", "topic_class_1": "interpretability Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "The paper does not claim it improves interpretability.", "error_type": "Unstated statement"}, {"segment_text": "Unclear experimental details: Some experimental details, such as the number of samples needed for training, are not fully elaborated.", "topic_class_1": "Missing Experiment Details", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors should consider conducting a human evaluation to assess the interpretability of the generated prompts.", "topic_class_1": "Missing Experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further analysis is needed to understand the factors limiting the overall performance, particularly in out-of-distribution settings.", "topic_class_1": "Analysis Weakness", "topic_class_2": NaN, "reliability": "No", "explanation": "This point has a huge overlap with Limited interpretability analysis", "error_type": "Duplication"}, {"segment_text": "Providing more detailed information about the experimental setup and hyperparameter choices would enhance the paper's reproducibility.", "topic_class_1": "Missing Experiment Details", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper presents a valuable contribution to the field of factual information extraction from LLMs.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "With further investigation and refinement, P-Adapters have the potential to significantly improve the robustness and consistency of LLM-based knowledge bases.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity and Quality: The paper is generally well-written and easy to follow, although some experimental details could be clearer.", "topic_class_1": "Clarity", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The P-Adapter approach is novel and offers a promising direction for improving factual information extraction from LLMs.", "topic_class_1": "Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors provide code and data resources to facilitate the reproduction of their results.", "topic_class_1": "Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "P-Adapters present a novel and efficient approach to address the critical issue of inconsistency in factual information extraction from LLMs.", "topic_class_1": "Method Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They demonstrate promising improvements in both precision and consistency, even in out-of-distribution settings.", "topic_class_1": "Impact", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the overall performance remains modest, and further analysis is needed to support claims of improved interpretability.", "topic_class_1": "Effectiveness Weakness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models", "body_text": "\n\nS AME P RE - TRAINING L OSS , B ETTER D OWNSTREAM : I MPLICIT B IAS M ATTERS FOR L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Language modeling on large-scale datasets leads to impressive performance gains on various downstream language tasks. The (validation) pre-training loss (or perplexity in autoregressive language modeling) is often used as the evaluation metric when developing language models since the pre-training loss tends to be well-correlated with downstream performance (which is itself difficult to evaluate comprehen- sively). Contrary to this conventional wisdom, this paper shows that 1) pre-training loss cannot fully explain downstream performance and 2) flatness of the model is well-correlated with downstream performance where pre-training loss is not. On simplified datasets, we identify three ways to produce models with the same (sta- tistically optimal) pre-training loss but different downstream performance: continue pre-training after convergence, increasing the model size, and changing the training algorithm. These experiments demonstrate the existence of implicit bias of pre- training algorithms/optimizers\u2014among models with the same minimal pre-training loss, they implicitly prefer more transferable ones. Toward understanding this im- plicit bias, we prove that SGD with standard mini-batch noise implicitly prefers flatter minima in language models, and empirically observe a strong correlation be- tween flatness and downstream performance among models with the same minimal pre-trainingloss. Wealsoproveinasyntheticlanguagesettingthatamongthemodels with the minimal pre-training loss, the flattest model transfers to downstream tasks.\n\n1 I NTRODUCTION\n\n Large language models (LLMs) trained on internet-scale data have improved performance on a wide array of downstream tasks (Devlin et al., 2018; Yang et al., 2019; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020). These models are trained with a language modeling pre-training loss to \u201cfill in the blanks\u201d\u2014either predicting the next token/word (autoregressive language modeling loss, or perplexity) or masked tokens (masked language modeling (MLM) loss). In common practice, the validation pre-training loss is used to monitor the training process (Brown et al., 2020; Zhang et al., 2022a) and compare different models since the pre-training loss is generally strongly correlated with downstream performance (Hernandez et al., 2021). Moreover, theoretical works on understanding LLMs also focus on how the pre-training loss affects downstream performance. Saunshi et al. (2020); Wei et al. (2021); Xie et al. (2021) show that good pre-training loss, or fitting the language modeling conditional probability well, is a main reason for downstream success of LLMs. Their analyses generally treat the language models as blackboxes and do not take into account how the models represents the conditional probability. In this paper, we question the conventional wisdom on the correlation between the validation pre-training loss and downstream performance for language modeling. Recent works have demonstrated that models with different architectures may have the same pre-training loss but different performance (Saunshi et al., 2022; Tay et al., 2021). Due to the expressivity of modern neural nets, many parameter configurations even within the same architecture can still have the same pre-training loss. A priori, it is unclear why all these configurations should have the same downstream performance. We find that different parameter configurations with the same pre-training loss can indeed have different downstream performance, especially when the pre-training loss reaches a near-optimal level. Concretely, using simplified text datasets, we find three situations that demonstrate such a phenomenon: 1 \u2022 Even after the pre-training loss converges, models at a later time step still tend to perform better. \u2022 Models trained by standard algorithms have better performance than adversarially trained models with the same pre-training loss. \u2022 Larger models tend to perform better downstream than smaller models even if they have the same pre-training loss. These situations are most prominent in the saturation regime , where the models are close to the minimal possible pre-training loss (aka the entropy of the conditional probability, which can be estimated in our simplified datasets). In the saturation regime, the pre-training loss of all models are almost the same, but the transferability to downstream tasks varies. Interestingly, this phenomenon also holds when linear probing on contextualized presentations is used for evaluating downstream performance instead of finetuning. Thus, even though the predicted conditional probabilities of two models are the same (and correct), the contextualized representations can behave differently. In each of the first two cases above, we find two models with the same pre-training loss and the same architecture; but one has a better performance than the other. They only differ by the training algorithms that are used to produce them. Therefore, this suggests the training algorithms have an implicit bias toward one of these models\u2014standard algorithms with more training steps biases towards parameter configurations that transfer better to downstream tasks. The third case has a more subtle but similar interpretation. There exists a hypothetical large model that represents the smaller model with worse performance (by padding zeros to the smaller model). The training algorithm on the large architecture could have chosen it, but did not. This suggests the algorithm has an implicit bias against the hypothetical model (which has an equally good loss). In supervised settings, optimizers are known to have an implicit bias toward selecting generalizable models among all models with small empirical loss. E.g., see Damian et al. (2021); Li et al. (2021), which show that SGD implicitly biases toward flatter minima, and references therein. However, the role of implicit bias in self-supervised learning has not been studied and is conceptually different. Unlike in supervised learning, the gap between empirical and population self-supervised losses is typically small, and thus implicit bias does not seem to contribute to bridging this gap. Instead, the implicit bias selects local minima of the population self-supervised loss that transfer better to downstream tasks. Why do the algorithms bias toward some type of models? In Section 3, we provide a first-cut theoretical analysis of the implicit bias in language modeling. Fortunately, despite the conceptual differences, mathematical tools from supervised settings can be straightforwardly adapted to language modeling settings. We prove that mini-batch SGD prefers flatter minima of population pre-training loss among all minima in the saturation regime. Interestingly, we obtain cleaner theoretical results for the standard mini-batch SGD, without the artificial label noise introduced in prior works (Damian et al., 2021; Li et al., 2021), partly because the mini-batch noise for LLMs does not vanish even at convergence. We corroborate our theory with empirical evidence in Section 4. We show that for models with the same pre-training loss in the three situations above, flatness of the model (measured by the trace of Hessian of the loss, as predicted by the theory) strongly correlates with the downstream performance. Finally, to complement the theory and experiments above, we also rigorously formalize the connection between flatness and downstream performance in a simplified Dyck language setting in Section 5. In this setting, we prove that there are many models with good MLM pre-training loss; among them, the flattest model learns the most useful features for downstream tasks. Here, results from the supervised setting cannot be readily adapted since they are obtained (partially) via generalization bounds (Wei & Ma, 2019a;b), which do not apply to the language modeling setting where the implicit bias is not related to the gap between the empirical and population loss. Proving the correlation between flatness and downstream performance in more general settings likely requires highly non-trivial and novel theoretical tools, and we hope to motivate future work on this topic.\n\n2 T HE E XISTENCE OF I MPLICIT B IAS IN L ANGUAGE M ODELING\n\n In this section, we systematically investigate the relationship between pre-training loss and downstream performance with experiments. We find out that models with the same pre-training loss but different training procedures can have different downstream performance. 2 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Number of / 1000 48 50 52 54 D o w n s t r e a m A cc u r a c y % Linear Probe 3 4 5 6 7 8 P r e - t r a i n i n g L o ss PCFG, Task C, 41M Pre-training ( 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Number of Steps / 1000 53 54 55 56 57 58 D o w n s t r e a m A cc u r a c y % Linear Probe 2 3 4 5 6 7 8 P r e - t r a i n i n g L o ss OPT, QNLI, 235M Pre-training\n\n2.1 F ORMULATIONS\n\n Masked language modeling. Consider a vocabulary W = { 0 , 1 ,...,c } , where 0 is a special token for the mask. Let x =[ x 1 ,...,x T ] denote the input sequence of length T , and x \u2212 t =[ x 1 ,...,x t \u2212 1 , 0 ,x t +1 ,...,x T ] denote the masked sentence, where t is sampled uniformly randomly and independently from [ T ] . 1 The MLM conditional probability refers to the probability of x t given the rest of the sequence Pr ( x t | x \u2212 t ) . We use Pr ( \u00b7| x \u2212 t ) to denote the c -dimensional probability vector Pr( \u00b7| x \u2212 t ) := [Pr( x t =1 | x \u2212 t ) ,..., Pr( x t = c | x \u2212 t )] \u2208 R c . In MLM pre-training, the model f \u03b8 ( \u00b7 ) (parameterized by \u03b8 ) outputs the predicted MLM conditional probability vector f \u03b8 ( x \u2212 t ) \u2208 R c . The model is trained to predict the masked token x t given the rest of the sentence x \u2212 t with cross entropy loss, L ( \u03b8 )= E x,t [ \u2113 ( f \u03b8 ( x \u2212 t ) ,x t )]= E x,t [ \u2212 log([ f \u03b8 ( x \u2212 t )] x t )] . Downstream evaluation. The language model f \u03b8 is composed of a feature extractor h \u03c8 , which outputs a sequence of contextual representations, and a linear classifier that outputs the conditional probability at every position. On downstream tasks, we use a randomly initialized g \u03d5 on top of the pre-trained h \u03c8 . In fine-tuning, both g \u03d5 and h \u03c8 are trained, while in linear probe, only g \u03d5 is updated. For fine-tuning, we use the contextual representations of the cls token. For linear probe, we concatenate the contextual representations of all the tokens together. Saturation regime. To study models with the same pre-training loss, we introduce the saturation regime in this paper, where the model output equals the true conditional probability, f \u03b8 ( x \u2212 t ) = Pr( \u00b7| x \u2212 t ) . In the saturation regime, the MLM loss is equal to the entropy of the true conditional probability L ( \u03b8 ) = E x,t [ \u2212 log(Pr( x t | x \u2212 t ))] = 1 T (cid:80) Tt =1 H ( x t | x \u2212 t ) , which is also the optimal pre-training loss. Thus, all models in the saturation regime have the same, optimal pre-training loss, and we will show that they behave differently on downstream tasks. Our experiments use expressive enough architectures such that there are multiple parameter configurations in the saturation regime for our simplified datasets. For real large-scale data, it is currently computationally challenging to arrive at the saturation regime. However, we hope that our experiments can provide insights for even larger models in the future and for other regimes where pre-training loss does not explain downstream performance.\n\n2.2 E XPERIMENTAL S ETUP\n\n We design controlled experiments to study the correlation between pre-training loss and downstream performance. In particular, we will find a set of models with almost the same pre-training loss. We effectively use the same architecture family so that the main difference between the models only stems from training algorithms. More details are provided in Section A. Datasets. We introduce three generative models to produce simplified datasets, with which we can study various factors systematically. With the knowledge of the true generative models that generate the data, we can compute the true conditional probability and scale up the models until they approach the saturation regime to ensure they have almost the same pre-training loss. Moreover, we can generate unlimited amount of text for pre-training to avoid overfitting to the empirical pre-training loss. 1) PCFG -generated dataset. PCFG (Chomsky, 1956) generates sentences with probabilistic trees and is widely used to understand natural language (Johnson, 1998; Roark & Bacchiani, 2003; Kim et al., 2019; Yang et al., 2021). We randomly generate the production rules which satisfy the Chomsky Nor- mal Form (Chomsky, 1956). The non-terminal symbols in the parse tree can be viewed as intrinsic 1 For simplicity, we only consider masking out one one token in each sentence . 3 2 5 9 12 41 80 235 436 730 / M 30 40 50 60 70 80 D o w n s t r e a m A cc u r a c y % PCFG, Task B Fine-tuningLinear Probe 3 . 203 . 223 . 243 . 263 . 283 . 303 . 323 . 343 . 36 P r e - t r a i n i n g L o ss Pre training 4 10 29 67 40 50 60 70 80 90 D t cc u % 3 . 75 3 . 76 3 . 77 3 . 78 3 . 79 3 . 80 P r e - t r a i n i n g L o ss Pre training ( HMM Task 10 5 12 41 80 235 436 / M 52 54 56 58 60 62 64 66 68 70 D o w n s t r e a m A cc u r a c y % OPT, QNLI Fine Probe 1 . 9 2 . 0 2 . 1 2 . 2 2 . 3 2 . 4 P r e - t r a i n i n g L o ss Pre training ( OPT quantities associated with the sentence such as sentiment and syntax. Thus we design three down- stream tasks A, B, and C to classify non-terminal symbols at different positions of the parse trees. 2) HMM-generated dataset. HMM samples the hidden variables from the transition probabilities and the tokens from the emission probabilities. (Wei et al., 2021; Xie et al., 2021) also analyze the properties of pre-trained language models with HMMs. We generate the transition and emission probabilities as random block-diagonal stochastic matrices. The downstream task is to classify the hidden variable in the sentence. We use task- k to refer to classifying the k -th hidden variable. 3) OPT-generated dataset. We also introduce a more realistic pre-training dataset generated by the OPT models (Zhang et al., 2022a). Starting from the bos token, we sample each token from the conditional probability output by the OPT model. For computational feasibility we only allow to generate the top-2000 most frequent tokens in the OPT vocabulary. We use QNLI and SST-2 from GLUE (Wang et al., 2018) as downstream tasks. Note that the true conditional probability can be computed efficiently for the three datasets given the knowledge of the generated models. For PCFG and HMM-generated datasets, we can compute the true conditional probability with the inside algorithm (Lari & Young, 1990) and the Viterbi algorithm (Forney, 1973), respectively. For the OPT-generated dataset, we can calculate the MLM conditional probability from the joint probability, and the joint probability can be decomposed into the autoregressive conditional probability of the OPT model. Models and algorithms. For PCFG and OPT generated datasets, we use transformers (Vaswani et al., 2017) following the implementation of BERT (Devlin et al., 2018). We use different model sizes ranging from 2M to 730M. For the HMM generated dataset, we use LSTM (Hochreiter & Schmidhuber, 1997) from 10M to 135M. In pre-training, all the models are pre-trained with AdamW following the protocol of Izsak et al. (2021). We use batch size 4096, and train each model for more than 30K steps until the pre-training loss converges. For comparison, we also consider other training algorithms. The first adversarial algorithm is inspired by Liu et al. (2020); Raghu et al. (2021), where the models are pre-trained with an additional meta-learning objective which messes up downstream performance. The second algorithm is manually setting the weight of the model to represent a lookup table which memorizes all the masked sentences and the corresponding true conditional probability. When the transformer is sufficiently large, the lookup table can be encoded as shown in Yun et al. (2019).\n\n2.3 R ESULTS\n\n We compare the downstream performance of models with the same pre-training loss in the following situations: (2) training for different number of steps after the pre-training loss converges, (2) using dif- ferent model sizes, and (3) training with normal training algorithms vs. adversarial training algorithms. In Figure 1, we plot the validation pre-training loss and the downstream performance of different models checkpoints along pre-training. After the pre-training loss converges, although the pre-training loss does not improve, the downstream accuracy continues increasing. Even with the same pre-training loss, larger models are better than smaller models. In Figure 2, we plot the pre-training loss and the downstream performance of models with different sizes. As we increase the model size, the pre-training loss approaches the entropy of the true conditional probability, which is 3.196, 3.758, and 1.865 for PCFG, HMM, and OPT respectively. For PCFG and OPT-generated datasets, we use the vertical dashed line to indicate the place where the pre-training loss saturates as 4 we scale up the model. For the much simpler HMM, the smallest 4M model can fit pre-training close to the entropy of the true conditional probability. With the same pre-training loss, scaling up the models improves linear probe performance by 6 . 9% , 4 . 5% , and 2 . 0% , on PCFG, HMM, and OPT generated data, respectively. See Section A for results on other downstream tasks. Naturally trained transformers are better than adver- sarially trained ones. In Table 1, we evaluate the 235M transformers on PCFG tasks A and B with different pre-training algorithms. Although the ad- versarially trained transformer has almost the same pre-training loss as the normally trained 235M trans- former, it is more than 6 % worse than the normal 235M model, and even worse than a normal 9M model on the downstream task B. The lookup table has perfect pre-training loss, but it performs worse than all normally trained transformers in Figure 2(a) on task B. Note that this is different from the label-orthogonal training in Saunshi et al. (2022). They find out models with the same pre-training loss and different downstream performance by subtracting the mean of the representations, essentially changing the architecture, while our experiment compares models with the same architecture. The experiments above indicate that for models with the same architecture family and the same pre- training loss, the choice of training algorithms, model sizes, and the number of steps that the optimizer works can affect the downstream performance. This indicates the existence of implicit bias of the training algorithms toward choosing more transferable parameter configurations among those with the same pre-training loss and architecture.\n\n3 I MPLICIT B IAS L EADS TO F LAT S OLUTIONS IN L ANGUAGE M ODELING\n\n As discussed in the introduction, the difference between the role of implicit bias in supervised learning andlanguagemodelingisconceptual, becausethegapbetweenempiricalandpopulationself-supervised loss is small and thus implicit bias is not needed for bridging this gap. Instead, the implicit bias benefits the performance on downstream tasks by picking networks that are more adaptable to those tasks. Fortunately, the mathematical tools developed for supervised learning can be adapted to language modeling, which even allows cleaner results by removing some artificial assumptions like adding label noise. Concretely, we show that mini-batch SGD can find models in the flatter areas of pre-training loss landscape. The flatness is measured by the trace of Hessian of the pre-training loss Tr[ \u2207 2 L ( \u03b8 )] . See Figure 4 (Left) for an illustration of the implicit bias. We analyze SGD on the population cross-entropy loss L ( \u03b8 )= E x,t [ \u2212 log([ f \u03b8 ( x \u2212 t )] x t )] with freshly sampled data at every iteration, because, as argued, the difference between empirical and population pre-training loss is not our focus. For simplicity, we present the results for batch size =1 , though they can be generalized to arbitrary batch size (see discussion below Theorem 3.3). Let \u03b7 be the learning rate and let \u03b8 \u03b7k denote the parameter at step k . We drop the superscript \u03b7 when there is no ambiguity. We will show that the implicit bias kicks in when SGD reaches a global minimizer\u2014it drives the iterate towards flatter global minimizers. For simplicity of demonstration, we analyze the process starting from a global minimizer \u03b8 , i.e. , we assume that \u03b8 \u03b7 0 = \u03b8 (for all \u03b7 ). At each iteration k , we get a fresh sample ( x,t ) , where x is a sentence and t is the position of the masked token, and update the parameter \u03b8 by \u03b8 k +1 = \u03b8 k \u2212 \u03b7 \u2207 \u03b8 \u2113 ( f \u03b8 k ( x \u2212 t ) ,x t ) . We assume the network is sufficiently expressive such that there are many fundamentally different global minimizers of the pre-training loss L . As a (non-trivial) regularity condition, following prior works (Fehrman et al., 2020; Li et al., 2021; Arora et al., 2022), we also assume that the minimizers of the loss function L are connected and form a smooth manifold. Assumption 3.1. Assume that the loss L is a C 3 -smooth function, and that the set of global minimizers, \u0393 , is a ( d \u2212 M ) -dimensional C 2 -submanifold of R d for some integer 1 \u2264 M \u2264 d , where for all \u03b8 \u2208 \u0393 , rank (cid:0) \u2207 2 L ( \u03b8 ) (cid:1) = M . A key observation for language model is that even if the model reaches the saturation regime, that is, the model reaches a point on the manifold \u0393 of the minimizers, the optimization process still has non-vanishing gradient noise, because the cross-entropy loss is typically non-zero at the global minimizers and thus the stochastic gradient variance is also non-zero. 2 Therefore, the dynamics of 2 This is in contrast with typical supervised setting where the empirical 0-1 loss and cross-entropy loss can both achieve zero and consequently the mini-batch noise vanishes. Such a difference enables us to prove cleaner results (without the label noise) than in the supervised setting (Damian et al., 2021; Li et al., 2021). 5 SGD do not completely stop; instead, the iterate oscillates around the manifold \u0393 . It turns out that this oscillation in turn encourages the parameter to move in a certain direction along the manifold, determined by the covariance structure of the stochastic gradient. The following lemma shows that the covariance of stochastic gradient for language models in the saturating regime has a favorable property, i.e. , it is equal to the Hessian of pre-training loss. Lemma 3.2 (Bartlett identity) . For any \u03b8 \u2208 \u0393 , \u03a3( \u03b8 ) = \u2207 2 L ( \u03b8 ) , where \u03a3( \u03b8 ) is the covariance of the stochastic gradient at \u03b8 , that is, \u03a3( \u03b8 ) = E t,x (cid:2) \u2207 \u03b8 log[ f \u03b8 ( x \u2212 t )] x t ( \u2207 \u03b8 log[ f \u03b8 ( x \u2212 t )] x t ) \u22a4 (cid:3) \u2212 \u2207 L ( \u03b8 ) \u22a4 \u2207 L ( \u03b8 ) . Though we give a proof of the lemma in Appendix C for completeness, the formula holds for the MLE loss of any well-specified probabilistic models at a global minimizer, and both the gradient covariance and the Hessian equals to the Fisher information matrix. With Lemma 3.2, we can invoke Corollary 5.2 of Li et al. (2021) to derive the following theorem which says that SGD will locally decrease the trace of Hessian along the solution of ordinary differential equation (1) defined below. where \u2207 \u0393 = P \u22a5 \u0393 \u2207 is the Riemannian gradient on manifold \u0393 , or just the ordinary gradient projected back to the tangent space of \u0393 at \u03b8 . In other words, the ODE (1) is essentially a projected gradient descent algorithm with loss function Tr[ \u2207 2 L ( \u03b8 )] , the constraint set \u0393 , and infinitesimal learning rate. We show that SGD effectively minimizes the trace of the Hessian Tr[ \u2207 2 L ( \u03b8 )] with the constraint set \u0393 similarly to ODE in (1). Theorem 3.3. Suppose the loss function L and the manifold of global minimizers \u0393 satisfy Assumption 3.1. For any K > 0 such that ODE (1) has a solution { \u02c6 \u03b8 ( t ) } Kt =0 , it holds that \u03b8 \u03b7K/\u03b7 2 converges in distribution to \u02c6 \u03b8 ( K ) as \u03b7 \u2192 0 . Finally, we note that the above result can be extended to an arbitrary batch size B . The covariance of stochastic gradient at \u03b8 with batch size, denoted by \u03a3 B ( \u03b8 ) , satisfies that \u03a3 B ( \u03b8 )= 1 B \u03a3( \u03b8 ) . Therefore \u03a3 B ( \u03b8 )= 1 B \u2207 2 L ( \u03b8 ) and we can again invoke Corollary 5.2 of Li et al. (2021) to derive the same result as in Theorem 3.3 but with the coefficient 14 in equation (1) replaced by 14 B .\n\n4 F LATTER M ODELS H AVE B ETTER D OWNSTREAM P ERFORMANCE\n\n In this section, we demonstrate with experiments that the flatness is well correlated with downstream performance in the setting introduced in Section 2. Evaluation of flatness. As in Theorem 3.3, we measure the flatness of different models in Section 2 by the trace of Hessian of the pre-training loss (smaller trace of Hessian indicates flatter minima.) Note that when the model approaches the saturation regime, the trace of Hessian is approximately the second order derivative times the square of the norm of the Jacobian, which is a high-dimensional matrix. For computational feasibility, we adopt a technique inspired by Wei et al. (2020) to unbiasedly estimate the trace of Hessian with random samples. Details are provided in Section B. 6 56 57 58 59 60 D o w n s t r e a m A cc u r a c y % OPT Linear Probe, QNLI 1 . 901 . 9 5 2 . 002 . 052 . 102 . 152 . 202 . 252 . 30 P r e - t r a i n i n g L o ss 26.9 12.7 8.5 4.6 3.2 Pre-training Trace of Hessian ( Results. In Figure 3, we compare the downstream accuracy and the trace of Hessian of different check- points obtained at different times during pre-training. On the PCFG and HMM datasets, the trace of Hes- sian demonstrates a clear decreasing trend after the validation pre-training loss converges, following the predictionofTheorem3.3. Furthermore, asthetraceofHessiandecreases, thedownstreamperformance improves by 1.6 % and 4.0 % on the PCFG and HMM datasets, respectively. We compare the trace of Hessian of the models pre-trained with adversarial algorithm and standard AdamW in Table 2. The trace of Hessian of the adversarially pre-trained model is 3 times larger than the normally pre-trained model, corresponding to a drop of 5.5 % in downstream performance. In Figure 5, we compare the downstream accuracy and the trace of Hessian of models with different sizes. On the dataset generated by a PCFG, the pre-training loss is almost the same for models larger than 9M. As we increase the model size, the trace of Hessian of the pre-training loss decreases from 2.68 to 1.54, correlating with the increase of linear probe accuracy from 43.2 % to 50.3 % . On the OPT and HMM-generated datasets, we can also observe an increase in linear probe accuracy with a sharp decrease in the trace of Hessian, as we increase the model size. Interaction between implicit bias and model size. Intuitively, the implicit bias drives the model toward flat minima on both larger models and smaller models. The smaller transformer architecture is a subset of the larger transformer architecture family (as justified in Section B). Thus the flattest minimum found within a larger transformer is flatter than the flattest minimum found within a smaller transformer, and performs better downstream. (See Figure 4 (Right).)\n\n5 F LATNESS R EGULARIZATION P ROVABLY I DENTIFIES T RANSFERABLE M ODELS ON S YNTHETIC L ANGUAGE\n\n Toward formally proving the connection between the flatness regularization (introduced by the stochas- tic gradient as argued in Section 3) and the downstream performance, we consider a setting with synthetic Dyck language. The simplicity of the data allows us to sharply analyze the internal working of a single-layer transformer (with an attention layer and an MLP layer) for masked language modeling. We show that multiple parameter configurations can predict the conditional probability well, including one ideal model that learns the correct representations capturing the intrinsic structure of the sentence, 7 \ud835\udc66 = \u22121 <<>-<> 1 1 -1 \u00b11 1 -1 <-<><> 1 \u00b11 1 -1 1 -1 Encodings Input \ud835\udc4b \ud835\udc44 \ud835\udc3e Softmax Attention 1-1 \u00b11 1 1-1 \ud835\udc67 1 1 1 1 1 1 -1 -1 -1 -1 -1 -1 \ud835\udc49 \ud835\udc49 Configuration (1) Configuration (2) -1 1 \ud835\udc62 \ud835\udc62 \ud835\udc66 = \u22121 \ud835\udc65 ! \ud835\udc65 \" \ud835\udc65 # \ud835\udc65 $ \ud835\udc65 % \ud835\udc65 & and many \u201ccheating\u201d models that essentially memorize the conditional probability using random features. We will prove that the flattest model is the desired model that transfers to downstream tasks. Pre-training Distribution. Consider a variant of the Dyck language (Nivat, 1970) consisting of matching brackets. The vocabulary of the language has two brackets \u27e8 and \u27e9 . Each sentence is composed of a sequence of tokens such that the total numbers of \u27e8 \u2019s and \u27e9 \u2019s are equal. To sample from the pre-training distribution P , we first draw a sentence uniformly over all valid sentences with even length T . Then, we randomly select one position in [ T ] and replace the bracket with a mask token. Downstream Task. The most intrinsic property about the synthetic language is the difference in the number of left and right brackets in a prefix, and thus we use it as the downstream task. Concretely, for any sequence x in {\u27e8 , \u27e9} \u2217 of length T , let g \u2217 ( x ) count the number of mismatches in x : Thus, the sentence x is a valid string in the language if and only if g \u2217 ( x )=0 . For MLM, the masked token can also be recovered from g \u2217 ( x ) : g \u2217 ( x ) = 1 if the masked token is \u27e8 , and g \u2217 ( x ) = \u2212 1 if the masked token is \u27e9 . To evaluate if the model learns the structure, we consider a downstream distribution of sentences which do not necessarily belong to the the language. Each token is sampled from {\u27e8 , \u27e9} uniformly, randomly, and independently. Encoding of the Inputs. With a slight abuse of notation, we also denote by x t the encoding of the t -th token. We encode the input as a one-hot vector in dimension d = 2 T , where the index of the nonzero element encodes the position and the sign encodes the bracket. Concretely, let e t \u2208 R d be the natural basis vector where the t -th entry is 1. Let x t = e t if the t -th token is \u27e8 and x t = \u2212 e t otherwise. If the position t is a mask, we set x t to v , where v \u223c Unif ( {\u00b1 e t + T } ) . Examples of the encodings with T =6 are provided in Figure 6 (Left). Note that the target function can be expressed as g \u22c6 ( x )= \u2212\u27e8 1 T , [ (cid:80) Tt =1 x t ] 1: T \u27e9 with this input encoding, where 1 T is the all one vector in R T and [ a ] 1: T refers to the first T coordinates in a . Models and Algorithms. Suppose Q,K \u2208 R k \u00d7 d are the query and key matrices, V \u2208 R m \u00d7 d is the value matrix and u \u2208 R m is the parameter of the output layer. Let \u03c8 =( Q,K,V ) . A single-layer transformer is composed of an attention layer and an MLP layer. [ Attn \u03c8,u ( x )] t = 1 m u \u22a4 \u03c3 ( (cid:80) T j =1 a t,j V x j ) , where the attention score a t, 1: T = softmax ( \u27e8 Qx t ,Kx t \u27e9 , \u00b7\u00b7\u00b7\u27e8 Qx t ,Kx T \u27e9 ) . \u03c3 ( x )=max { x, 0 } is the relu activation. We use the output of the first token, f \u03c8,u ( x )=[ Attn \u03c8,u ( x )] 1 . We use the squared loss for both MLM and downstream adaptation. The loss function of MLM is L ( \u03c8,u ) . In downstream adaptation, we have a finite dataset { x ( i ) } ni =1 sampled i.i.d. from P ds . The training loss with n data is (cid:98) L P ds ( \u03c8,u ) , and the population loss for the downstream task is L P ds ( \u03c8,u ) . Main Intuitions. We are interested in two kinds of parameter configurations both with good pre- training loss: (1) learning the natural and transferable features 1 T and (2) fitting the pre-training task by memorizing the masked sentences. We construct the two solutions as follows. For solution (1), first note that the softmax attention layer can take the average of all the token encodings [ x t ] T t =1 in a sentence. Let us denote the sum by z \u2208 R d , z = (cid:80) Tt =1 x t . Note that the first T coordinates in z are \u00b1 1 indicating the bracket type and the last T coordinates indicate the position of the mask (See Figure 6 (Right)). On top of z , two neurons can predict the masked token in MLM perfectly. Consider the two neurons V 1 =[ 1 T ; 0 T ] , V 2 =[ \u2212 1 T ; 0 T ] . Then g \u2217 ( x )= \u03c3 ( V \u22a4 2 z ) \u2212 \u03c3 ( V \u22a4 1 z ) , which is the transferable solution. For solution (2), we set the entries in V to i.i.d. samples from N (0 ,T ) . If m is sufficiently large, we can find the coefficient u to express g \u2217 ( x ) with random Gaussian features, i.e. g \u2217 ( x )= u \u22a4 \u03c3 ( V z ) . 8 We observe that the trace of Hessian of configuration (1) is smaller than configuration (2), due to a main difference between them\u2013the cancellation between activated neurons. In configuration (1), for every possible input, only one of the neurons \u03c3 ( V \u22a4 1 z ) and \u03c3 ( V \u22a4 2 z ) is activated. In contrast, in configuration (2), many neurons can be activated at the same time. Among them, the output coefficient u i \u2019s contain both positive and negative values, leading to cancellation between activated neurons. In Lemma D.1, we link the trace of the Hessian with the cancellation between neurons. Indeed, we show that the mimimum of trace of the Hessian can be achieved only if there is no such cancellation. Therefore solution (1) is also the minimizer of the trace of Hessian. The intuitions are formalized in Theorem 5.1. Consider minimizing the trace of Hessian among all the solutions to the MLM pre-training task: minimize \u03c8,u Tr[ \u2207 2 \u03c8 L ( \u03c8,u )]+Tr[ \u2207 2 u L ( \u03c8,u )] , subject to L ( \u03c8,u )=0 . Theorem 5.1. Suppose m \u2265 2 and T \u2265 6 . The flattest solution \u02c6 \u03c8, \u02c6 u are defined as the solution of the optimization problem above. \u02dc u is the minimizer of downstream training loss on top of \u02c6 \u03c8 , \u02dc u \u2208 argmin u \u2225 u \u2225 2 subject to (cid:98) L P ds ( \u02c6 \u03c8,u )=0 . Then with probability at least 1 \u2212 2 \u2212 n , L P ds ( \u02c6 \u03c8, \u02dc u )=0 .\n\n6 R ELATED W ORK\n\n Language modeling and downstream adaptation. Large language modeling has revolutionized the NLP field. Starting from Devlin et al. (2018), a line of works improve the downstream performance on a wide range of tasks with increasing model size and data amount (Yang et al., 2019; Radford et al., 2019; Raffel et al., 2020). LLMs even exhibit unexpected emergent behaviors, such as in-context learning (Xie et al., 2021; Min et al., 2022), step-by-step reasoning (Wei et al., 2022), and zero-shot learning (Brown et al., 2020). Kaplan et al. (2020); Hernandez et al. (2021) study the behavior of language models with increasing size, and find out that the pre-training loss is typically correlated with downstream performance as model size increases. In practice, the pre-training loss is used as an evaluation metric for language models. A notable example is the efficient transformer line of works, which benchmark the pre-training loss given the same computation constraint (Dai et al., 2020; Wang et al., 2020; Choromanski et al., 2020; Liu et al., 2021). Understanding the success of language modeling. Empirical works on understanding MLM find out that the representations of language models encode rich semantic and syntactic information (Peters et al., 2018; Htut et al., 2019; Hewitt & Manning, 2019; Mamou et al., 2020). Theoretical works show that fitting the MLM conditional probability is a sufficient condition for good performance on downstream tasks. Zhang & Hashimoto (2021) show MLM representations recover latent variables in graphical models. Wei et al. (2021) show linear probe on top of MLM models solves downstream tasks on datasets generated by HMMs. Recently, Saunshi et al. (2022) show that models with the same pre-training loss but different architectures can have different downstream performance. Tay et al. (2021) find out that a narrow but deep transformer is better than a wide but shallow transformer with the same pre-training loss. Zhang et al. (2022b) demonstrate that Albert (Lan et al., 2019) generalizes better to OOD tasks than Bert on a synthetic reasoning task. These works indicate that the architecture is an important factor for good downstream performance beyond pre-training loss. This paper discovers the role of implicit bias in language modeling, which happens with models in the same architecture. Implicit bias in supervised learning. The training algorithm chooses solutions with certain properties, and usually leads to better generalization (Gunasekar et al., 2018; Soudry et al., 2018; Li et al., 2017; Ji & Telgarsky, 2018; Arora et al., 2019; Lyu & Li, 2019; Li et al., 2020; Woodworth et al., 2020; HaoChen et al., 2020). Recently, Blanc et al. (2019); Damian et al. (2021); Li et al. (2021) demonstrate label noise SGD biases the models toward flatter minima. However, the setting of implicit bias in supervised learning is different from language modeling. In language modeling, we have access to gigantic corpus, and cannot interpolate the pre-training dataset. Moreover, we care about the adaptability of the solution on downstream tasks instead of generalization in distribution.\n\n7 C ONCLUSION\n\n We study the relationship between pre-training loss and downstream performance on language models. We discover that implicit bias matters beyond pre-training loss, and explore the mechanism of implicit bias in language modeling. Our experiments focus on simplified datasets due to constraint of computa- tional resources. We hope that the phenomenon can predict the implicit bias on more complex datasets as the community scale the models to even larger. Theoretically we provide cases where the flatness regularization can decide the performance on downstream performance. We wish this motivates future works on the relationship between implicit bias and the internal working of the models. 9\n\nR EPRODUCIBILITY S TATEMENT\n\n To ensure reproducibility, we describe the implementation details of the algorithms and the construction of the datasets in Section A and Section B. The code of the experiments is provided in the supplementary material. We provide the proof in Section C and Section D.\n\nR EFERENCES\n\n 10 1 11 12 13\n\nA D ETAILS IN S ECTION 2 A.1 G ENERATING S IMPLIFIED D ATASETS\n\n PCFG-generated dataset. We consider a PCFG with vocabulary size 200. The state space is S , and | S | =50 . All the production rules have two symbols on the right side. The sentence length is limited to 32, which means the depth of the parse tree is limited to 6. We generate a total of 2 \u00d7 10 7 sentences, which is 3 . 4 \u00d7 10 8 tokens. The downstream tasks are classifying the non-terminal symbols in the parse tree of the PCFG (50-way classification). The label is defined as y =argmax s \u2208 S Pr( s | x 1 ,x 2 ,...,x 1+ L ) . Tasks A, B and C are defined on the symbols corresponding to span length L =32 , 16 and 8 , respectively. Each of the downstream task contains 0.1M examples. Examples of the generated trees are provided in Figure 7. HMM-generated dataset. We consider an HMM with vocabulary size 200 and state space size 100. The sentence length is restricted to 16. We generate a total of 1 \u00d7 10 7 sentences, which is 1 . 6 \u00d7 10 7 tokens. The downstream task is to classify the latent variable in the HMM generative model. We consider task-6 and task-10, which classify the 6-th and 10-th hidden variables respectively. Each of the downstream task contains 0.1M examples. OPT-generated dataset. We use the 125M OPT model to generate the training dataset. To simplify the dataset, we further process the logit of OPT to select only from the top-2000 tokens in the vocabulary. Starting from the bos token, we sample every token of the sentence from the predicted autoregressive LM probability. The sentence length is restricted to 24. We generate a total of 2 \u00d7 10 8 sentences, which is 3 . 2 \u00d7 10 9 tokens. Examples of the generated text are provided in Figure 8.\n\nTHE T RUE C ONDITIONAL P ROBABILITIES\n\n We can compute the true MLM conditional probability Pr ( x t | x \u2212 t ) from the joint probability Pr( x t ,x \u2212 t ) , Since we already know the generative model, we can compute the joint probability efficiently. For PCFG, we can compute the joint probability with the inside algorithm, which decomposes the joint probability into lower layers in the parse tree. For HMM, we can compute the joint probability with the Viterbi algorithm. For OPT, we have Pr( x 1 ,...,x T )=Pr( x 1 ) (cid:81) T \u2212 1 t =1 Pr( x t +1 | x 1 ,...,x t ) .\n\nA.3 M ODELS\n\n We use transformers on PCFG and OPT-generated datasets. We use learning rate 1e-3 and warmup proportion 0.06. All the models are trained based on the implementation of Izsak et al. (2021). We list the sizes of the transformers in Table 3. d-model is the size of the hidden layers. d-inter is the size of the 14 intermediate layers in MLP. n-head is the number of heads per layer. # layers is the number of layers. For LSTMs, we use the implementation of PyTorch. We consider d-hidden in [128,256,512,768,1024], and # layers in [4,6,8,12,16].\n\nA.4 A LGORITHMS\n\n The lookup table. To evaluate the downstream performance of the lookup table, we first create the lookup table with the data of the downstream task. With the method mentioned in Section A.2, we can generate the true conditional probability of each token and use it as the contextual embeddings. The adversarial algorithm. The adversarial algorithm we use to mess up the downstream performance is maximizing a meta-learning objective in pre-training. Suppose the linear head of the downstream task is g \u03d5 and the feature representation is h \u03c8 . The meta-learning algorithm first trains the head g \u03d5 to minimize the training loss of the downstream task, and then update h \u03c8 to maximize the validation loss on the downstream tasks. Concretely, we randomly sample two disjoint subsets D 1 and D 2 from the downstream training dataset D . We train g \u03d5 to minimize the loss of downstream tasks on D 1 , \u02c6 \u03d5 ( \u03c8 ) \u2208 argmin 1 | D 1 | (cid:80) ( x,y ) \u2208 D 1 \u2113 ( g \u03d5 ( h \u03c8 ( x )) ,y ) . Then we train h \u03c8 to maximize the validation loss on D 2 during pre-training, minimize \u03c8 L ( \u03c8 ) \u2212 \u03bb | D 2 | (cid:80) ( x,y ) \u2208 D 2 \u2113 ( g \u02c6 \u03d5 ( \u03c8 ) ( h \u03c8 ( x )) ,y ) . The optimization can be efficiently carried out with closed form solution of \u03d5 as shown in Liu et al. (2020). Fine-tuning. Following the standard protocol of Devlin et al. (2018), we use the contextual embeddings of the CLS token for fine-tuning. We use AdamW with learning rate 1e-4. We perform 200 warmup steps and train on the downstream tasks for 10 epochs. Linear probe. Since the CLS token is not trained in pre-training, we concatenate embeddings of all the tokens in the sentence as the representations. We use AdamW with learning rate 1e-3 to train the linear head. We train on the downstream tasks for 100 epochs. Note that to make the capacity of the linear probe itself controlled, we adopt a random Gaussian projection to dimension 512 on the concatenation of the embeddings. We report the standard deviation of linear probe and fine-tuning from 5 random seeds. Evaluation of pre-training loss. Since we have access calculate the true conditional probability, we can calculate the cross entropy loss as the sum of the entropy of the true conditional probability and the KL divergence between the predicted and true conditional probabilities. This is more accurate than evaluating on the validation datasets in the standard ways. We report the number of pre-training loss with 10 6 sentences, and calculate the standard deviation on 5 subsets, each of which has size 2 \u00d7 10 5 .\n\nA.5 R ESULTS ON O THER D OWNSTREAM T ASKS .\n\n We also provide results on other downstream tasks in this subsection. On PCFG Task A, OPT SST-2 and the Task-6 of HMM, we can also observe the increase in downstream performance as we scale up the models in the saturation regime. 15 2 5 9 12 41 80 235 436 730 Model Size / M 78 80 82 84 86 88 90 92 94 D o w n s t r e a m A cc u r a c y % PCFG, Task A Fine-tuningLinear Probe 3 . 203 . 223 . 243 . 263 . 283 . 303 . 323 . 343 . 36 P r e - t r a i n i n g L o ss Pre-training (a) PCFG \u2192 Task A 5 12 41 80 235 436 730 Model Size / M 50 55 60 65 70 75 80 85 D o w n s t r e a m A cc u r a c y % OPT, SST2 Fine-tuningLinear Probe 10 11 12 13 14 15 P r e - t r a i n i n g L o ss Pre-training (b) OPT \u2192 SST-2 4 10 29 67 135 Model Size / M 40 50 60 70 80 90 D o w n s t r e a m A cc u r a c y % HMM, Task-6 Fine-tuningLinear Probe 3 . 75 3 . 76 3 . 77 3 . 78 3 . 79 3 . 80 P r e - t r a i n i n g L o ss Pre-training (c) HMM \u2192 Task-6\n\nB D ETAILS IN S ECTION 4\n\n UnbiasedestimateofthetraceofHessian. Sincetheoutputdimension c andtheumberofparameters are all very large, computing the Jacobian \u2207 \u03b8 f \u03b8 ( x \u2212 t ) will be very inefficient. We approximate the Jacobian with random samples as follows. Suppose f ( x \u2212 t ) is the predicted logits of the conditional probability, the trace of Hessian can be expressed as \u2207 \u03b8 f \u03b8 ( x \u2212 t )] \u02c6 y \u2225 22 ] . To approximate this expectation, we can sample i.i.d. samples \u02c6 y from softmax ( f ( x \u2212 t )) and use the average as the unbiased estimate. For all experiments, we use number of samples 50. Details in Figure 3. To verify Theorem 3.3 that SGD biases the model towards flatter minima, we conduct MLM on PCFG and HMM-generated datasets with SGD. We set the proportion of warmup stage to 12 % total number of steps, and fix the learning rate to 1e-3 after the warmup. We evaluate the downstream performance and the trace of Hessian of different checkpoints along pre-training. The standard deviation of trace of Hessian is calculated based on 5 times of sampling 50 examples as mentioned above. Apart from the PCFG task C and HMM task-10, we also provide results on PCFG tasks A, B and HMM task-6 in Figure 10. n s t r e a A u r a c y % P r t r a i n i n g L o ss Viewing a small model as a special case of large model. We show that a small neural network is a special case of a large neural network by considering two cases (1) adding additional layers without changing the functionality and (2) adding more channels/attention heads without change the functionality. For (1), since the transformer contains residual connections, we can add new layers on top of a smaller model and fill in zeros to the added parameters. In this way, we can obtain a larger model with the same functionality as the small model. Concretely, suppose a small model is g \u03b8 , an additional block is h \u03d5 , and the large model can be expressed as f \u03b8,\u03d5 ( \u00b7 )= h \u03d5 ( g \u03b8 ( \u00b7 ))+ g \u03b8 ( \u00b7 ) . Then setting the parameters \u03d5 to all 0 can make f \u03b8,\u03d5 ( \u00b7 )= g \u03b8 ( \u00b7 ) . For (2), since the transformers contain layer norm, we can duplicate the parameters channelwise to duplicate the feature dimension. Since we keep the mean and variance across channels unchanged, 16 the functionality will also be preserved. Similarly, we can also add more heads with this duplication technique. Therefore, the small models can be enlarged into a larger models with functionality preserved. The smaller transformer architecture can be viewed as a subset of the larger transformer architecture. In this sense, a set of transformers with different sizes and the same pre-training loss found in Section 2 can be viewed as a set of transformers with the same size. Note that the training algorithm only finds out the natural larger models, instead of the larger models which are augmented from the smaller models. This indicates that the implicit bias of the optimizer can interact with the model architecture. The implicit bias drives the model toward flat minima on both larger models and smaller models. The smaller transformer architecture is a subset of the larger transformer architecture, thus the flattest minima found with a larger transformer is flatter than the minima found with a smaller transformer. (See Figure 6). 17\n\nC O MITTED P ROOFS IN S ECTION 3\n\n Proof of Lemma 3.2. We first recall loss Note that conditioned on any x \u2212 t ,t , it holds that where in the last step, we use the assumption that \u03b8 \u2208 \u0393 that is, for all x,t f \u03b8 ( x \u2212 t )=Pr( \u00b7| x \u2212 t ) which implies the following Since \u03b8 is a global minimizer of L we have that \u2207 L ( \u03b8 )= E t,x \u2207 \u03b8 log[ x \u2212 t )] x t =0 . Therefore, we have that which completes the proof 18\n\nD O MITTED P ROOFS IN S ECTION 5 D.1 O MITTED P ROOFS OF T HEOREM 5.1\n\n Recall that the loss function of MLM is L ( \u03c8, u ) = E x \u223c P [( f \u03c8,u ( x ) \u2212 g \u2217 ( x )) 2 ] . In downstream adaptation, we have access to a finite dataset { x ( i ) } ni =1 sampled i.i.d. from P ds . The training loss is (cid:98) L P ds ( \u03c8,u ) = 1 n (cid:80) ni =1 [( f \u03c8,u ( x ( i ) ) \u2212 g \u2217 ( x ( i ) )) 2 ] , and the population loss for the downstream task is L P ds ( \u03c8,u )= E x \u223c P ds [( f \u03c8,u ( x ) \u2212 g \u2217 ( x )) 2 ] . Proof of Theorem 5.1. We first calculate the trace of Hessian of the pre-training loss and then derive a lower bound for it in Lemma D.1. We then show that the lower bound can be achieved only if the output of the attention are in one direction for all the downstream input in Lemma D.3. This translates to constant sample complexity for the downstream task. Lemma D.1. Denote by h Q,K ( x ) = (cid:80) Tj =1 a j x j the output of the attention head. In the setting of Theorem 5.1, I + = { i \u2208 [ m ] | u i > 0 } and I \u2212 = { i \u2208 [ m ] | u i < 0 } . The trace of Hessian can be lower bounded, where the lower bound is achieved if and only if the following conditions are satisfied, Denote by D + = { x | y = 1 } and D \u2212 = { x | y = \u2212 1 } . I x is the set of index of neurons which is activated on x , I x = { i \u2208 [ T ] | V \u22a4 i x > 0 } . By the condition in equation 5, the attention is taking the average of [ x t ] Tt =1 . We can map D + and D \u2212 to the feature space, H + = { h Q,K ( x ) | y = 1 } and H \u2212 = { h Q,K ( x ) | y = \u2212 1 } . We first show that one neuron cannot be activated on inputs from both D + and D \u2212 , and all non zero neuron has to be activated on some input. Also note that a neuron cannot be activated on no input, unless the weight is 0 . Fact D.2. (1) \u2200 x \u2208 D + , I x \u2286 I + . Similarly we have \u2200 x \u2208 D \u2212 , I x \u2286 I \u2212 . (2) Suppose V i \u0338 =0 , then there exists h \u2208 H + \u222a H \u2212 , V \u22a4 i h> 0 . Proof of Fact D.2. (1) Otherwise, suppose j \u2208 I x \u2229 I \u2212 , since y = 1 m (cid:80) mi =1 u i \u03c3 ( V \u22a4 i h Q,K ( x )) > 0 , there has to be j \u2032 \u2208 I x \u2229 I + , which contradicts the condition in equation 4. (2) Suppose v \u22a4 h \u2264 0 for all h \u2208 H + \u222a H \u2212 . Then we have v \u22a4 h =0 for all h , since v \u22a4 h< 0 indicates v \u22a4 ( \u2212 h ) > 0 , and \u2212 h belongs to the support of P due to the symmetry of the distribution. However, in Lemma D.5, we show that the matrix stacking all input together has full row rank, thus v has to be 0 , leading to a contradiction. We have the following lemma characterizing the solutions achieving all the qualities in Lemma D.1. Intuitively, all the neurons can be divided into two sets, and each input can only activate neurons in one of the sets, leading to no cancellation between activated neurons. This holds because of equation 4 and the properties of the input distribution. Lemma D.3. Suppose Q,K,V satisfy the equality in Lemma D.1. For all i \u2208 I \u2212 , on downstream data x , if g \u2217 ( x )=1 , we have V \u22a4 i h Q,K ( x )= c i > 0 . c i is a constant which holds for every x if g \u2217 ( x )=1 . If g \u2217 ( x )= \u2212 1 , we have V \u22a4 i h Q,K ( x )=0 . For all i \u2208 I + , on downstream data x , if g \u2217 ( x )= \u2212 1 , we have V \u22a4 i h Q,K ( x )= c i > 0 . c i is a constant which holds for every x if g \u2217 ( x )=1 . If g \u2217 ( x )=1 , we have V \u22a4 i h Q,K ( x )=0 . 19 Now let us consider the downstream task. It suffices to consider the constant vector c . If samples satisfying g \u2217 ( x ) = 1 and g \u2217 ( x ) = \u2212 1 both show up in the downstream dataset, the minimal norm solution \u02dc u is \u02dc u I \u2212 = mc I \u2212 \u2225 c I \u2212 \u2225 2 2 , \u02dc u I + = \u2212 mc I + \u2225 c I + \u2225 2 2 , and \u02dc u [ m ] \\ ( I + \u222a I \u2212 ) =0 . Then we can verify that = [ g ( x =1] \u2212 [ g ( x = g \u2217 ( x ) Therefore, L P ds ( \u02c6 \u03c8, \u02dc u )= E x \u223c P ds [( f \u02c6 \u03c8, \u02dc u ( x ) \u2212 g \u2217 ( x )) 2 ]=0 , which completes the proof. We first show that when the pre-training loss equals 0 , the trace of Hessian equals the square of the norm of the gradient. Lemma D.4. For any parameters \u03b8 , if the pre-training loss L ( \u03b8 )= E x [( f \u03b8 ( x ) \u2212 y ) 2 ]=0 , the trace of Hessian equals the square of the norm of the gradient, Proof of Lemma D.4. We can express the Hessian as follows. Since L ( \u03b8 )= E x [( f \u03b8 ( x ) \u2212 y ) 2 ]=0 , we have with probability 1 , \u2113 \u2032 ( f \u03b8 ( x ) ,y )=0 and \u2113 \u2032\u2032 ( f \u03b8 ( x ) ,y )=2 is a constant. Proof of Lemma D.1. 4) The equality in step 6 is achieved if and only if the gradient of Q and K is 0 . Equation (7) is from AM-GM and the equality is achieved iff The equality in step 8 is achieved iff on all input, there is no cancellation between activated neurons, 20 Since the attention score a j satisfies a j > 0 and (cid:80) Tj =1 a j =1 , and all embeddings x t in one masked sentence are orthogonal to each other with norm 1 , we have \u2225 h Q,K ( x ) \u2225 2 \u2265 1 \u221a T . The equality is achieved iff a j = 1 T for all x and all j \u2208 [ T ] . Proof of Lemma D.3 . Suppose V i is a neuron with i \u2208 I \u2212 . Then there exists h \u2208 H \u2212 , V \u22a4 i h > 0 . Without loss of generality, suppose the masked position in h is 1 , i.e. h 1 = 0 , h 2 = 1 . Now let us consider the components in V i corresponding to the input positions and the mask positions separately. We claim that V ( c ) i 2: T = c 1 for some c> 0 and V ( p ) i 1 is either 0 or c . To prove this, consider \u02dc h , which is only different from h on the mask, h \u2212 \u02dc h =2 e 2 . Also consider \u2212 h and \u2212 \u02dc h . Due to the symmetry of the distribution, \u2212 h and \u2212 \u02dc h are in H + . By Fact D.2, V \u22a4 i ( \u2212 h ) \u2264 0 and V \u22a4 i ( \u2212 \u02dc h ) \u2264 0 . V \u22a4 i ( \u2212 h ) cannot be 0 , because this will leads to V \u22a4 i h =0 . Case 1. V \u22a4 i ( \u2212 h ) < 0 and V \u22a4 i ( \u2212 \u02dc h ) < 0 . We have V \u22a4 i h = V \u22a4 i \u02dc h > 0 , indicating that V \u22a4 i ( h \u2212 \u02dc h ) = 2 V ( p ) i 1 =0 . Now we show that V ( c ) i 2: T = c 1 . Due to the condition of equation 3, we know that for any h \u2208 H \u2212 masked on the first token, either V \u22a4 i h =0 or they equal to the same positive value c for all h . We claim that V \u22a4 i h = c for any h \u2208 H \u2212 . Otherwise there exist H \u2212 , 1 and H \u2212 , 0 , H \u2212 , 0 \u2229 H \u2212 , 1 = \u2205 and H \u2212 , 0 \u222a H \u2212 , 1 = H \u2212 \u2229{ h | h 2 = \u00b1 1 } . V \u22a4 i h = c for any h \u2208 H \u2212 , 1 and V \u22a4 i h =0 for any h \u2208 H \u2212 , 0 . This cannot happen for T \u2265 6 . By Lemma D.5 we know that the matrix stacking all such h 2: T together has full row rank, thus V ( c ) i 2: T = c 1 for some c> 0 and V ( p ) i 1 =0 . Case 2. V \u22a4 i ( \u2212 h ) < 0 and V \u22a4 i ( \u2212 \u02dc h ) = 0 , which indicates V \u22a4 i h = V ( p ) i 1 . Consider another h \u2032 \u2208 H \u2212 which is not equal to h and h \u2032 2 =1 . Similarly we can find \u02dc h \u2032 , h \u2032 \u2212 \u02dc h \u2032 =2 e 2 . Still we have V \u22a4 i ( \u2212 h \u2032 ) < 0 and V \u22a4 i ( \u2212 \u02dc h \u2032 ) = 0 , this tells us V \u22a4 i h \u2032 = V \u22a4 i h , due to the condition of equation 3. Applying this to different h \u2032 s, we have that V \u22a4 i h equals the same positive value for all h \u2208 H \u2212 and the masked position is 0 . By Lemma D.5 we know that the matrix stacking all such h 2: T together has full row rank, thus V ( c ) i 2: T = c 1 for some c> 0 and V ( p ) i 1 = c . We have proved that V ( c ) i 2: T = c 1 for some c> 0 and V ( p ) i 1 is either 0 or c . We continue to show that V ( c ) i = c 1 for the same c> 0 and either V ( p ) i is 0 or its coordinates is \u00b1 c . For case 1, consider h \u2032\u2032 \u2208 H \u2212 whose masked position is 2 , h \u2032\u2032 4 = 1 . Also suppose that h \u2032\u2032 1 = 1 By Fact D.2, we know that V \u22a4 i ( \u2212 h \u2032\u2032 ) \u2264 0 and V \u22a4 i ( \u2212 \u02dc h \u2032\u2032 ) \u2264 0 , this implies that \u2212 V ( c ) i 1 \u2264 V ( p ) i 2 \u2264 V ( c ) i 1 . Applying the same argument above, we know that either V ( p ) i 2 = 0 or V ( p ) i 2 = \u00b1 V ( c ) i 1 , otherwise both V \u22a4 i h \u2032\u2032 > 0 and V \u22a4 i \u02dc h \u2032\u2032 > 0 hold, and V \u22a4 i h \u2032\u2032 \u0338 = V \u22a4 i \u02dc h \u2032\u2032 , contradicting condition in equation 3. If V ( p ) i 2 = V ( c ) i 1 , from equation 3 we know V \u22a4 i h \u2032\u2032 = V \u22a4 i h , which indicates V ( p ) i 2 = V ( c ) i 1 = c 2 . In this case we can find another h \u2032\u2032\u2032 whose masked position is 2 , h \u2032\u2032\u2032 4 =1 but h \u2032\u2032\u2032 1 = \u2212 1 . Then V \u22a4 i h \u2032\u2032\u2032 \u0338 = V \u22a4 i h , contradicting equation 3. Thus V ( p ) i 2 \u0338 = V ( c ) i 1 . Similarly V ( p ) i 2 \u0338 = \u2212 V ( c ) i 1 . The only possible situation is V ( p ) i 2 = 0 . Applying the argument in this paragraph to other masked position, we have V ( p ) i = 0 . Since H \u2212 is invariant under permutation, V ( c ) i 1 = c , and V ( c ) i = c 1 . For case 2, exactly the same argument as the above paragraph with the same h \u2032\u2032 and h \u2032\u2032\u2032 shows that the coordinates of V ( p ) i is \u00b1 c . Therefore, we have shown that for any i \u2208 I \u2212 , V ( c ) i = c 1 for same c> 0 . The symmetry of distribution immediately tells us for any i \u2208 I + , V ( c ) i = c 1 for c< 0 . On the downstream distribution P \u2217 , since there is no masked token, only V ( c ) i is working. Since V ( c ) i = c 1 always holds, we complete the proof. 21 Lemma D.5. Suppose M \u2208 R (2 k +1) \u00d7 (2 k +1) is a matrix composed of \u00b1 1 . The first row of M is [1 ,..., 1 (cid:124) (cid:123)(cid:122) (cid:125) k+11\u2019s , \u2212 1 ,..., \u2212 1 (cid:124) (cid:123)(cid:122) (cid:125) k-1\u2019s ] . Define a permutation \u03c1 (1)=2 , \u03c1 (2)=3 ... \u03c1 (2 k +1)=1 . For all i \u2265 2 , M i,\u03c1 ( j ) = M i \u2212 1 ,j . Then the rank of M is 2 k +1 . Proof of Lemma D.5. Note that M i + M \u03c1 k +1 ( i ) =2 e i for all i \u2208 [2 k +1] , which means we can express the orthonormal basis as linear combination of the rows in M . Therefore, the rank of M is 2 k +1 . 22\n\nD.2 T HE EXISTENCE OF RANDOM FEATURE SOLUTIONS D.6\n\n Theorem D.6. Suppose m \u2265 \u02dc O (2 T T 3 \u03f5 \u2212 2 ) , and V i \u223cN (0 ,TI 2 T ) for all i \u2208 [ m ] . With probability at least 1 \u2212 \u03b4 over V , there exists \u03c8 \u2032 ,u \u2032 , satisfying L ( \u03c8 \u2032 ,u \u2032 ) \u2264 \u03f5 and \u2225 u \u2032 \u2225 22 \u2264 O ( T 2 \u03b4 \u2212 1 ) . Proof of Theorem D.6. Since the number of possible input in pre-training is finite, we can invoke Lemma 9 in Bai & Lee (2020) to show that random Gaussian features can fit the pre-training task. Lemma D.7 (Lemma 9 in Bai & Lee (2020)) . Suppose \u2225 h \u2225 2 = (cid:113) 1 T , v \u223cN (0 ,TI 2 T ) . There exists a random variable a ( v ) such that and a satisfies E v [ a 2 ]= O ( T 2 ) . Consider Q = K = 0 . In this case we have h Q,K ( x ) = 1 T (cid:80) Tj =1 x j . Also note that for the pre- training task, y = \u2212\u221a T 1 \u22a4 h Q,K ( x ) for all x . Since x j are norm 1 orthogonal to each other, we have \u2225 h Q,K ( x ) \u2225 2 = 1 T for all x . Now we can show that V i \u223c N (0 ,TI 2 T ) , i \u2208 [ m ] independently is the random feature solution which can solve the pre-training task. Suppose g ( h ) = 1 m (cid:80) mr =1 \u03c3 ( V \u22a4 r h ) a ( V r ) , and g ( R ) ( h ) := 1 m (cid:80) mr =1 \u03c3 ( V \u22a4 r h ) a ( V r ) I ( \u2225 V r \u2225 2 \u2264 \u221a TR ) . R is large enough such that Pr (cid:16) sup r \u2208 [ m ] \u2225 V r \u2225 2 \u2265\u221a TR (cid:17) \u2265 1 \u2212 \u03b4/ 2 . We have g ( h ) = g ( R ) ( h ) on this event. Let g \u2217 ( R ) ( h ) be the truncated version of g \u2217 ( h ) = \u2212\u221a T 1 \u22a4 h , g \u2217 ( h ) ( R ) = E v [ \u03c3 ( v \u22a4 h ) a ( v ) I ( \u2225 v \u2225 2 \u2264\u221a TR )] . We have By Chebyshev and a union bound, we have Pr For t = \u03f5 2 , we have m \u2265 n ( h ) R 2 T 2 \u03f5 \u2212 2 . Choosing R = \u02dc O ( (cid:112) ( T )) will make Pr (cid:16) \u2225 v \u2225 > \u221a TR (cid:17) 14 \u2264 c\u03f5T . Also note that n ( h )=( T/ 2 \u2212 1) (cid:0) T T/ 2+1 (cid:1) . Thus m \u2265 \u02dc O (2 T T 3 \u03f5 \u2212 2 ) suffices. 23", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the relationship between pre-training loss and downstream performance in language models.", "topic_class_1": "Summary of topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors find that models with the same pre-training loss can exhibit different downstream performance, contrary to the common assumption that pre-training loss is well-correlated with downstream performance.", "topic_class_1": "Summary of findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They identify three ways to produce models with the same optimal pre-training loss but different downstream performance: (1) continuing pre-training after loss convergence, (2) increasing model size, and (3) changing the training algorithm.", "topic_class_1": "Summary of methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This suggests an implicit bias in the pre-training algorithms that selects more transferable models among those with the same minimal loss.", "topic_class_1": "Summary of conclusions and findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Theoretically, the authors prove that SGD with mini-batch noise implicitly prefers flatter minima in the pre-training loss landscape.", "topic_class_1": "Summary of conclusions and findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirically, they observe a strong correlation between flatness (measured by trace of Hessian) and downstream performance for models with the same minimal pre-training loss.", "topic_class_1": "Summary of conclusions and findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They also prove in a synthetic language setting that among models minimizing the pre-training loss, the flattest one transfers best to downstream tasks.", "topic_class_1": "Summary of conclusions and findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper challenges the common assumption about the correlation between pre-training loss and downstream performance, providing both empirical evidence and theoretical analysis.", "topic_class_1": "Strength on the novelty and solid efforts", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- It identifies implicit bias as an important factor affecting the transferability of language models with the same architecture and pre-training loss.", "topic_class_1": "Strength on the main contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors provide theoretical results showing SGD's implicit bias towards flatter minima in language modeling, cleanly adapting tools from supervised learning.", "topic_class_1": "Strength on the evaluation and experimental results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Empirical results on simplified datasets convincingly demonstrate the phenomenon of models with same loss but different downstream performance.", "topic_class_1": "Strength on the evaluation and experimental results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The main limitation is that the empirical evaluation focuses on simplified datasets due to computational constraints.", "topic_class_1": "Concerns on the generalizability of conclusions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It's unclear how well the results generalize to more complex real-world datasets and larger models.", "topic_class_1": "Concerns on the generalizability of conclusions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The theoretical result in the synthetic language setting has limited applicability and the assumptions may not hold in practice.", "topic_class_1": "Concerns on the generalizability of conclusions", "topic_class_2": NaN, "reliability": "No", "explanation": "The disadvantage of about synthetic data is already mentioned in previous point", "error_type": "Duplication"}, {"segment_text": "- The paper lacks experiments comparing flatness measures other than trace of Hessian.", "topic_class_1": "Concerns on the lack of measures over flatness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Strength on the writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors provide sufficient details of the experimental setup and theoretical derivations.", "topic_class_1": "Strength on the contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The phenomenon of implicit bias affecting downstream performance is a novel finding and the theoretical analysis connecting flatness to downstream performance is original.", "topic_class_1": "Strength on the novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The empirical methodology and datasets are described in enough detail to enable reproducibility.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the code for the experiments is not provided which somewhat limits reproducibility.", "topic_class_1": "Concern on the reproducibility", "topic_class_2": NaN, "reliability": "No", "explanation": "The authors provided code in the supplementary material. They also described detaild experimental setups. (See \"reproducibility statement\" in the paper.)", "error_type": "Neglect"}, {"segment_text": "Overall, this is a high-quality paper making novel contributions.", "topic_class_1": "Summary comment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes important contributions in highlighting the role of implicit bias in the transferability of language models, going beyond just the pre-training loss.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It provides convincing empirical evidence of the phenomenon and backs it up with novel theoretical analysis connecting flatness to downstream performance.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The main weakness is the focus on simplified datasets.", "topic_class_1": "Summary of weaknesses", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "But overall, this is an impactful paper that advances our understanding of what makes language models adaptable to downstream tasks.", "topic_class_1": "Summary comment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper challenges the conventional wisdom linking pre-training loss with downstream performance in language modeling.", "topic_class_1": "Summary of topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It uncovers that while pre-training loss correlates with downstream performance, the flatness of the model\u2014measured by the trace of the Hessian of the loss function\u2014exhibits a stronger correlation.", "topic_class_1": "Summary of findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By manipulating model size, training duration, and algorithms, the authors demonstrate that models with identical pre-training losses can have significantly different downstream performances.", "topic_class_1": "Summary of methods", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They attribute this variance to the implicit biases of training algorithms, favoring models that generalize better to new tasks.", "topic_class_1": "Summary of assumptions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Through theoretical analysis and empirical evidence, the paper shows that stochastic gradient descent (SGD) inherently prefers flatter minima, which are associated with better transferability to downstream tasks.", "topic_class_1": "Summary of experiments and findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper provides a fresh perspective on evaluating language models by focusing on the model's flatness rather than just pre-training loss.", "topic_class_1": "Strength on the novel perspective introduced", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It combines theoretical insights with empirical evidence, offering a comprehensive analysis of the impact of training algorithms' implicit biases on model performance.", "topic_class_1": "Strength on the theoretical and empirical efforts", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The findings have practical implications for designing and training language models more efficiently, emphasizing the quality of the training process over simplistic loss metrics.", "topic_class_1": "Strength on the findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are conducted on simplified datasets, which may limit the generalizability of the findings to more complex, real-world datasets.", "topic_class_1": "Concerns on the generalizability of conclusions", "topic_class_2": NaN, "reliability": "No", "explanation": "Such costly experiments usually can not handle such a large scale dataset. ", "error_type": "Out-of-scope"}, {"segment_text": "While the paper introduces the concept of flatness, it does not provide concrete guidelines on how to achieve or measure flatness in practical settings.", "topic_class_1": "Concerns on the limited discussion on flatness", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written, with clear explanations of the theoretical background, methodology, and significance of the findings.", "topic_class_1": "Strength on the writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is of high quality, combining theoretical proofs with rigorous empirical testing to validate the hypotheses.", "topic_class_1": "Strength on the contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The focus on the flatness of models and the exploration of training algorithms' implicit biases present novel insights into language model evaluation and training.", "topic_class_1": "Strength on the novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors make an effort to ensure reproducibility by detailing the implementation of algorithms and dataset generation, along with providing code in supplementary materials.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a significant contribution to the field of language modeling by challenging the conventional reliance on pre-training loss as a metric for evaluating model performance.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By highlighting the importance of model flatness and the implicit biases of training algorithms, it opens new avenues for research and development in language model training methodologies.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the study's reliance on simplified datasets may raise questions about the applicability of its findings to more complex tasks, its combination of theoretical and empirical analyses offers valuable insights.", "topic_class_1": "Overall summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The clarity, novelty, and quality of the research are commendable, making it a valuable addition to the literature.", "topic_class_1": "Summary of the paper quality", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the relationship between pre-training loss and downstream performance in language models.", "topic_class_1": "Summary of topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It challenges the common assumption that lower pre-training loss directly translates to better downstream performance.", "topic_class_1": "Summary of assumption challenged", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors argue that the implicit bias of training algorithms plays a crucial role in selecting models that are more adaptable to downstream tasks, even among models with the same pre-training loss.", "topic_class_1": "Summary of arguments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They demonstrate this phenomenon through experiments on simplified datasets and provide theoretical analysis supporting their claims.", "topic_class_1": "Summary of experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, they show that the flatness of the loss landscape, as measured by the trace of the Hessian, correlates with downstream performance and prove this connection in a synthetic language setting.", "topic_class_1": "Summary of experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Challenges conventional wisdom: The paper raises important questions about the sole reliance on pre-training loss as a metric for evaluating language models and highlights the impact of implicit bias.", "topic_class_1": "Strength on the novelty of questions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Controlled experiments: The use of simplified datasets allows for controlled experiments and precise analysis of the factors influencing downstream performance.", "topic_class_1": "Strength on the experimental setup", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Theoretical grounding: The paper provides theoretical analysis of the implicit bias of SGD in language modeling, demonstrating its preference for flatter minima.", "topic_class_1": "Strength on the theoretical analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Formal connection between flatness and downstream performance: The authors prove in a synthetic language setting that flatter models are indeed more transferable to downstream tasks.", "topic_class_1": "Strength on the theoretical analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Simplified datasets: While the use of simplified datasets enables controlled experiments, it raises concerns about thegeneralizability of the findings to real-world, complex datasets.", "topic_class_1": "Concern on the generalizability of conclusions", "topic_class_2": NaN, "reliability": "No", "explanation": "The experiments in this paper is costly. They provided sufficient proof for that.", "error_type": "Out-of-scope"}, {"segment_text": "Limited downstream tasks: The experiments primarily focus on a few specific downstream tasks, which might not be representative of the diverse range of NLP applications.", "topic_class_1": "Concern on the limited evaluation settings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Computational feasibility: Reaching the saturation regime, where the pre-training loss is near-optimal, might be computationally challenging for large-scale models and datasets.", "topic_class_1": "Concern on the computational feasiblity", "topic_class_2": NaN, "reliability": "No", "explanation": "The paper presents thought provoking insights and backs them with theoretical and empirical evidence. How to leverage such insights is another research / engineering problem.", "error_type": "Misunderstanding"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written and presents the ideas and findings clearly.", "topic_class_1": "Strength on the writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-designed and executed, with both theoretical and empirical evidence supporting the claims.", "topic_class_1": "Strength on the contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The paper makes a novel contribution by highlighting the importance of implicit bias in language modeling and its connection to flatness and downstream performance.", "topic_class_1": "Strength on the novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors provide detailed descriptions of the datasets, models, and algorithms used, along with the code in the supplementary material, ensuring reproducibility.", "topic_class_1": "Strength on the reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a valuable contribution to the understanding of language models by highlighting the importance of implicit bias and its connection to flatness and downstream performance.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "No", "explanation": "The major content is about the relation between pre-training loss and the downstream tasks.", "error_type": "Unstated statement"}, {"segment_text": "While the experiments are conducted on simplified datasets, the findings raise important questions about the evaluation and training of large language models and encourage further investigation in this direction.", "topic_class_1": "Summary of weaknesses and strengths", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models", "body_text": "\n\nK NOWLEDGE - IN -C ONTEXT : T OWARDS K NOWLEDGE - ABLE S EMI -P ARAMETRIC L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC) , which empowers a parametric text-to-text language model with a knowledge- rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi- parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC Large with 770M parameters easily outperforms large language models (LMs) that are 4-39x larger by a large margin. We also demonstrate that KiC exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.\n\n1 I NTRODUCTION\n\n Recently, large-scale fully-parametric language models have achieved great success in solving natural language processing (NLP) tasks (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Kaplan et al., 2020). However, they generally require a huge number of model parameters to store the necessary knowledge for solving multiple NLP tasks in the zero/few-shot setting. Meanwhile, their problem solving capability only emerges after reaching a certain model scale (Wei et al., 2022). In addition, large parametric language models are hard to adapt to the evolving world knowledge without expensive model re-training. To overcome these challenges, there has been an increasing interest in developing semi-parametric language models, where a parametric language model is augmented with an external memory containing a large amount of text chunks (Borgeaud et al., 2022; Izacard et al., 2022; Khandelwal et al., 2019; Zhong et al., 2022). Although these semi-parametric approaches are shown to be more effective than their much larger parametric counterparts, there remain several challenges. The first challenge is that useful knowledge pieces are generally sparsely distributed over a large textual corpus. Therefore, it is difficult to locate and retrieve the correct text chunk that contains the right knowledge to complement a particular input instance. Second, it is difficult to determine the proper text chunk granularity to cover the desired knowledge. Thus, people usually use oversized text chunks to build indexing, which makes it even harder to determine whether knowledge is contained. On the other hand, there have been a rich collection of knowledge resources such as diverse knowledge graphs, where different kinds of knowledge are densely and compactly organized in structured or semi-structured forms. In this paper, we leverage these knowledge resources to 1 construct a semi-parametric language model, by simply using off-shelf encoders and retrievers to index and search the external memory. In particular, our primary contribution is developing a novel semi-parametric language model architec- ture, Knowledge-in-Context (KiC) , that is fueled by a large knowledge-rich external memory (Section 2). Specifically, the memory covers six broad categories of knowledge types: entity, dictionary, commonsense, event, script and causality (Section 2.2). Our comprehensive analysis reveals that a wide range of natural language tasks (31 out of 35 tasks) benefit from adding knowledge, where different knowledge resources help with different subsets of tasks. Interestingly, some tasks are even improved by 10%+ after adding suitable knowledge. To adaptively utilize knowledge, we exploit KiC to dynamically identify the most useful knowledge pieces for each input instance from a certain task and places them in the current context for answering the question. We adopt a single text-to-text transformer (e.g., T5) to generate the output answer from the input. Specifically, we append the retrieved knowledge pieces to the input instance and convert them into a natural language sequence with prompt templates. It is then fed into the text-to-text model to generate the output answer (also in natural language). The major advantage of such a text-to-text paradigm is that it handles multiple natural language tasks with the same interface and can also generalize to unseen tasks (Sanh et al., 2022; Raffel et al., 2020). Moreover, we find this training paradigm is suitable for our model design as it can teach our KiC model to learn how to select and use knowledge through various seen language tasks and then generalize well to use knowledge for solving unseen tasks. Our experimental analysis further shows that such instance-adaptive (context-dependent) knowledge augmentation is critical to the success of KiC model. However, due to the inherent discrete nature, it is difficult to train KiC in a fully differentiable manner to select the correct knowledge category for each instance. To solve this problem, we find that KiC can be reformulated as a special mixture-of-experts (MoE) model (Jacobs et al., 1991; Jordan & Jacobs, 1994; Shazeer et al., 2017; Fedus et al., 2022), where the knowledge selector is identified as the router that is used to determine the sequence-to-expert assignment in MoE (Section 2.3). Furthermore, the memory partition corresponding to each knowledge category together with the text-to-text model can be recognized as a special semi-parametric expert in MoE. This key observation inspires us to develop a novel learning algorithm to train KiC with instance-adaptive knowledge selection capabilities. In our experiments (Section 3), we adopt the same setting as T0 (Sanh et al., 2022), where we train KiC models on a collection of tasks and then evaluate on another set of unseen tasks in a zero-shot manner. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. With only 0.77B parameters, KiC Large outperforms zero-shot baseline models such as GPT-NeoX-20B or OPT-30B that are 25-38x larger. It achieves 39.4% zero-shot performance on MMLU benchmark, very close to the GPT-3\u2019s 43.9% that has 175B parameters (227x larger) and uses 5 training examples. Meanwhile, KiC exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models. Finally, we discuss related work in Section 4 and conclude our paper with some future directions in Section 5.\n\n2 K NOLWEDGE - IN -C ONTEXT L ANGUAGE M ODEL 2.1 O VERVIEW\n\n In this section, we introduce our proposed KiC language model, which augments a parametric text-to- text Transformer (backbone) model with a knowledge-rich external memory (Figure 1). Overall, KiC consists of the following modules: (i) a parametric text-to-text backbone, (ii) an external knowledge memory with a retriever, and (iii) a knowledge selector. As shown in Figure 1, for each input instance, the knowledge selector first selects a particular knowledge category based on the input context and then retrieves the most helpful knowledge pieces for solving the current problem. The retrieved knowledge is used to complement the input context via concatenation, which is further converted into a natural language sequence using prompt templates. Then, the prompted textual inputs are fed into the text-to-text backbone model, which generates the output solution in natural language. The text-to-text backbone model can be any encoder-decoder models (e.g., T5, BART) or decoder-only models (e.g., GPT, PaLM). For convenience and without loss of generality, we adopt T5 as our backbone model throughout this paper. In the following subsections, we will explain in detail how to 2 (cid:39)(cid:85)(cid:82)(cid:88)(cid:74)(cid:75)(cid:87)(cid:17) (cid:53)(cid:72)(cid:87)(cid:85)(cid:76)(cid:72)(cid:89)(cid:72)(cid:85) construct the knowledge memory along with its retriever (Section 2.2) as well as how to learn the entire KiC model in a fully-differentiable end-to-end manner (Section 2.3).\n\n2.2 E XTERNAL K NOWLEDGE M EMORY AND R ETRIEVER\n\n Knowledge-rich external memory A significant advantage of semi-parametric models over fully- parametric ones is that we could flexibly change the knowledge resources. As shown in Table 6, structured knowledge resources can often provide more relevant and accurate knowledge than plain text. In this work, we include the following popular representative knowledge resources. We summarize the number of knowledge instances from each resource in Appendix Table 5. \u2022 Dictionary: We consider dictionary (lexical) knowledge, which records definitions and example sentences of English words. In our experiment, we leverage the largest open-source dictionary Wiktionary 1 as the lexical knowledge resource. Specifically, we use the Wiktionary dump dated April.30 2022 that contains 1.3M word senses and 470K example sentences for 1M words/phrases. \u2022 Commonsense: Besides the lexical knowledge, we also include commonsense knowledge from ConceptNet (Liu & Singh, 2004), which covers broad knowledge in our daily life. In ConceptNet, all knowledge are in the format of triplets with human-defined relations (e.g., \u201cbird\u201d -C APABLE O F - \u201cfly\u201d ). We follow previous works (Zhang et al., 2020) to include the core 600K high-quality triplets. \u2022 Entity: We cover named entity knowledge in Wikipedia and Wikidata (Vrande\u02c7ci\u00b4c & Kr\u00f6tzsch, 2014). Given an entity (e.g., \u201cUnited States\u201d ), we convert each property of it to be a separate triplet (e.g., \u201cUnited States\u201d -C APITAL - \u201cWashington D.C.\u201d ) such that the format is same as other knowledge resources. In addition to structured entity knowledge, we also include all Wikipedia sentences related to the entity. \u2022 Event: We cover knowledge about daily events with human-constructed (i.e., ATOMIC (Hwang et al., 2021) and GLUCOSE (Mostafazadeh et al., 2020)) or auto-extracted event knowledge graphs (i.e., ASER (Zhang et al., 2022a)). Similar to commonsense knowledge, all event knowledge graphs store knowledge in the triplet format, where relations are human-defined or discourse relations, the head and the tail are events. An ASER example is \u201cI am hungry\u201d - BEFORE - \u201cI eat food\u201d . \u2022 Script: Besides the knowledge covered by pre-defined relations, we also include script knowledge to cover more complex ones. We use the script knowledge from (Sun et al., 2022), which can also be regarded as a special kind of commonsense knowledge. We use 325K pieces of script knowledge, each containing a pair of related verbal and nonverbal information (e.g., \u201cOf course not. I\u2019m going......to his house.\u201d and \u201cthinking\u201d ) as well as the context where they situate. Given a query, we retrieve the most relevant scenario as external knowledge. \u2022 Causality: The last external knowledge resource we include is the auto-extracted causal knowledge CausalBank Li et al. (2020), which collects large-scale English sentences expressing cause-effect relations. It consists of 133M because mode sentences (i.e., sentences captured by 12 patterns such as \u201cbecause\u201d , \u201ccaused by\u201d , etc.) and 181M therefore mode sentences (i.e., sentences captured by 19 patterns such as \u201ctherefore\u201d , \u201cresult in\u201d , etc.) . 1 https://en.wiktionary.org/wiki/Wiktionary:Main_Page 3 (cid:3) (cid:46)(cid:48)(cid:3)(cid:20) (cid:40)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:3)(cid:20) (cid:55)(cid:72)(cid:91)(cid:87)(cid:16)(cid:87)(cid:82)(cid:16)(cid:55)(cid:72)(cid:91)(cid:87) (cid:3) (cid:46)(cid:48)(cid:3)(cid:22) (cid:40)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:3)(cid:22) (cid:40)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79) The knowledge selector can be identified as a router that is used to determine the sequence-to-expert assignment in MoE. Each expert is made up of the (shared) text-to-text model and the external memory of a particular knowledge category. Therefore, each expert is in itself a stand-alone semi-parametric language model specialized in a certain type of knowledge. To allow the option of not using any knowledge, we also include a \u201cgeneralist\u201d module, which is the (shared) text-to-text model alone. Note that although the effectiveness of knowledge such as entity and dictionary knowledge has been demonstrated on a wide range of tasks (e.g., (Zhang et al., 2019b)), other types of knowledge such as commonsense and script knowledge are only used for carefully selected tasks that tend to require these types of knowledge (Ye et al., 2019; Qiu et al., 2019). Retriever We use the target word as the key and definition as the value for dictionary knowledge and use every utterance as the key and the background context as the value for script knowledge. To effectively retrieve knowledge from the other four knowledge resources, we follow the previous work Borgeaud et al. (2022) to use dense retrieval techniques. Specifically, we convert all knowledge pieces into natural language sentences as values (e.g., I am hungry before I eat food. ) and then encode all sentences into dense vectors as keys using a SOTA sentence encoder MPNet Song et al. (2020).Given a query, we encode it with the same sentence encoder model and then retrieve the most relevant knowledge with the maximum inner product search (MIPS) search which is able to reduce search complexity from O ( n ) to O (log n ) . In KiC, we employ SCaNN Guo et al. (2020) as the MIPS search algorithm.\n\n2.3 K I C: A M IXTURE OF S EMI -P ARAMETRIC E XPERTS\n\n As we will show in our comprehensive analysis (Table 1), for a particular task, some knowledge categories help the performance while others might hurt. For this reason, it is critical to dynamically select the correct knowledge type in order to facilitate the solution of the problem. In our work, instead of using task-dependent knowledge selection, we consider a more fine-grained instance-dependent strategy: we adaptively choose the knowledge based on each input instance. We now proceed to explain how KiC learns to make such instance-dependent knowledge selection. Note that the discrete decision made by the knowledge selector will seep into the overall neural architecture in the form of a discrete latent variable. There could be several alternative methods (such as reinforcement learning (Sutton & Barto, 2018)) for learning the model with discrete latent variables. In this paper, we develop a simple yet effective approach for learning KiC in a fully-differentiable end-to-end manner. The key idea is based on an important observation that KiC can be reformulated as a special one-layer mixture-of-experts architecture, as shown in Figure 2. Note that the knowledge selector can be identified as the router that is used to determine the sequence-to-expert assignment in MoE. This is slightly different from the settings of the recent MoE works (Shazeer et al., 2017; Fedus et al., 2022), where their routers perform token-to-expert assignments. Meanwhile, each expert is made up of the text-to-text module together with a particular category of knowledge memory. Interestingly, each expert is in itself a stand-alone semi-parametric language model, which retrieves a particular kind of knowledge from its own memory to augment its inputs. In other words, each expert can be understood as a specialist with expertise in a specific knowledge category. In addition, we also 4 include a special expert named generalist , which is used to handle situation where we do not need knowledge from our memory. Furthermore, due to the original KiC design, the text-to-text modules in all the experts (and the generalist) share the same model parameters with the only difference being the non-parametric parts (i.e., the knowledge memories). Inspired by the above KiC-MoE equivalence, we now proceed to develop a fully-differentiable learning strategy for KiC by leveraging existing MoE learning approaches used in (Fedus et al., 2022). More formally, the knowledge selector S ( x ) is modeled as a ( K + 1 )-way classifier, which outputs a ( K + 1) -dimensional normalized probability vector. Its k -th element, denoted as S k ( x ) , represents the probability of choosing the k -th knowledge category for k = 0 , 1 , . . . , K , where k = 0 represents the choice of generalist (i.e., no external knowledge). Let T ( \u00b7 ) denote the text-to-text transformer and c k be the knowledge retrieved from the k -th category. Then, in KiC, we select the top-1 knowledge category according to S ( x ) and compute the output according to the following expressions: where \u2295 denotes concatenation of the input x and the retrieved knowledge c \u00af k (both in the form of natural language). Observe that KiC first selects the knowledge category \u00af k that has the highest probability, and then retrieves the most relevant knowledge c \u00af k from that category to complement the input x . The knowledge-augmented input is fed into the text-to-text model to generate the logits for the output tokens. Similar to SwitchTransformer (Fedus et al., 2022), we multiply the output logits from T ( \u00b7 ) by the probability S \u00af k ( x ) from the selector to compute the final logits for the output tokens. This is a simple yet quite effective strategy to enable differentiable learning in MoE, which was successfully used in both Shazeer et al. (2017) and Fedus et al. (2022). We adopt this similar strategy and our experiments in Section 3 will demonstrate its effectiveness in KiC learning as well. 2 Note that we currently only consider the top-1 knowledge selection (routing) for simplicity and leave the generalization to top-n selection as future work. Finally, similar to MoE, we also add an auxiliary load balancing loss together with the standard cross-entropy loss during KiC learning: where y denotes the target sequence, the subscript t indexes the t -th output token, and \u03b1 is a positive hyper-parameter that controls the tradeoff between the two loss terms. We find that, without a load balancing term, the knowledge selector tends to select only one knowledge category throughout the entire training process, which was also observed in MoE learning. There could be different choices of the load balancing loss such as the ones used in (Shazeer et al., 2017; Fedus et al., 2022), which encourage the diversity of knowledge selection in different ways based on S ( x ) . Without loss of generality, we use the same load balancing loss as in SwithTransformer (Fedus et al., 2022). The above KiC-MoE equivalence may also lead to interesting observations that could potentially benefit the studies of both semi-parametric language models and MoEs. For example, in MoE works, the experts are generally designed to be different parametric neural modules (e.g., different MLPs (Fedus et al., 2022; Shazeer et al., 2017)). However, our work shows that this may not be the only option: we can construct different experts by using the same parametric module but with different inputs. By bridging these two active areas, we hope there could be more fruitful future outcomes.\n\n3 E XPERIMENTS 3.1 A NALYSIS OF KNOWLEDGE USEFULNESS\n\n To verify our assumption that external knowledge resources can facilitate LMs in general language understanding and see effects of using different types of knowledge, we conduct single-task fine- tuning experiments on a wide range of downstream tasks (Table 1). We evaluate on 35 tasks in total and classify them into 10 categories following the P3 task categorization framework. For each 2 It might be tempting to use Gumbel-Softmax to handle the discrete latent variable in KiC. However, in order to use the straight-through-estimator during backpropagation, it has to compute the hidden states for all the experts, i . e., executing the text-to-text transformer by ( K + 1) times, which is prohibitive when K increases. 5 to dynamically useful pieces\n\n3.2 M AIN RESULTS\n\n Our main model KiC is initialized with T5-LM-adapt, an improved version of T5 that continues training T5 for additional 100K steps on the LM objective (Lester et al., 2021) to leverage its ability to generate natural language. Similar to T0, we train our KiC model on a mixture of multiple tasks (39 tasks in total) by combining and shuffling all training instances from different tasks (8.4M in 6 total) and predict on unseen (held-out) tasks to evaluate zero-shot generalization ability. Our final KiC Large model is trained using 128 NVIDIA V100 GPUs for 42 hours. Zero-shot Generalization We evaluate our KiC model on two groups of zero-shot datasets. 1) Held-out tasks of P3 contain two coreference tasks, three NLI tasks, three sentence completion tasks and one word sense disambiguation (WSD) task. Table 2) shows that our KiC Large model outperforms all zeroshot baseline models (e.g., GPT-NeoX, OPT) that are 25-38x larger. Moreover, KiC Large beats T0 XL that has 3B parameters on all 9 tasks by a large margin with our adaptive knowledge selector and only 0.77B parameters. 2) Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) benchmark is designed to measure knowledge acquired in model pretraining. MMLU covers 57 subjects under four categories, i.e., STEM, Humanities, Social Sciences and Other. Comparison with SOTA LMs are shown in Table 3. We can see that KiC Large beats all fine-tuning baseline models 7 RoBERTa Large and GPT-2 without using any training data from MMLU. Surprisingly, KiC Large achieves an average performance of 39.4% using only 0.77B parameters, which is just 4.5% below GPT-3 that has 175B parameters (227x larger) plus 5 training examples. KiC in Multi-task training To see whether our KiC learning can help with multi-tasking training, we reproduce T0 Large with the same collection of tasks and evaluate KiC Large on the validation set of each in-domain task (Table 4). Here, in-domain tasks can be divided into two groups - tasks used in multitask training and tasks not used in multitask training but within the observed task category. Again, KiC Large outperforms T0 Large , with significant improvement on in-domain unseen tasks (tasks with *) such as Race and BoolQ and knowledge-intensive tasks such as CosmosQA and DREAM. It demonstrates the superiority of our proposed KiC learning in multi-tasking training. Emerging behavior Wei et al. (2022) discover that language models usually can only perform a near random zero/few-shot performance when they are small but achieves a substantial performance jump when they reach a certain critical threshold of scale (size). A language model is generally considered superior if it can show emerging behavior at a smaller model scale. Therefore, we compare our KiC model with T5 and T0 on held-out tasks to see how performance change with respect to their model sizes. From Figure 3, we can see that T5 is around random guess when the model is below 11B. T0 is better than T5 as it shows emerging behavior when it increases from 3B to 11B. Surprisingly, our KiC model shows emerging behavior when it increases from 0.22B to 0.77B, which demonstrates that our semi-parametric model can achieve the same language understanding capacity using much fewer parameters with the help of adaptive knowledge selector and external knowledge.\n\n3.3 R ESULT ANALYSIS\n\n We further analyze the improved instances to investigate which kind of knowledge is more helpful for a certain task after multitask training that leverages multi-source knowledge. We observe that the dominating knowledge type picked by the selector may not be the same as that in the single-task single-knowledge setting. For example, for the RTE dataset, the top-2 frequently selected knowledge types (script and causal) actually hurt the performance in the previous setting (Table 1). Besides, it is also possible that a benchmark originally constructed to mainly test one type of knowledge does not benefit from this type. For example, for a commonsense reasoning benchmark Winogrande, more instances are improved by the entity knowledge than other types of knowledge including commonsense. One possible reason might be that entity knowledge we include in this work also involve other types of knowledge: e.g., for a Winogrande instance \u201cIn the sentence below, does the _ stand for ear or piercing? Mary wanted to get another piercing in her ear, but the _ was much too tiny.\u201d , the selected entity knowledge is a description about \u201cpiercing\u201d ( \u201cThe word \u201cpiercing\u201d can refer to the act or practice of body piercing, or to an opening in the body created by this act or practice...\u201d ). The observations show the importance of our instance-adaptive knowledge selector. 8\n\n4 R ELATED W ORK\n\n Knowledge Injection of PLMs Although PLMs can capture knowledge such as linguistic, semantic, commonsense, and world knowledge to some extent, they can only memorize knowledge vaguely in parameters, causing poor performance on knowledge-intensive tasks. Recent studies make a great effort to inject knowledge such as lexical knowledge, entity knowledge graph, or syntactic knowledge into LM pre-training (Yang et al., 2021). For example, besides masked language modeling (MLM) and next sentence prediction (NSP), Lauscher et al. (2020) add synonyms and hyponym-hypernym relation prediction between words and Levine et al. (2020) add supersense prediction of masked words into LM training objectives. To use entity knowledge, ERNIE 2.0 (Sun et al., 2020) introduces named entity masking to learn better embeddings for semantic units, Peters et al. (2019) include entity linking, hypernym linking into pre-training and K-BERT (Liu et al., 2020) uses entity knowledge triples to construct knowledge-rich sentence trees. For syntax knowledge injection, Wang et al. (2021) integrate dependency relation prediction into LM training and Bai et al. (2021) incorporate syntax tree information through a syntax-aware self-attention mechanism. Semi-parametric language models Most of the existing works on semi-parametric language models (Khandelwal et al., 2019; Zhong et al., 2022; Grave et al., 2017; Merity et al., 2017; de Mas- son d'Autume et al., 2019; Guu et al., 2020; Fan et al., 2021; Lewis et al., 2020) mainly focus on improving the language modeling capability (e.g., improving perplexities) or a particular category of downstream task (e.g., open-domain question answering). Some recent works (Izacard et al., 2022; Borgeaud et al., 2022; Petroni et al., 2021) seek to improving diverse downstream tasks with an external memory. All these works augment the parametric language model with memories of plain texts. In contrast, we focus on developing semi-parametric language models with a knowledge-rich memory for improving the performance of a wide range of downstream language tasks.\n\n5 C ONCLUSIONS AND F UTURE W ORK\n\n This work develops a novel semi-parametric language model architecture, Knowledge-in-Context (KiC) , which empowers a parametric text-to-text language model with a knowledge-rich external memory containing six different types of knowledge. We also design an instance-adaptive knowledge selector to retrieve the most helpful pieces of knowledge for each input instance. As a knowledge-rich semi-parametric language model, KiC only needs a relatively smaller parametric part to achieve superior zero-shot performance on unseen tasks and exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models. Future work may include future exploiting unstructured plain texts to pre-train KiC. 9\n\nR EFERENCES\n\n 10 11 open for Computational . doi: /v1/D18-1260. URL https://aclanthology.org/D18-1260 . 12 13 14 15\n\nA PPENDIX A E XPERIMENTAL D ETAILS\n\n A.1 K NOWLEDGE S TATISTICS Question High-pressure systems stop air from rising into the colder regions of the atmo- sphere where water can condense. What will most likely result if a high-pressure system remains in an area for a long period of time? Answer Drought CausalBank (KG) Persistent high pressure has a stabilizing effect on the weather, causing subsiding air that dries out the atmosphere. Wikipedia High - pressure systems are alternatively referred to as anticyclones. On English - language weather maps, high - pressure centers are identified by the letter H in English, within the isobar with the highest pressure value. On constant pressure upper level charts, it is located within the highest height line contour.\n\nA.2 I MPLEMENTATION DETAILS\n\n\n\nB A DDITIONAL E XPERIMENTAL R ESULTS\n\n 16 17", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes a novel semi-parametric language model architecture called Knowledge-in-Context (KiC) that augments a parametric text-to-text language model with an external memory containing six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge.", "topic_class_1": "Summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "For each input instance, the KiC model adaptively selects the most helpful knowledge type and retrieves the most relevant knowledge pieces.", "topic_class_1": "Summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The knowledge-augmented input is then fed into a text-to-text model like T5 to generate the output.", "topic_class_1": "Summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors find that KiC can be formulated as a special mixture-of-experts (MoE) model, where the knowledge selector acts as the router to determine the input-to-expert assignment, inspiring a novel end-to-end learning algorithm.", "topic_class_1": "Summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on over 40 tasks show that KiC Large with 770M parameters outperforms much larger language models by a large margin in zero-shot settings.", "topic_class_1": "Summary", "topic_class_2": "Experiment Summary", "reliability": "No", "explanation": "Zero-shot evaluations on 9 tasks (as shown in Table 2) and MMLU (Table 3) suggest that KiC-large with 0.77B parameters outperform much larger language models by a large margin on most of them.", "error_type": "Inaccurate Summary"}, {"segment_text": "KiC also exhibits emergent abilities at a much smaller scale compared to fully-parametric models.", "topic_class_1": "Summary", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The KiC architecture that augments a parametric LM with a knowledge-rich external memory containing multiple knowledge types is novel and well-motivated.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength", "reliability": "No", "explanation": "Missing baselines with augmenting LM with knowledge-rich external memory. The paper stresses the novelty as semi-parametric.", "error_type": "Misunderstanding"}, {"segment_text": "- Formulating KiC as a special MoE model is an interesting observation that enables fully differentiable end-to-end learning of the model.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The comprehensive experiments on over 40 tasks demonstrate the effectiveness of KiC in achieving superior zero-shot performance with much fewer parameters compared to large language models.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Experiment Strength", "reliability": "No", "explanation": "Simiar to 6. Experiments cover single task fine-tuning, and zero-shot generalization. Specifically, zero-shot evaluation cover 9 tasks and MMLU.", "error_type": "Misunderstanding"}, {"segment_text": "- Analysis on the usefulness of different knowledge types for different tasks provides good insights.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Experiment Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- More details can be provided on the prompting and knowledge selection/retrieval process with concrete examples to improve clarity.", "topic_class_1": "Weakness / Method", "topic_class_2": "Implementation Detail Shortage", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Ablation studies on the impact of different knowledge types and the knowledge selector would strengthen the paper.", "topic_class_1": "Weakness / Method", "topic_class_2": "Ablation Shortage", "reliability": "No", "explanation": "These experiments of ablating knowledge types and selector is conducted in Table 1 at the beginning of the experiment section.", "error_type": "Neglect"}, {"segment_text": "- Some more discussion on the limitations and potential negative impacts of the model is recommended.", "topic_class_1": "Weakness / Experiment", "topic_class_2": "Analysis Recomendation", "reliability": "No", "explanation": "This has not to be be the focus of this paper.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is overall clearly written with good organization.", "topic_class_1": "Strength / Quality", "topic_class_2": "Good Writing", "reliability": "No", "explanation": "The writing is not clear.", "error_type": "Writing"}, {"segment_text": "The proposed KiC model is novel and the comprehensive experiments demonstrate the high quality of the work.", "topic_class_1": "Strength / Novelty", "topic_class_2": "Novelty and Comprehensive Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Key implementation details are provided, though some more specifics on the prompting and knowledge selection would further aid reproducibility.", "topic_class_1": "Suggestion on reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors state that code will be released.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": "Reproducibility", "reliability": "No", "explanation": "I don't find authors' statement in the paper.", "error_type": "Unstated statement"}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents Knowledge-in-Context (KiC), a novel semi-parametric language model architecture that effectively incorporates multiple types of knowledge from an external memory to achieve impressive zero-shot performance across many tasks with a relatively small number of parameters.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The MoE-based formulation of KiC that enables end-to-end learning is innovative.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Method Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The extensive experiments and analyses provide strong support for the proposed approach.", "topic_class_1": "Summary of Strength on Experiments", "topic_class_2": "Experiment Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some added details on the methodology and discussion on limitations would further strengthen the paper.", "topic_class_1": "Suggestion on paper presentation", "topic_class_2": "Detail Shortage", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a high-quality paper that makes valuable contributions to the field.", "topic_class_1": "Reviewer opinion", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Overview", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces Knowledge-in-Context (KiC), a novel semi-parametric language model that augments a parametric text-to-text transformer model with a diverse, knowledge-rich external memory.", "topic_class_1": "Summary ", "topic_class_2": "Overview", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "KiC employs six types of knowledge: entity, dictionary, commonsense, event, script, and causality, to enhance the model's ability to perform across 40+ diverse natural language processing tasks.", "topic_class_1": "Summary ", "topic_class_2": "Overview", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The model adaptively selects relevant knowledge for each input instance, showcasing emergent abilities at a smaller scale compared to fully-parametric models.", "topic_class_1": "Summary ", "topic_class_2": "Overview", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "KiC achieves superior zero-shot performance on unseen tasks with significantly fewer parameters, demonstrating its effectiveness and efficiency.", "topic_class_1": "Summary ", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty and Impact: KiC introduces a unique approach to integrating external knowledge into language models, addressing the limitations of fully-parametric models in adaptability and scalability.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This could significantly impact areas requiring rapid adaptation to new knowledge.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Technical Quality: The model design is robust, leveraging a diverse set of knowledge sources and an innovative instance-adaptive knowledge selector.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are comprehensive, covering a wide range of tasks and demonstrating significant improvements.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity and Reproducibility: The paper is well-organized, with clear explanations of the methodology and experimental setup.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": "Strength", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The inclusion of implementation details and a commitment to releasing code further enhance its reproducibility.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": "Strength", "reliability": "No", "explanation": "While the paper discussed about implementation details, it doesn't mention anything about release the code.", "error_type": "Unstated statement"}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limitations in Knowledge Coverage: While KiC covers a broad range of knowledge types, the scalability of adding new knowledge types or updating existing ones remains unclear.", "topic_class_1": "Weakness / Method", "topic_class_2": "Coverage Weaknesses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Potential for Bias: The reliance on external knowledge sources, especially those automatically extracted, may introduce biases or inaccuracies that could affect the model's outputs.", "topic_class_1": "Weakness / Method", "topic_class_2": "Potential Weaknesses", "reliability": "No", "explanation": "This is too generic and thus can be applied to any paper.", "error_type": "Superficial Review"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written, providing clear explanations of the model architecture, knowledge integration mechanism, and experimental results.", "topic_class_1": "Strength / Clarity", "topic_class_2": "Writing Quality", "reliability": "No", "explanation": "The writing is not clear.", "error_type": "Writing"}, {"segment_text": "Quality: The research is of high quality, presenting a thorough analysis of the model's performance across a diverse set of tasks.", "topic_class_1": "Strength / Quality", "topic_class_2": "Research Quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: KiC's approach to integrating external knowledge into a semi-parametric model is novel and represents a significant contribution to the field.", "topic_class_1": "Strength / Novelty", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The detailed description of the experimental setup and the promise to release code make this work highly reproducible.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": "Reproducibility", "reliability": "No", "explanation": "Again, I don't find the paper promise they will release the code.", "error_type": "Duplication"}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The Knowledge-in-Context (KiC) model represents a significant advance in semi-parametric language modeling, effectively leveraging external knowledge to enhance performance across a broad range of tasks.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Strength Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Its innovative approach to knowledge integration and the ability to adaptively select relevant knowledge for each input instance are particularly noteworthy.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Strength Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While there are limitations related to the scalability of knowledge coverage and potential biases, the overall impact, technical quality, and novelty of the work are commendable.", "topic_class_1": "Summary of Concerns on Method and Experiments ", "topic_class_2": "Strength & Weakness Tradeoff", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The clarity of presentation and commitment to reproducibility further strengthen this submission.", "topic_class_1": "Summary of Strength on Reproducibility", "topic_class_2": "Strength & Weakness Tradeoff", "reliability": "No", "explanation": "Again, I don't find the paper promise they will release the code.", "error_type": "Duplication"}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes Knowledge-in-Context (KiC), a novel semi-parametric language model architecture that combines a parametric text-to-text model (e.g., T5) with a knowledge-rich external memory.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The external memory encompasses six knowledge categories: entity, dictionary, commonsense, event, script, and causality.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "KiC employs a knowledge selector to dynamically choose the most relevant knowledge type for each input instance, augmenting the input with retrieved knowledge pieces before feeding it to the text-to-text model for answer generation.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper argues that KiC can be viewed as a special mixture-of-experts (MoE) model, enabling the development of a novel differentiable learning algorithm for training the knowledge selector.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments demonstrate that KiC outperforms much larger fully-parametric models on a variety of tasks, exhibiting emergent abilities at a smaller scale.", "topic_class_1": "Summary", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel architecture: KiC introduces a unique approach to integrating diverse knowledge sources into a semi-parametric language model framework.", "topic_class_1": "Strength / Method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Adaptive knowledge selection: The instance-dependent knowledge selection mechanism allows KiC to utilize the most relevant knowledge for each specific input.", "topic_class_1": "Strength / Method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strong empirical results: KiC outperforms significantly larger fully-parametric models on a wide range of tasks, demonstrating its effectiveness and efficiency.", "topic_class_1": "Strength / Experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Emergent abilities: KiC exhibits emergent abilities at a smaller model scale compared to fully-parametric models, suggesting superior knowledge utilization.", "topic_class_1": "Strength / Experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clear and well-written paper: The paper presents the KiC architecture and learning algorithm in a clear and well-organized manner.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited analysis of knowledge quality: The paper lacks quantitative and qualitative analysis to assess the quality of the retrieved knowledge pieces.", "topic_class_1": "Weakness / Analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Missing baselines: The paper does not compare KiC with a baseline that uses few-shot examples to generate augmentation data, making it difficult to fully gauge the performance gains attributable to the proposed approach.", "topic_class_1": "Weakness / Baselines", "topic_class_2": NaN, "reliability": "No", "explanation": "As the paper stresses on semi-parameteric language models with external knowledge memory, missing baselines should be related to LLM with external memory, or retrival-augmented generation with LLM. Using few-shot examples is not accurate.", "error_type": "Out-of-scope"}, {"segment_text": "Applicability to large PLMs: The current implementation of KiC may not be readily scalable to large PLMs due to computational constraints.", "topic_class_1": "Weakness / Experiments", "topic_class_2": NaN, "reliability": "No", "explanation": "The parameters of the method are just a PLM and a selector. Thus, the method does not introduce much new parameters and doesn't cause computation problems.", "error_type": "Misunderstanding"}, {"segment_text": "Limited task types: The experiments primarily focus on classification tasks, leaving the performance of KiC on other NLP tasks (e.g., question answering) unexplored.", "topic_class_1": "Weakness / Experiments", "topic_class_2": NaN, "reliability": "No", "explanation": "Question answering covers many types of tasks. Current tasks such as PIQA and WIQA are also question answering tasks. Other NLP tasks may include knowledge reasoning, complex logical query QA, machine reasoning comprehension, explainable QA.\nQA should be considered as a type of classification, and the paper tested QA tasks in Table 4. The truth shortage of task type is that the paper doesn't include generation tasks.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written and easy to follow.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": NaN, "reliability": "No", "explanation": "The writing is not clear and thus hard to follow.", "error_type": "Writing"}, {"segment_text": "Quality: The research is well-designed and executed, with comprehensive experiments and detailed analysis.", "topic_class_1": "Strength / Quality", "topic_class_2": NaN, "reliability": "No", "explanation": "This conflict with the reviews within weaknesses.", "error_type": "Contradiction"}, {"segment_text": "Novelty: KiC presents a novel approach to semi-parametric language modeling with its adaptive knowledge selection and diverse knowledge sources.", "topic_class_1": "Strength / Novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides code in the supplementary materials, facilitating reproducibility.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additional Questions for Authors:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Can you provide more details on the quantitative and qualitative analysis of the retrieved knowledge pieces?", "topic_class_1": "Question on analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Have you considered comparing KiC with a baseline that uses few-shot examples for data augmentation?", "topic_class_1": "Question on baseline", "topic_class_2": NaN, "reliability": "No", "explanation": "Same as 17. Besides, using few-shot examples with more powerful LLMs (chatgpt or GPT 4) should also be considered as baselines, as they have much stronger zero-shot performacens than LLMs such as GPT3 or GPT-NeoX used in the paper\n(This question is duplicated) The parameters of the method are just a PLM and a selector. Thus, the method does not introduce much new parameters and doesn't cause computation problems.", "error_type": "Misunderstanding"}, {"segment_text": "How can KiC be scaled to larger PLMs, addressing the computational constraints?", "topic_class_1": "Question on experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Do you plan to explore the performance of KiC on other NLP tasks beyond classification?", "topic_class_1": "Question on experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper presents a valuable contribution to the field of NLP, and KiC is a promising approach to semi-parametric language modeling with its adaptive knowledge selection and diverse knowledge sources.", "topic_class_1": "Strength / Method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addressing the weaknesses and exploring the additional questions will further strengthen the work and broaden its impact.", "topic_class_1": "Suggestion on analysis and experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "KiC is a promising semi-parametric language model architecture that effectively leverages diverse knowledge sources to achieve strong performance on a variety of tasks.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, further analysis is needed to assess the quality of the retrieved knowledge and the scalability of the approach to larger PLMs.", "topic_class_1": "Summary of Weakness on  Analysis and Method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additionally, exploring the performance of KiC on a wider range of NLP tasks would provide a more comprehensive understanding of its capabilities.", "topic_class_1": "Suggesion on Experiments", "topic_class_2": NaN, "reliability": "No", "explanation": "This is nice-to-have tasks but not must.", "error_type": "Out-of-scope"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Recursion of Thought: Divide and Conquer Reasoning with Language Models", "body_text": "\n\nR ECURSION OF T HOUGHT : D IVIDE AND C ONQUER R EASONING WITH L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n With the recent advances in language models, attempts are being made to apply them to solving multi-step reasoning problems. A major breakthrough in this line of research is to let language models generate intermediate steps, often called Chain of Thought (CoT), before producing a final answer. However, language models have an upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM. Although several thousand tokens are enough to handle various tasks, solving more complex reasoning tasks can require orders of magnitude more tokens. Therefore, the context limit imposes a fundamental limit on the model\u2019s reasoning capability. Inspired by human\u2019s in- credible reasoning ability based on abstraction and recursion, we propose Recur- sion of Thought (RoT) as a model-agnostic framework with the novel paradigm of teaching a language model to divide and conquer complex problems by recur- sively creating multiple contexts. Since RoT casts the context-related operations as tokens, a language model can trigger the recursion operations by simply produc- ing the corresponding tokens. On multiple arithmetic and algorithmic reasoning tasks, we demonstrate that RoT dramatically improves the recent large-scale lan- guage model GPT-3 to solve extremely complex problems. Moreover, RoT can make tiny, randomly initialized Transformers or LSTMs to solve problems that even humans find daunting.\n\n1 I NTRODUCTION\n\n Although neural networks have achieved amazing results on various domains, e.g., images, texts, audios, videos, games, etc., nearly all of them are classified as System 1 tasks (Kahneman, 2013), which can be achieved by fast and reflexive thinking. On the other hand, applying neural networks to System 2 tasks, which require sequential reasoning, is still in the early stages of research. Currently, one of the most prominent directions is to use large language models (LMs) by formulating the reasoning problems as a sequence modeling task. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or a chain of thought, before producing the final answer. Directly answering a question would require a model to fully solve the problem in a single forward pass, meaning the range of solvable problems is severely limited by the model\u2019s capacity. On the other hand, generating CoT before the answer allows the problem\u2019s complexity to be spread across the CoT, making each token generation more straightforward given the previous tokens. This is closer to how humans solve complex problems, as we think step by step, instead of producing an answer reflexively. Although CoT seems promising, there is a critical issue that significantly limits its utility: the effec- tive context size of sequence models cannot grow unbounded. In this work, a context refers to the set of input tokens that a model is conditioned on when generating an output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Trans- formers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and 1 RNNs (Hochreiter & Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art language models, such as GPT-3 (Brown et al., 2020) and PaLM (Chowd- hery et al., 2022), limit the maximum context length by up to 2048 tokens. However, the length of CoT can grow rapidly with the problem\u2019s complexity. For example, even in relatively simple integer multiplication, CoT quickly exceeds the context limit if the number of digits in each operand becomes more than a few. This problem must be overcome to solve more challenging and useful reasoning problems, whose solutions may require millions of tokens. Humans can handle this issue by using abstraction and recursion. We divide a large problem into smaller subproblems and focus on each subproblem while solving it, instead of considering the entire problem at every step. We can further subdivide a subproblem into even smaller subproblems. With this intuition, we propose Recursion of Thought (RoT) as a model-agnostic framework for recursively solving multi-step reasoning problems. The key feature of RoT is to grant the model the ability to recursively create and utilize multiple contexts for subproblems. We achieve this feat by introducing several special tokens that a model can output to control its context. During inference, the model recursively solves the problems by producing appropriate tokens at the right time. Moreover, RoT supports tail recursion, which enables general computation with an indefinitely long chain of recursion. We demonstrate RoT on four basic arithmetic operations (addition, subtraction, multiplication, and division) and four algorithmic tasks (longest common subsequence, longest palindromic subse- quence, 0-1 knapsack, and matrix chain multiplication) to show its generality. Without any task- specific component, such as a calculator, all tasks are formulated as autoregressive sequence model- ing problems. These tasks require a model to generalize by just seeing a tiny fraction of the problem space since the space is combinatorially large. For example, even in simple arithmetic operations, two 6-digit operands result in one trillion possible combinations. Hence, we evaluate whether a model understands the underlying rules, instead of brute force memorization. In our experiments, the range of problems that CoT can handle is seriously constrained by the context limit. On the other hand, RoT leads language models to achieve near perfect accuracy, even if the problem size increases to the extreme, where solving one problem requires producing hundreds of thousands of tokens. Moreover, the dramatic improvement is not limited to large pre-trained language models like GPT-3. RoT can make tiny, randomly initialized Transformers or LSTMs perform extremely complex reasoning. The key messages of this work are summarized as follows: \u2022 The reasoning capability of current language models is seriously constrained by the maxi- mum length of a single context . \u2022 Our Recursion of Thought (RoT) unleashes the reasoning capability of language models by letting them recursively create and utilize multiple contexts of subproblems, following the principle of divide and conquer. In the supplementary file, we provide the source code to fully reproduce our experiments.\n\n2 R ELATED W ORK\n\n Chain of Thought. Among several prior works on applying language models to reasoning, Scratch- pad (Nye et al., 2021) may be the most closely related to our work. It is the first approach to fine-tune language models to produce CoT before generating an answer. It demonstrates its effectiveness on 8-digit addition, polynomial evaluation, and Python program execution. It also mentions the con- fined context size as a major limitation to be overcome. In order to unlock the full potential of Scratchpad, the authors argue that Transformers should be improved to allow greater context sizes. We solve this exact problem from a completely different perspective, i.e., using multiple contexts to divide-and-conquer. Our approach is more practical and scalable, compared to increasing the context limit. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt . For instance, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in the similar manner. Kojima et al. (2022)\u2019s prompting is even simpler; after a question, they start the answer with \u201cLet\u2019s think step by step,\u201d and then let the model finish the rest. Even without fine-tuning, these methods significantly improve the reasoning accuracy of 2 language models. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks. However, all of these works are still limited by the maximum context size. Neural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed & de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019). System 1 Approaches for Reasoning. It is hard to define strict criteria to distinguish between System 1 and System 2 approaches. We classify a model as System 1 if it directly outputs an answer, while System 2 generates its process, as well. In that sense, RoT, CoT and NPI are System 2 approaches, while there have been various System 1 approaches to solve symbolic reasoning. Zaremba & Sutskever (2014) train LSTMs with curriculum learning to solve integer addition up to nine digits. Kaiser & Sutskever (2016) propose a convolutional architecture called Neural GPU that performs binary addition and multiplication. Trained on 20-bit problems, this model operates like a digital circuit for the arithmetic operations, which can generalize up to 2,000 bits. Similarly, Yan et al. (2020) solve 8-bit binary addition and 12-bit binary multiplication with Transformers. If Neural GPUs are like digital circuits, Neural Arithmetic Logic Units (Trask et al., 2018) are like analog circuits for arithmetic. They represent numerical quantities with the activation values of neural networks and design a clever architecture for arithmetic operations. Although these System 1 approaches fall behind System 2 methods in terms of generality, they can be highly efficient in specific domains. We believe future systems will often be a hybrid of System 1 and System 2, harnessing the advantages of both types.\n\n3 R ECURSION OF T HOUGHT\n\n The main idea of Recursion of Thought (RoT) is to let the model recursively solve small subproblems in separate contexts, keeping each reasoning step simple and learnable. Our RoT is model-agnostic and general enough to be combined with any kind of sequence model that supports autoregressive generation. The only requirement is that the model should be able to infer p (cid:0) x i (cid:17) 1 \u00b6 X 1 (cid:2) i (cid:6) , the proba- bility of the next token x i (cid:17) 1 given a sequence X 1 (cid:2) i (cid:0) (cid:18) x 1 ; ... ; x i (cid:24) . Therefore, sequence models such as Transformers, RNNs, or more advanced ones can all be used in the RoT framework. RoT teaches a sequence model to solve a problem using the divide and conquer paradigm in a supervised man- ner. That is, we assume that ground truths for the intermediate steps of how to recurse are readily available, as in Scratchpad (Nye et al., 2021) or NPI (Reed & de Freitas, 2016). For better understanding, we discuss RoT in the reverse order of the pipeline. In \u00a73.1, we first describe how to perform RoT inference with a fully trained model. In \u00a73.2, we introduce the training process. Finally, in \u00a73.3, we discuss how to recursively divide the problems and build the training data for intermediate steps automatically.\n\n3.1 I NFERENCE\n\n Recursion of Thought (RoT) grants a language model the ability to control the recursion process. For basic recursion control, we first introduce the following special tokens: GO , STOP , and THINK . GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO - STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem solving. We formulate each inference context of a QA problem, denoted X , as the following concatenation: where Q and A are the main question and answer sequence, and Q sub , (cid:152) and A sub , (cid:152) are the ques- tions and answers of the top-level subproblems. During inference, a model is given Q and tasked to generate the rest. Questions (both Q and Q sub , (cid:152) ) start with a GO token, and answers (both A 3 1 \ud835\udc44 sub 1 GO 8 1 \ud835\udc34 sub,1 THINK THINK \ud835\udc34 sub,2 GO 4 0 + 3 5 = \ud835\udc44 sub,2 7 5 STOP 9 \ud835\udc34 9 STOP 7 5 STOP \ud835\udc4b GO 8 + 1 \ud835\udc44 \ud835\udc34 \ud835\udc4b GO 4 0 + 3 5 = THINK GO 0 + 5 = \ud835\udc34 sub,1 \ud835\udc44 sub,1 \ud835\udc44 5 STOP THINK GO 4 + 3 = \ud835\udc34 sub,2 \ud835\udc44 sub,2 \ud835\udc34 7 5 STOP GO 0 + 5 \ud835\udc44 \ud835\udc34 \ud835\udc4b GO 4 + \ud835\udc44 \ud835\udc34 7 and A sub , (cid:152) ) end with a STOP token. For trivial cases, i e., the base cases of recursion, the context contains no (cid:0) Q sub , (cid:152) , A sub , (cid:152) (cid:6) pair. A subproblem can have smaller, lower-level subproblems recur- sively, but only the top-level subproblems remain in a context. As a result, we can abstract away the details for solving the subproblems and keep only the high-level results in the current context. For tail recursion, where the last subquestion\u2019s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion Q sub ,N , its answer A sub ,N is treated as the final answer A , and the context X does not have duplicate A . Algorithm 1 summarizes the inference process. Figure 1 presents a example of solving 408 (cid:17) 351 for better understanding. More detailed illustrations of inference can be found in Appendix A. The RoT function (L1) takes a fully trained model with a question Q as input and returns the answer A as output. The procedure starts by initializing the context X with the original question Q (e.g., GO 4 0 8 + 3 5 1 = in Figure 1). i ans is the starting index of the answer, which is initialized to \u00b6 X \u00b6 (cid:17) 1 , where \u00b6 (cid:0) \u00b6 is the length of a sequence. Then, in the main loop, the model iteratively generates a next token x from X , which is appended to the end of X . After the initialization, the model is expected to (i) generate answer A directly or (ii) output GO . If the model immediately generates A and finishes it with a STOP , the answer is returned (L9), which is the base case of the recursion. Up to this point, the algorithm is identical to common language models. On the other hand, if the model decides to output GO , which signals the start of a subproblem, its index is stored in i go (L11). Since a subproblem has started, the next several tokens from the model should constitute the question of the subproblem. In Figure 1, the first subproblem of the main context X 1 is adding the last digits, i.e., 8 (cid:17) 1 . Once the subquestion is generated, the next step is to find an answer to it. This is the pivotal moment: instead of producing the answer, the model outputs the THINK token, which initiates the recursion with a new context (L16-23). First, we separate the subquestion Q sub , starting from i go (L16). Second, using it as an input, we trigger the recursive call (L17) to obtain the answer A sub . As shown as red arrows in Figure 1, this call creates another context X 2 and initializes its question part with the subquestion. Then, the same inference process is executed inside the new context, sometimes running more recursions. In the case of 8 (cid:17) 1 , the answer 9 STOP is immediately returned since it is a base case. Finally, the THINK token is replaced 4 with A sub (L21) and the starting position of the answer is updated (L23). Hence, when predicting the next token, the model sees the returned answer as input, instead of the THINK token. Algorithm 1 Require: A sequence model M trained for Re- cursion of Thought, a question sequence Q 1: function R O T( M , Q ) 2: X (cid:2) Q V Initialize context with Q 3: i ans (cid:2) \u00b6 X \u00b6 (cid:17) 1 V Start of answer 4: t (cid:2) false V Tail recursion 5: while True do 6: x (cid:2) M (cid:0) X (cid:6) V Generate next token 7: X (cid:2) (cid:18) X ; x (cid:24) 8: if x (cid:0) STOP then 9: return X i ans (cid:2) \u00b6 X \u00b6 10: else if x (cid:0) GO then 11: i go (cid:2) \u00b6 X \u00b6 V Mark last GO 12: else if x (cid:0) TAIL then 13: i go (cid:2) \u00b6 X \u00b6 14: t (cid:2) true V Mark tail recursion 15: else if x (cid:0) THINK then 16: Q sub (cid:2) X i go (cid:2)\u00b6 X \u00b6(cid:14) 1 17: A sub (cid:2) R O T (cid:0) M , Q sub (cid:6) 18: if t then 19: return A sub 20: end if 21: X (cid:2) (cid:18) X 1 (cid:2)\u00b6 X \u00b6(cid:14) 1 ; A sub (cid:24) 22: V Replace THINK with A sub 23: i ans (cid:2) \u00b6 X \u00b6 (cid:17) 1 24: end if 25: end while 26: end function Algorithm 2 Creating the target sequence \u00b6 A sub ,n \u00b6(cid:14) 1 (cid:24) 5: end for 6: Y (cid:2) (cid:18) Y ; A (cid:24) 7: return Y Once a subproblem is finished, the model can solve another subproblem in the same way, or output the final answer. In Figure 1, the second subproblem is to add all the remaining digits, i.e., 40 (cid:17) 35 . To solve it, the model recursively solves two other subproblems, i.e., adding each digit, be- fore generating the answer. If the model outputs the final answer followed by a STOP token, the answer is returned (L9). If the model starts a sub- problem with TAIL instead of GO , it becomes a tail recursion (L14), and its answer is returned di- rectly as the final answer (L19).\n\n3.2 T RAINING\n\n We teach RoT in a supervised manner; the model is trained with the ground truth (GT) intermediate steps, which also include when to output the spe- cial tokens. Each training example is constructed as a pair of a ground truth context sequence X and the corresponding target sequence Y . The GT context X is structured as Equation 1 and au- tomatically built by the algorithms that will be in- troduced in \u00a73.3 and Appendix D. In this section, we discuss how to construct the target sequence Y for X , and define the training objective. Algorithm 2 summarizes the process of convert- ing X to Y , where Y has the same length with X . Refer to Figure 2 for an example. Overall, Y is a copy of X except for the parts corresponding to Q and A sub , (cid:152) . Since the question Q is always given in a context, Q is replaced by special PAD tokens (L1), which mean \u201cnothing to predict for this part.\u201d Each subproblem\u2019s answer A sub ,n is replaced by a THINK token followed by several PAD s that fill in the rest to make sure \u00b6 X \u00b6 (cid:0) \u00b6 Y \u00b6 (L4). This way, the model is trained to output THINK instead of the first token of A sub ,n . Since the whole A sub ,n will be returned from the recur- sive process and replace the THINK during infer- ence (L17,21 of Algorithm 1), we do not need a training signal for the rest of A sub ,n . Given a pair (cid:0) X, Y (cid:6) , the training objective is de- fined as follows: where I is the indicator function that excludes PAD s from training. Simply, it means that the se- quence model is trained to output y i (cid:17) 1 as the next token for a given X 1 (cid:2) i . Its form is almost identical to the standard language modeling objective: L LM (cid:0) (cid:14) < i log p (cid:0) x i (cid:17) 1 \u00b6 X 1 (cid:2) i (cid:6) , which is to predict the next token given previous tokens. With this objective, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent. For decoder-only transformers with causal masks, the training can be efficiently done in parallel for all tokens. 5\n\n3.3 T HE R ECURSIVE R EASONING P ROCEDURES\n\n As explained in \u00a73.2, we teach the recursive reasoning procedure for a problem type by providing ground truth contexts. To produce these contexts, we design an exemplary reasoning procedure for each problem type and implement it as a teacher program that automatically generates the contexts given a problem. The major desiderata for the exemplary reasoning procedures are two fold: (i) each context should be short, and (ii) the next tokens to generate should be obvious from the pre- vious sequence. Although the definition of \u201cbeing obvious\u201d may vary depending on the model\u2019s architecture (e.g., Transformers vs. LSTMs), the reasoning procedures developed for human gener- ally meet these desiderata. Therefore, our procedures for the arithmetic problems are borrowed from the elementary school math. For example, the addition procedure shown in Figure 1 is a recursive version of adding digits one by one starting from the last digit. In another example of multiplica- tion, an N -digit (cid:18) M -digit multiplication is divided into an N -digit (cid:18) 1-digit multiplication and an N -digit (cid:18) (cid:0) M (cid:14) 1 (cid:6) -digit multiplication. The N -digit (cid:18) 1-digit multiplication is further divided into a 1-digit (cid:18) 1-digit multiplication and an (cid:0) N (cid:14) 1 (cid:6) -digit (cid:18) 1-digit multiplication. For the algorithmic problems, we also borrow standard, well-known algorithms. In Appendix D, we provide the full details of the procedures for each problem type, with Python code snippets of the key parts. Note that our proposals for the reasoning procedures in Appendix D are one of many possible solutions, which are not necessarily optimal. Training Data Distribution. We use the same problem distribution for both training and evalu- ation, since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix C. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole prob- lem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need more sophisticated method, such as the adaptive curriculum learning in Reed & de Freitas (2016).\n\n4 E XPERIMENTS\n\n Since Recursion of Thought is the first approach of this kind, we mainly compare with two baselines. The first one is to output an answer directly from a question, which we call Without Thought (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as Chain of Thought (CoT; not to be confused with the CoT prompting (Wei et al., 2022)) for consistency. We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix B for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems. Refer to Appendix I for more detailed analysis of the context sizes. Note that we train these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct. In other words, we impose stricter rules on both RoT and CoT by not counting \u201clucky guesses\u201d as correct.\n\n4.1 T HE R EASONING P ROBLEMS\n\n To evaluate the reasoning capabilities, we test various reasoning tasks that are grouped into two categories: arithmetic reasoning and algorithmic reasoning. We below provide a rough description of the tasks, whose details can be found in Appendix C. All the reasoning tasks share one characteristic in common: we can easily adjust the problem\u2019s difficulty. Therefore, we can gradually increase the degree of difficulty and see which method fails first. Since the goal of our experiments is to test the reasoning capability of language models, all problems are formulated in pure sequence modeling, with no external program (e.g., calculator) called by the models. 6 Arithmetic Reasoning. We test four basic arithmetic operations, i.e., addition, subtraction, mul- tiplication, and division, with two non-negative integers. The difficulty of arithmetic problems is represented by the maximum number of digits in an operand. For instance, in 6-digit multiplication, each operand can range from 0 to 999,999. When we sample a problem, we sample each operand from the log-uniform distribution. Compared to the uniform distribution where the samples are highly biased towards extremely large numbers, we get roughly the same ratio of samples for each number of digits. The sampling schemes for each operation are elaborated in Appendix C.1. Algorithmic Reasoning. We test four algorithmic tasks with distinct characteristics. These prob- lems are generally solved via dynamic programming (DP), and the length of intermediate steps can increase rapidly since the time complexity of DP algorithms ranges from O (cid:0) N 2 (cid:6) (LCS, LPS, and 0-1 Knapsack) to O (cid:0) N 3 (cid:6) (MCM). (1) Longest Common Subsequence (LCS): Given two random sequences of length N , the model finds the longest common subsequence and its length. The se- quences consist of 10 characters from 0 to 9, and the problem difficulty is defined to be the sequence length N . (2) Longest Palindromic Subsequence (LPS): Similar to LCS, given a random sequence of length N , the model finds the longest palindromic subsequence and its length. (3) 0-1 Knapsack : Given a list of N items with specific value and weight, the model finds the best combination of items that maximizes the total value under the weight limit of a knapsack. The problem difficulty is repre- sented by the number of items. (4) Matrix Chain Multiplication (MCM): The computational cost of multiplying N ( % 2 ) matrices varies greatly depending on the order of multiplication. MCM is the task of finding the best multiplication order that yields minimal computation cost. The difficulty is controlled by the number of matrices N .\n\n4.2 U NLEASHING GPT-3\u2019 S R EASONING C APABILITY THROUGH R ECURSION OF T HOUGHT\n\n Despite their remarkable language modeling capabilities, the state-of-the-art large language models, including GPT-3 (Brown et al., 2020), struggle to solve even the basic arithmetic tasks (Nye et al., 2021). For instance, it cannot correctly handle multiplication with more than one or two digits. Using the OpenAI API, we fine-tune GPT-3 on the reasoning tasks in \u00a74.1 for 10K steps with a batch size of 256. Each training batch is randomly sampled from the training data distribution explained in \u00a73.3. The results are presented in Figure 3a, and the technical details are described in Appendix E. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix C. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any language model without the help of external programs. For reference, Minerva (Lewkowycz et al., 2022) achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition. Even WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT finetunes the GPT-3 to achieve near perfect scores in every experiment. As presented in Appendix I, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.\n\n4.3 R ECURSION OF T HOUGHT WITH T INY L ANGUAGE M ODELS\n\n Recent research on reasoning has been mostly focused on extremely large pre-trained language mod- els. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. Since RoT is model-agnostic, we test the two basic sequence model architectures: Transformer Vaswani et al. (2017) and LSTM Hochreiter & Schmidhuber (1997). For Transformer, we use a decoder-only model with 4 layers, 2 attention heads, 128 embedding dimensions, and 256 feed-forward dimensions, a total of only 536K parameters. It is a million times smaller than the largest PaLM (Chowdhery et al., 2022) with 540B parameters. The context limit is set to 2048 following GPT-3 and PaLM. For LSTM, we use 4 layers, 64 input dimensions, and 256 hidden dimensions, which result in 272K parameters. We set the context limit of the LSTM to 512 since (i) it takes a lot of time for LSTMs to process the tokens sequentially, and (ii) they are not good at handling long-term dependency. 7 \u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000\u0003\u0000/\u0000H\u0000Q\u0000J\u0000W\u0000K \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000$ \u0000F\u0000F \u0000U \u0000D\u0000F \u0000\\ \u0000/\u0000&\u00006 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0017 \u0000\u0016\u0000\u0015 \u0000\u0017\u0000\u0013 \u0000\u0017\u0000\u001b \u0000\u0018\u0000\u0019 \u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000\u0003\u0000/\u0000H\u0000Q\u0000J\u0000W\u0000K \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000$ \u0000D\u0000F \u0000\\ \u0000/\u00003\u00006 \u0000\u0015 \u0000\u0017 \u0000\u0019 \u0000\u001b \u0000\u0014\u0000\u0013 \u0000\u0014\u0000\u0015 \u0000\u0006\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000,\u0000W\u0000H\u0000P\u0000V \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000$ \u0000F\u0000F \u0000D\u0000F \u0000\\ \u0000\u0013\u0000\u0010\u0000\u0014\u0000\u0003\u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N \u0000\u0015 \u0000\u0017 \u0000\u0019 \u0000\u001b \u0000\u0014\u0000\u0013 \u0000\u0014\u0000\u0015 \u0000\u0006\u0000\u0003\u0000R\u0000I\u0000\u0003\u00000\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000D\u0000F \u0000\u0013\u0000\u0011\u0000\u0018 By virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 3b and Figure 3c. We test both arithmetic and algorithmic reasoning problems with Transformer, and the arithmetic problems with LSTM. For each experiment, we train a ran- domly initialized model and evaluate it on a test set of 30K unique problems. With a batch size of 256, Transformers and LSTMs are trained for 500K steps and 800K steps, respectively. We repeat each experiment eight times and report the average and standard deviation of the accuracies. Ap- pendix K enumerates the exact values of Figure 3. With the tiny Transformer, we experiment to the extent where even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype. Throughout the experiments, we observe consistent patterns: \u2022 WT\u2019s accuracy drops most quickly as the problem difficulty increases. 8 \u2022 CoT achieves near perfect accuracy, but it can only be applied to simple problems due to the context limit. \u2022 RoT achieves near perfect accuracy and can be scaled up to extremely complex problems. Despite the small sizes, RoT makes the Transformers master all types of extremely complex prob- lems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.\n\n5 D ISCUSSION\n\n The results of the tiny Transformer suggest that we might have to rethink the capability of large language models. If RoT enables the tiny Transformer to easily master 32-digit multiplication or division, what would a million times bigger model, like PaLM, be capable of? In contrast to the currently ongoing arms race in language models, the number of parameters might not be the main bottleneck anymore to increase models\u2019 reasoning capability. We believe that our new paradigm of utilizing multiple contexts has the potential to make a huge leap in this line of research. The current limitation of RoT is the need for supervision to learn divide and conquer for each task. In order to apply RoT to a wider range of tasks, it may be crucial to reduce the expensive supervi- sion. As one possible approach, we may borrow the RL-based methodologies that are developed for reducing supervision of NPI (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019). Interestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16-digit multiplication. We believe this problem is rooted in more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.\n\n6 C ONCLUSION\n\n Despite the remarkable advances in language models, their reasoning capability has always been constrained by the maximum size of a single context. In this work, we introduce Recursion of Thought to solve this problem by utilizing multiple contexts. We prove its potential through exten- sive experiments, showing that it is possible to make language models solve problems that require hundreds of thousands of tokens. We believe the core idea of utilizing multiple contexts will play an essential role in future language models.\n\nR EFERENCES\n\n Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\n\nLanguage models are few-shot\n\n learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin ( . , , , URL https:// .neurips cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html . , , https // .net =BkbY4psgg . Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, 9\n\n,\n\n\n\n,\n\n 10 11\n\nA A S TEP BY S TEP I LLUSTRATION OF R O T I NFERENCE\n\n In this section, we provide a step by step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.\n\nStep 1\n\n The context is initialized with the question Q .\n\nQ\n\n\n\nStep 2\n\n The model generates the first subquestion 8 (cid:17) 1 . GO Step 3 Instead of immediately producing the answer, the model outputs the THINK token. GO THINK\n\nStep 4\n\n The THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last GO of X 1 , i.e., 8 (cid:17) 1 . Q sub , 1 A sub , 1 X GO GO THINK GO 12\n\nStep\n\n 5 Since the subproblem is a base case, the model outputs the answer 9 immediately. Q GO GO 8 1 = THINK A GO 8 1 9 STOP Step 6 The answer is returned and replaces the THINK token. GO 4 GO 9 STOP\n\nStep 7\n\n The model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer. Q Q A 1 X GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK\n\nStep\n\n 8 The THINK token creates a new context X 3 for solving 40 (cid:17) 35 . Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK Q X GO 4 0 + 3 5 = 13\n\nStep 9\n\n Since 40 (cid:17) 35 is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5. Then it outputs the THINK token to solve the subproblem. Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK Q Q sub , 1 A sub , 1 X 3 GO 4 0 + 3 5 = GO 0 5 = THINK Step 10 The new context X 4 is created to solve 0 (cid:17) 5 . Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 5 = GO = STOP Q A sub , GO = THINK Q Q , A , X 3 GO 4 = GO = THINK Q A X 4 GO = 5 STOP 14 The answer is returned to X 3 and replaces the THINK token. The model generates the next subproblem. 15 Step 13 X 5 created to solve the subproblem 4 (cid:17) 3 . Since this is a base case, the model produces the answer directly. Q Q sub , 1 A sub , 1 X GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK Q Q sub , 1 A sub , 1 Q sub , 2 A sub , 2 X 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = THINK Q A X 5 GO 4 + 3 = 7 STOP Step 14 The answer from X 5 replaces the THINK token in X 3 . Q Q sub , 1 A sub , 1 X GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK Q Q sub , 1 A sub , 1 Q sub , 2 A sub , 2 X 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 16 Step 15 Since all subproblems are solved in X 3 , the answer 75 is generated and returned to X 1 . Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = THINK Q Q sub , 1 A sub , 1 Q sub , 2 A sub , 2 X 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP A 7 5 STOP Step 16 The answer of X 3 replaces the THINK token in X 1 . Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 GO 4 0 + 3 5 = 7 5 STOP Step 17 Since the subproblems in X 1 are all solved, the model produces the final answer. Q Q sub , 1 A sub , 1 X 1 GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP Q sub , 2 A sub , 2 A GO 4 0 + 3 5 = 7 5 STOP 7 5 9 STOP\n\nB E XAMPLES OF C O T T RAINING D ATA\n\n If we solve the example of 408+351 in figure 1 with RoT, the following five contexts are produced. \u2022 X 1 : GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = 7 5 STOP 7 5 9 STOP \u2022 X 2 : GO 8 + 1 = 9 STOP \u2022 X 3 : GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 7 5 STOP \u2022 X 4 : GO 0 + 5 = 5 STOP 17 \u2022 X 5 : GO 4 + 3 = 7 STOP The CoT context of the same problem is: \u2022 X CoT : GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = GO 0 + 5 STOP GO 4 + 3 STOP 7 5 STOP 7 5 9 STOP In a slightly more complicated example of 34 (cid:18) 5, the RoT contexts are as follows: \u2022 X 1 : GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = THINK \u2022 X 2 : GO 4 * 5 = 2 0 STOP \u2022 X 3 : GO 3 * 5 = 1 5 STOP \u2022 X 4 : GO 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = 1 7 STOP 1 7 0 STOP \u2022 X 5 : GO 0 + 0 = 0 STOP \u2022 X 6 : GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP \u2022 X 7 : GO 5 + 2 = 7 STOP The corresponding CoT context is: \u2022 X CoT : GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP 1 7 0 STOP Notice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT, if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix I for more details on the distribution of context sizes.\n\nC P ROBLEM S PECIFICATIONS C.1 T HE A RITHMETIC P ROBLEMS\n\n For arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative in- tegers. For subtraction, we add a constraint that the first operand is not less than the second one, in order to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R , e.g., GO 7 (cid:21) 3 = 2 R 1 STOP . As briefly mentioned in \u00a74.1, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than 0.01%. Thus, we define a variation of the log- uniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits. The probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not the support of a log-uniform distribution, and samples are overly concen- trated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter \u03b4 . To sample an integer in range (cid:18) \u03b1, \u03b2 (cid:6) with offset \u03b4 , we first uniformly sample a real number r in range (cid:18) log (cid:0) \u03b1 (cid:17) \u03b4 (cid:6) , log (cid:0) \u03b2 (cid:17) \u03b4 (cid:6)(cid:24) . Then, r is transformed to (cid:30) exp (cid:0) r (cid:6) (cid:14) \u03b4 $ . We denote the extended log-uniform distribution U log (cid:0) \u03b1, \u03b2, \u03b4 (cid:6) . As \u03b4 gets larger, the samples are more dispersed to larger numbers. In the experiments, we set \u03b4 (cid:0) 3 . Additionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers a and b for the dividend and the divisor. In about half of the 18 (cid:21) 3898 777 (cid:17) 1355 89057 (cid:14) 6 21437 (cid:18) 12 241554 (cid:21) 1901 154874 (cid:17) 81059 296410 (cid:14) 9 15007 (cid:18) 15 116475 (cid:21) 12908 64936 (cid:17) 216852 45 (cid:14) 3 539860 (cid:18) 427 488317 (cid:21) 197443 3 (cid:17) 340939 78906 (cid:14) 3 3583 (cid:18) 9754 7519 (cid:21) 325 3 (cid:17) 984775 56560 (cid:14) 29960 13 (cid:18) 66 3560 (cid:21) 847611 50581 (cid:17) 1183 98 (cid:14) 6 266394 (cid:18) 185 9711 (cid:21) 1385 415 (cid:17) 943 16551 (cid:14) 920 3988 (cid:18) 12 44540 (cid:21) 103 110 (cid:17) 49 25606 (cid:14) 194 5514 (cid:18) 57 19721 (cid:21) 58 15 (cid:17) 17058 45 (cid:14) 37 5 (cid:18) 1712 59544 (cid:21) 24 36278 (cid:17) 100 129443 (cid:14) 70196 17 (cid:18) 430178 333057 (cid:21) 333057 6 (cid:17) 23516 221 (cid:14) 54 227 (cid:18) 127 25719 (cid:21) 5142 1462 (cid:17) 848 11010 (cid:14) 818 20888 (cid:18) 54 7544 (cid:21) 46 1002 (cid:17) 2773 47759 (cid:14) 67 96 (cid:18) cases, the dividend a would be less than the divisor b , so the quotients will be zero for cases. To ensure a diverse range of quotients, we sample the divisor b from U log (cid:0) 1 , 10 N , 3 (cid:6) , the quotient c from U log (cid:0) 0 , 10 N ' b, 3 (cid:6) , and the remainder r from U log (cid:0) 0 , b, 3 (cid:6) . The dividend is calculated from these values: a (cid:0) b (cid:18) c (cid:17) r . This way, we can sample division problems with a diverse range of quotients and remainders. Table 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sam- pling scheme can be observed from the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero. 19\n\nC.2 T HE A LGORITHMIC P ROBLEMS C.2.1 L ONGEST C OMMON S UBSEQUENCE (LCS)\n\n The question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by ; . Here is an example of length-4 LCS problem: \u2022 Q : GO 1 2 3 4 LCS 2 4 6 8 = \u2022 A : 2 4 ; 2 STOP For a length- N LCS problem, we sample two sequences of length N . Each character of the se- quences are randomly sampled from 0-9 with equal probability.\n\nC.2.2 L ONGEST P ALINDROMIC S UBSEQUENCE (LPS)\n\n The question of a length- N LPS problem starts with the LPS , followed by a sequence of length N . Similar to LCS, the answer contains the corresponding LPS and its length separated by ; . The following is an example of length-8 LPS problem: \u2022 Q : GO LPS 4 1 2 5 3 2 6 1 = \u2022 A : 1 2 3 2 1 ; 5 STOP The sequence of an LPS problem is sampled in the same way as done for the LCS problem.\n\n.2.3 0-1 K NAPSACK\n\n Each item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 1 2 & 3 4 represents an item with a value of 12 and a weight of 34. The question part of a 0-1 Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by , , the token @ , and the capacity of the knapsack. The answer part starts with a list of items to include, then $ , and finally the total value. The following is an example of a 3-item knapsack problem. \u2022 Q : GO KNAPSACK 5 & 1 2 , 2 5 & 1 5 , 1 9 & 1 8 @ 4 0 = \u2022 A : 2 5 & 1 5 , 1 9 & 1 8 $ 4 4 STOP In this example, given a knapsack of capacity 40, the last two are selected with the total value of 44. For a fixed number of items, we uniformly sample each item\u2019s value and weight from the integers of range [1, 99].\n\nC.2.4 M ATRIX C HAIN M ULTIPLICATION (MCM)\n\n The cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices A , B , and C , whose shapes are 4 (cid:18) 2 , 2 (cid:18) 8 , and 8 (cid:18) 3 respectively, the cost of computing (cid:0) AB (cid:6) C is 4 (cid:18) 2 (cid:18) 8 (cid:17) 4 (cid:18) 8 (cid:18) 3 (cid:0) 160 , while another order A (cid:0) BC (cid:6) costs only 2 (cid:18) 8 (cid:18) 3 (cid:17) 4 (cid:18) 2 (cid:18) 3 (cid:0) 72 . In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by ; . The example above is represented as the following sequences. \u2022 Q : GO MCM 4 (cid:18) 2 , 2 (cid:18) 8 , 8 (cid:18) 3 = \u2022 A : 4 (cid:18) 2 , ( 2 (cid:18) 8 , 8 (cid:18) 3 ) ; 7 2 STOP Given a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].\n\nC.2.5 S ORTING\n\n Although not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix J. The problem difficulty is defined by the maximum number of 20 terms. For a sorting problem of at most N terms, we first uniformly sample the number of terms from (cid:18) 2 , N (cid:24) . Then we sample each term from U log (cid:0) 0 , 1000 , 5 (cid:6) . The following is an example of the sorting problem. \u2022 Q : GO SORT 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 , 4 1 = \u2022 A : 4 1 , 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 STOP\n\nD D ETAILS OF THE R ECURSIVE R EASONING P ROCEDURES\n\n In this section, we elaborate the procedures to recursively solve the arithmetic problems. Specifi- cally, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each sub- problem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group these information: For instance, T(Mul, (3, 4)) represents a regular subproblem of 3 (cid:18) 4 , and T(Add, (12, 340), \u2019tail\u2019) represents a subproblem of 12 (cid:17) 340 which should be performed as a tail re- cursion. Once the thought method returns a list of T s, we can recursively find more subproblems for each subproblem.\n\nD.1 A DDITION\n\n The core idea of our recursive procedure for addition is to first add the last digits, and then add the rest. If the sum of the last digits is greater than or equal to 10, we insert another subproblem for adding the carry right after adding the last digits. 1 class Add(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 left, right = args 5 6 # Base cases 7 if left < 10 and right < 10: 8 return [] 9 10 l_last, r_last = left % 10, right % 10 11 thoughts = [T(Add, (l_last, r_last))] 12 13 l_rest, r_rest = left // 10, right // 10 14 if l_last + r_last >= 10 : 15 thoughts.append(T(Add, (l_rest, 1))) 16 l_rest += 1 17 18 if l_rest > 0 and r_rest > 0: 19 thoughts.append(T(Add, (l_rest, r_rest))) 20 21 return thoughts Figure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry. \u2022 X 1 : GO 3 1 7 + 6 5 = GO 7 + 5 = 1 2 STOP GO 3 1 + 1 = 3 2 STOP GO 3 2 + 6 = 3 8 STOP 3 8 2 STOP 21 \u2022 X 2 : GO 7 + 5 = 1 2 STOP \u2022 X 3 : GO 3 1 + 1 = GO 1 + 1 = 2 STOP 3 2 STOP \u2022 X 4 : GO 1 + 1 = 2 STOP \u2022 X 5 : GO 3 2 + 6 = GO 2 + 6 = 8 STOP 3 8 STOP \u2022 X 6 : GO 2 + 6 = 8 STOP\n\nD.2 S UBTRACTION\n\n Similar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits x and y , we always borrow 10 for x to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before x . Therefore, the base cases of subtraction are when a & 19 and b & 9 . If the subtraction result of the last digits is smaller than 10, i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand m . 1 class Sub( Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 left, right = args 5 6 # Base cases 7 if left <= 19 and right <= 9: 8 return [ ] 9 10 l_last = left % 10 + 10 11 r_last = right % 10 12 thoughts = [T(Sub, (l_last, r_last)) ] 13 l_rest, r_rest = left // 10, right // 10 14 if l_last - r_last < 10: 15 thoughts .append(T(Sub, (l_rest, 1))) 16 l_rest -= 1 17 if r_rest > 0 : 18 thoughts.append(T(Sub, (l_rest, r_rest))) 19 20 return thoughts Here is an example of 432-216: \u2022 X 1 : GO 4 3 2 - 2 1 6 = GO 1 2 - 6 = 6 STOP GO 4 3 - 1 = 4 2 STOP GO 4 2 - 2 1 = 2 1 STOP 2 1 6 STOP \u2022 X 2 : GO 1 2 - 6 = 6 STOP \u2022 X 3 : GO 4 3 - 1 = GO 1 3 - 1 = 1 2 STOP 4 2 STOP \u2022 X 4 : GO 1 3 - 1 = 1 2 STOP \u2022 X 5 : GO 4 2 - 2 1 = GO 1 2 - 1 = 1 1 STOP GO 4 - 2 = 2 STOP 2 1 STOP \u2022 X 6 : GO 1 2 - 1 = 1 1 STOP \u2022 X 7 : GO 4 - 2 = 2 STOP Notice that the final answer and the questions of each subproblem can be easily constructed from previous sequence.\n\nD.3 M ULTIPLICATION\n\n The base cases of multiplication are (i) when either operands are 0 or 1, or (ii) when both operands are less than 10. If one of the operands is 0, then the answer is zero; when one of them is 1, then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum. 22 There are two types of non-base cases. For the simpler case, where the second operand is less than 10, we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed. 1 class Mul(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 left, right = args 5 6 # Base cases 7 if left = right <= 1: 8 return [] 9 if left 9 9: 10 return [] 11 12 thoughts ] 13 right : 14 thoughts ( right))) 15 thoughts right)) 16 17 = left ) 18 a2 = (left // 10 19 thoughts ( (Add, ) tail')) 20 else: 21 a1 left * right ) 22 thoughts ( left right ))) 23 24 a2 * right // 10) 25 thoughts. ( (Mul, left right // 10))) 26 27 thoughts. (T(Add, 10 a1) tail')) 28 return thoughts Here are some example contexts of multiplication: \u2022 X 1 : GO 4 3 * 2 1 = GO 4 3 * 1 = 4 3 STOP GO 4 3 * 2 = 8 6 STOP TAIL 8 6 0 + 4 3 = THINK \u2022 X 2 : GO 4 3 * 1 = 4 3 STOP \u2022 X 3 : GO 4 3 * 2 = GO 3 * 2 = 6 STOP GO 4 * 2 = 8 STOP TAIL 8 0 + 6 = THINK \u2022 X 4 : GO 3 * 2 = 6 STOP \u2022 X 5 : GO 4 * 2 = 8 STOP \u2022 X 6 : GO 8 0 + 6 = GO 0 + 6 = 6 STOP 8 6 STOP \u2022 X 7 : GO 0 + 6 = 6 STOP \u2022 X 8 : GO 8 6 0 + 4 3 = GO 0 + 3 = 3 STOP GO 8 6 + 4 = 9 0 STOP 9 0 3 STOP \u2022 X 9 : GO 0 + 3 = 3 STOP \u2022 X 10 : GO 8 6 + 4 = GO 6 + 4 = 1 0 STOP GO 8 + 1 = 9 STOP 9 0 STOP \u2022 X 11 : GO 6 + 4 = 1 0 STOP \u2022 X 12 : GO 8 + 1 = 9 STOP Notice that we use tail recursion in X 1 and X 3 . 23\n\nD.4 C OMPARISON\n\n Comparison is used as a subroutine during division. The procedure for comparison consists of three steps: 1. Compare the numbers of digits. 2. If the numbers of digits are the same, compare the most significant digits. 3. If the most significant digits are identical, compare the remaining digits recursively. We find that the sequence models can perform the first step without an explicit subproblem. There- fore, we only add intermediate steps for the second and the third steps. 1 class Compare(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 left, right = args 5 6 # Base cases 7 if left < 10 and right < 10: 8 return [] 9 10 thoughts = [] 11 digit_l, digit_r = len(str(left)), len(str(right)) 12 if digit_l == digit_r: 13 # Compare first digit 14 l_first, r_first = int(str(left)[0]), int(str(right)[0]) 15 thoughts.append(T(Compare, (l_first, r_first))) 16 if l_first == r_first: 17 # Compare the rest 18 l_rest = int(str(left)[1:]) 19 r_rest = int(str(right)[1:]) 20 thoughts.append(T(Compare, (l_rest, r_rest))) 21 22 return thoughts The following is an example of comparing 153 and 159. \u2022 X 1 : GO 1 5 3 VS 1 5 9 = GO 1 VS 1 = EQ STOP GO 5 3 VS 5 9 = LT STOP LT STOP \u2022 X 2 : GO 1 VS 1 = EQ STOP \u2022 X 3 : GO 5 3 VS 5 9 = GO 5 VS 5 = EQ STOP GO 3 VS 9 = LT STOP LT STOP \u2022 X 4 : GO 5 VS 5 = EQ STOP \u2022 X 5 : GO 3 VS 9 = LT STOP\n\nD.5 D IVISION\n\n Solving division is the most challenging among the four basic arithmetic operations since the pro- cedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division. The base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0, and the remainder is the dividend. If the dividend is equal to the divisor, than the quotient is 1, and the remainder is 0. Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem. If it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division. 24 To explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend a and the divisor b . First, we split the a into the last digit x and the remaining digits m . Then, we divide m with the divisor b , i.e., we are solving one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient q 1 (cid:0) m ' b and the remainder r 1 (cid:0) m mod b from the subproblem is added to the context. Next, we concatenate the remainder and x , which is numerically computing r (cid:18) 10 (cid:17) x , and divide it again with b . Let the quotient and the remainder of this operation q 2 and r 2 . Then, the quotient of the final answer is q 1 (cid:18) 10 (cid:17) q 2 , while the remainder is simply r 2 . class Div(Problem): 2 @staticmethod def thought(args) -> list[T]: 4 left, right = args 5 thoughts = [T(Compare, (left, right))] 6 7 # Base cases 8 if left <= right: 9 return thoughts 10 11 thoughts.append(T(Compare, (left, right * 10))) 12 if left <= right * 10: 13 diff = left - right 14 thoughts.append(T(Sub, (left, right))) 15 thoughts.append(T(Div, (diff, right))) 16 else: 17 thoughts.append(T(Div, (left // 10, right))) 18 left_remainder = (left // 10) % right * 10 + left % 10 19 thoughts.append(T(Div, (left_remainder, right))) 20 return thoughts The following is an example of 76 (cid:21) 29 . \u2022 X 1 : GO 7 6 \u00f7 2 9 = GO 7 6 VS 2 9 = GT STOP GO 7 6 VS 2 9 0 = LT STOP GO 7 6 - 2 9 = 4 7 STOP GO 4 7 \u00f7 2 9 = 1 R 1 8 STOP 2 R 1 8 STOP \u2022 X 2 : GO 7 6 VS 2 9 = GO 7 VS 2 = GT STOP GT STOP \u2022 X 3 : GO 7 VS 2 = GT STOP \u2022 X 4 : GO 7 6 VS 2 9 0 = LT STOP \u2022 X 5 : GO 7 6 - 2 9 = GO 1 6 - 9 = 7 STOP GO 7 - 1 = 6 STOP GO 6 - 2 = 4 STOP 4 7 STOP \u2022 ... \u2022 X 9 : GO 4 7 \u00f7 2 9 = GO 4 7 VS 2 9 = GT STOP GO 4 7 VS 2 9 0 = LT STOP GO 4 7 - 2 9 = 1 8 STOP GO 1 8 \u00f7 2 9 = 0 R 1 8 STOP 1 R 1 8 STOP \u2022 X 10 : GO 4 7 VS 2 9 = GO 4 VS 2 = GT STOP GT STOP \u2022 X 11 : GO 4 VS 2 = GT STOP \u2022 X 12 : GO 4 7 VS 2 9 0 = LT STOP \u2022 X 13 : GO 4 7 - 2 9 = GO 1 7 - 9 = 8 STOP GO 4 - 1 = 3 STOP GO 3 - 2 = 1 STOP 1 8 STOP \u2022 ... \u2022 X 17 : GO 1 8 \u00f7 2 9 = GO 1 8 VS 2 9 = LT STOP 0 R 1 8 STOP \u2022 X 18 : GO 1 8 VS 2 9 = GO 1 VS 2 = LT STOP LT STOP \u2022 ... 25\n\nD.6 L ONGEST C OMMON S UBSEQUENCE (LCS)\n\n Given sequences A and B , the algorithm starts by comparing the last characters of the two se- quences. If the last two characters are the same, we find LCS of the subsequences without the last characters, i.e., LCS of A (cid:2)(cid:14) 1 and B (cid:2)(cid:14) 1 . Otherwise, we compute the LCSs of the cases where the last character of either side is removed, and return the better one. In the following code, LCS._answer is the subroutine that finds the LCS of two sequences. Equal returns TRUE if the two arguments are the same, or FALSE otherwise. 1 class LCS(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 l, r = args 5 if len(l) == 0 or len(r) == 0: 6 return [] 7 8 thoughts = [T( Equal , (l[ - 1], r[ - 1]))] 9 if l [ - 1] = = r[ - 1]: 10 thoughts.append(T(LCS, (l[: - 1], r[: - 1]))) 11 return thoughts 12 13 lcs1_args = (l[: - 1], r) 14 lcs2_args = (l, r[:-1]) 15 lcs1 = LCS._answer(lcs1_args) 16 lcs2 = LCS._answer(lcs2_args) 17 thoughts.extend([ 18 T(LCS, lcs1_args), 19 T(LCS, lcs2_args), 20 T(Compare, (len(lcs1), len(lcs2))) 21 ]) 22 return thoughts The following is an example of finding the LCS of 123 and 234 . \u2022 X 1 : GO 1 2 3 LCS 2 3 4 = GO EQUAL 3 , 4 = FALSE STOP GO 1 2 LCS 2 3 4 = 2 ; 1 STOP GO 1 2 3 LCS 2 3 = 2 3 ; 2 STOP GO 1 VS 2 = LT STOP 2 3 ; 2 STOP \u2022 X 2 : GO EQUAL 3 , 4 = FALSE STOP \u2022 X 3 : GO 1 2 LCS 2 3 4 = GO EQUAL 2 , 4 = FALSE STOP GO 1 LCS 2 3 4 = ; 0 STOP GO 1 2 LCS 2 3 = 2 ; 1 STOP GO 0 VS 1 = LT STOP 2 ; 1 STOP \u2022 ... \u2022 X 21 : GO 1 2 3 LCS 2 3 = GO EQUAL 3 , 3 = TRUE STOP GO 1 2 LCS 2 = 2 ; 1 STOP 2 3 ; 2 STOP \u2022 ... \u2022 X 23 : GO 1 VS 2 = LT STOP\n\nD.7 L ONGEST P ALINDROMIC S UBSEQUENCE (LPS)\n\n The overall algorithm for LPS is similar to LCS. The base cases are when the sequence length is less then 3. If it is not a base case, we first check if the characters at both ends of the sequence are the same. If they are the same, we find the LPS of the subsequence excluding them. Otherwise, we compare the cases where one of the end characters are excluded. 1 class LPS(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 # Base cases 5 if len(args) == 1: 6 return [] 26 7 elif len(args) == 2: 8 return [T(Equal, args)] 9 10 thoughts = [T(Equal, (args[0], args[1]))] 11 if args[0] == args[-1]: 12 sub_lps = LPS._answer(args[1:-1]) 13 thoughts.extend([ 14 T(LPS, args[1:-1]), 15 T(Add, (len(sub_lps), 2)) 16 ]) 17 else: 18 lps1_args = args[:-1] 19 lps2_args = args[1:] 20 lps1 = LPS._answer(lps1_args) 21 lps2 = LPS._answer(lps2_args) 22 thoughts.extend([ 23 T(LPS, lps1_args), 24 T(LPS, lps2_args), 25 T(Compare, (len(lps1), len(lps2))) 26 ]) 27 return thoughts The following is an example of LPS. \u2022 X 1 : GO LPS 1 2 3 2 = GO EQUAL 1 , 2 = FALSE STOP GO LPS 1 2 3 = 1 ; 1 STOP GO LPS 2 3 2 = 2 3 2 ; 3 STOP GO 1 VS 3 = LT STOP 2 3 2 ; 3 STOP \u2022 X 2 : GO EQUAL 1 , 2 = FALSE STOP \u2022 X 3 : GO LPS 1 2 3 = GO EQUAL 1 , 3 = FALSE STOP GO LPS 1 2 = 1 ; 1 STOP GO LPS 2 3 = 2 ; 1 STOP GO 1 VS 1 = EQ STOP 1 ; 1 STOP \u2022 ... \u2022 X 10 : GO LPS 2 3 2 = GO EQUAL 2 , 2 = TRUE STOP GO LPS 3 = 3 ; 1 STOP GO 1 + 2 = 3 STOP 2 3 2 ; 3 STOP \u2022 ... \u2022 X 14 : GO 1 VS 3 = LT STOP D.\n\n8 0-1 K NAPSACK\n\n The base cases are when there is only one item. In this case, we simply compare the item\u2019s weight and the knapsack\u2019s capacity, to determine whether the item should be included. If it is a non-base case, we compare two possibilities: (i) include the first item, or (ii) exclude the first item. We recursively compute the subproblems and find the case with the best value. 17 27 18 # When including the current item 19 if weight <= capacity: 20 items_sub, value_sub = Knapsack._answer( 21 (items[1:], capacity - weight)) 22 value_incl = value_sub + value 23 thoughts.extend([ 24 T(Sub, (capacity, weight)), 25 T(Knapsack, (items[1:], capacity - weight)), 26 T(Add, (value_sub, value)), 27 T(Compare, (value_incl, value_max)), 28 ]) 29 30 return thoughts The following is an example of 0-1 knapsack problem with three items and a knapsack capacity of 10. \u2022 X 1 : GO KNAPSACK 3 & 9 , 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = 4 & 2 , 9 & 5 $ 1 3 STOP GO 9 VS 1 0 = LT STOP GO 1 0 - 9 = 1 STOP GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = $ 0 STOP GO 0 + 3 = 3 STOP GO 3 VS 1 3 = LT STOP 4 & 2 , 9 & 5 $ 1 3 STOP \u2022 X 2 : GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 9 & 5 @ 1 0 = 9 & 5 $ 9 STOP GO 2 VS 1 0 = LT STOP GO 1 0 - 2 = 8 STOP GO KNAPSACK 9 & 5 @ 8 = 9 & 5 $ 9 STOP GO 9 + 4 = 1 3 STOP GO 1 3 VS 9 = GT STOP 4 & 2 , 9 & 5 $ 1 3 STOP \u2022 ... \u2022 X 11 : GO 9 VS 1 0 = LT STOP \u2022 X 12 : GO 1 0 - 9 = 1 STOP \u2022 X 13 : GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = GO KNAPSACK 9 & 5 @ 1 = $ 0 STOP GO 2 VS 1 = GT STOP $ 0 STOP \u2022 ... \u2022 X 17 : GO 0 + 3 = 3 STOP \u2022 X 18 : GO 3 VS 1 3 = LT STOP D.\n\n9 T ERNARY A DDITION AND M ULTIPLICATION\n\n Ternary addition and multiplication arises as a subproblem while solving MCM, which will be ex- plained in the next section. They are simple extensions of addition and multiplication to three integers. 1 class TernaryAdd(Problem): 2 @staticmethod 3 def thought(args) -> list[T] : 4 a1, a2, a3 = args 5 return [ 6 T(Add, (a1, a2)), 7 T(Add, (a1 + a2, a3), 'tail') 8 ] 9 10 11 class TernaryMul(Problem) : 12 @staticmethod 13 def thought (args) -> list[T ] : 14 a1, a2, a3 = args 15 return [ 16 T( Mul , ( a1 , a2 )), 17 T(Mul, (a1 * a2, a3), ' tail' ) 18 ] 28\n\nD.10 M ATRIX C HAIN M ULTIPLICATION (MCM)\n\n Given N matrices, the N (cid:14) 1 subproblems are defined for each possible binary split. For the mul- tiplication of four matrices ABCD , there are three possible binary splits: A (cid:0) BCD (cid:6) , (cid:0) AB (cid:6)(cid:0) CD (cid:6) , and (cid:0) ABC (cid:6) D . For each binary split, the total cost is the sum of (i) the minimum cost of computing the first group, (ii) the minimum cost of computing the second group, and (iii) the cost of multiply- ing the two matrices resulting from each group. Once we get the total costs of each binary split, we return choose the best split with the minimum cost. The following code implements this procedure. 1 class MCM(Problem): 2 @staticmethod 3 def thought(args) -> list[T]: 4 mats, min_order, min_cost = args 5 6 # Base cases 7 if len(mats) == 1: 8 return [] 9 10 if min_order is None: 11 # Top-level problem 12 l_mats, r_mats = mats[:1], mats[1:] 13 else: 14 # Middle of recursion 15 l_mats, r_mats = mats 16 17 l_args = (l_mats, None, None) 18 r_args = (r_mats, None, None) 19 l_order, l_cost = MCM._answer(l_args) 20 r_order, r_cost = MCM._answer(r_args) 21 agg_cost = l_mats[0][0] * r_mats[0][0] * r_mats[-1][1] 22 thoughts = [ 23 T(MCM, l_args), 24 T(MCM, r_args), 25 ( ] ] ] ] 26 ( 27 ] 28 32 33 34 35 36 37 38 42 The following is an example of three-matrix MCM. \u2022 X 1 : GO MCM 3 \u00d7 9 , 9 \u00d7 4 , 4 \u00d7 5 = GO MCM 3 \u00d7 9 = 3 \u00d7 9 ; 0 STOP GO MCM 9 \u00d7 4 , 4 \u00d7 5 = 9 \u00d7 4 , 4 \u00d7 5 ; 1 8 0 STOP GO 3 * 9 * 5 = 1 3 5 STOP GO 0 + 1 8 0 + 1 3 5 = 3 1 5 STOP TAIL MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7 5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = THINK \u2022 ... \u2022 X 32 : GO MCM 3 \u00d7 9 , 9 \u00d7 4 | 4 \u00d7 5 ACC 3 \u00d7 9 , ( 9 \u00d7 4 , 4 \u00d7 5 ) ; 3 1 5 = GO MCM 3 \u00d7 9 , 9 \u00d7 4 = 3 \u00d7 9 , 9 \u00d7 4 ; 1 0 8 STOP GO MCM 4 \u00d7 5 = 4 \u00d7 5 ; 0 STOP GO 3 * 4 * 5 = 6 0 STOP GO 1 0 8 + 0 + 6 0 = 1 6 8 29 STOP GO 1 6 8 VS 3 1 5 = LT STOP ( 3 \u00d7 9 , 9 \u00d7 4 ) , 4 \u00d7 5 ; 1 6 8 STOP \u2022 ...\n\nD.11 S ORTING\n\n Among several sorting algorithms, we choose merge sort for our experiments with CoT and RoT. Note that WT is not relevant to the sorting algorithm since it produces the answer directly. The merge sort algorithm is simple: (i) split the given sequence to two equally sized subsequences, (ii) sort each subsequence, and (iii) merge the two sorted sequences. Since the final merge operation is quite complicated, we define the merge as a problem type. 1 class Merge (Problem): 2 @ staticmethod 3 def thought(args) -> list[T]: 4 l, r = args 5 if len(l) == 0 or len(r) == 0: 6 return [] 7 8 thoughts = [T(Compare, (l[0], r[0]))] 9 if l[0] < r[0] and len(l) > 1: 10 thoughts.append(T(Merge, (l[1:], r))) 11 elif l[0] >= r[0] and len(r) > 1: 12 thoughts.append(T(Merge, (l, r[1:]))) 13 return thoughts 14 15 16 class MergeSort(Problem): 17 @ staticmethod 18 def thought(args) -> list[T]: 19 if len(args) < 2: 20 return [] 21 22 l_len = (len(args) + 1) // 2 23 l = args[:l_len] 24 r = args[l_len: ] 25 return [ 26 T(MergeSort, l), 27 T(MergeSort, r), 28 T(Merge, (tuple(sorted(l)), tuple(sorted(r))), 'tail') 29 ]\n\nE F INE -T UNING GPT-3 FOR R ECURSION OF T HOUGHT\n\n Using the OpenAI API, we fine-tune GPT-3 for Recursion of Thought. The goal is to learn 16-digit addition, 16-digit subtraction, 8-digit multiplication, and 8-digit division simultaneously. GPT-3\u2019s fine-tuning API takes a dataset where each example is a prompt-completion pair in plain text. It is converted to tokens by a special tokenizer for GPT, which we cannot control. This API is not directly compatible with RoT due to several reasons. \u2022 There is no special tokens such as GO , THINK , and STOP . \u2022 The input and target sequences have to be the same. However, they are different in RoT due to the THINK token. Once THINK is produced, the RoT framework triggers the recursion process to find the subproblem\u2019s answer and replace the THINK token with it. Therefore, the THINK token appears in the target sequences, but never in the input sequences. Moreover, the way that GPT-3 tokenizes numbers hinders the learning of arithmetic reasoning rules. GPT-3 tokenizes a multi-digit number into a set of two-digit or three-digit numbers. For example, the text 1234567 is converted to the sequence of tokens 123 45 67 . Under this tokenization scheme, the relationship between the numbers become obscured. As an example, the tokens 7 , 17 , 30 27 , ..., 997 all have 7 as their last digit. Since there is no direct way for a model to know that they share the same digit, it is crucial to use each digit as a token. We believe that OpenAI needs to correct this tokenization of GPT-3 for numbers. Luckily, we can mimic the RoT procedures with the API by using several tricks. First, we replace the special tokens with plain lower-case words, e.g., GO (cid:0) go and STOP (cid:0) stop , which are included in the vocabulary of GPT-3. Second, we add a space before each token to make sure that the GPT tokenizer separates each token. We also add space before each digit to prevent the tokenizer grouping a number into 2-to-3-digit tokens. Finally, to simulate the behavior of the THINK and STOP tokens, we derive multiple examples from each context, one for each THINK or STOP output. As an example, context X 3 in Figure 1 is converted to the following JSON lines for GPT-3 as follows: In the case of Without Thought (WT), each problem is simply converted into a single example: In both cases of RoT and WT, we fine-tune GPT-3 for 10K steps with a batch size of 256. Among the several variants of GPT-3, we use Ada which is offered at the lowest cost. Note that RoT produces multiple contexts for each problem, and each RoT context is converted to multiple training examples. For this reason, the GPT-3 fine-tuned for RoT encounters much fewer problems during training, although the number of training steps are the same.\n\nF T RAINING D ETAILS OF THE T INY M ODELS\n\n In all experiments, we use a batch size of 256 and Adam optimizer Kingma & Ba (2015) with a learning rate of 0.001, i.e., the default learning rate in PyTorch. We train the Transformers for 500K steps and and decay the learning rate by half every 50K steps. Since the LSTMs converge slower than the Transformers, we train them for 800K steps and decay the learning rate by half every 100K steps. At every 20K steps, we evaluate the model on a test set of 30K problems, and if a model reaches a perfect accuracy of 1.0, we do not train the model further. The models can be trained on a single GPU with 12GB memory.\n\nG E FFICIENT E VALUATION OF R O T\n\n At the problem scales that RoT is tested, solving a single problem can require hundreds of thousands of tokens. Therefore, we have to develop an efficient evaluation algorithm. Since the models are deterministic, we do not need to evaluate the same context multiple times. Therefore, given a test 31 set, we first find all unique contexts that should appear while solving the test set. Note that there would be a lot of duplicate contexts, especially for the base cases. These unique contexts can be evaluated in parallel. We evaluate each context in a similar manner to the training. For each context, we construct the target sequence, as we described in \u00a73.2, and check if the model outputs the target sequence correctly, given the GT context. In case of GPT-3, we convert the contexts and target sequences as described in Appendix E to feed into the API. Once every context is individually evaluated, the next step is to aggregate the results. In this aggregation stage, a (sub)problem is marked correct only if its subproblems are all marked correct. This way, we efficiently evaluate RoT with minimal cost.\n\nH F AILURE C ASE A NALYSIS OF GPT-3 WITH R O T\n\n In this section we provide some failure cases of RoT in the GPT-3 experiments for further insights. Although the model achieves an accuracy of over 99% in almost every case, there are several prob- lems where it makes mistakes. Nonetheless, only a few characters are wrong in each context. The most common type of error is repeating the same character for a wrong number of times. The following is the examples of this error type. Prompt = = Completion = think Model Output = think Prompt = Completion Model Output Prompt Completion Model Output Another common case is incorrectly counting the number of digits while comparing two numbers. When comparing numbers following our recursive procedure, the model has to immediately return the answer if the number of digits are different. In the following example, the first number has seven digits, and the second one has six digits. Prompt go = Completion gt stop Model Output go 1 vs 8 Although the model should directly output the answer, i.e., gt stop , it decides that the number of digits are the same and moves on to the next step, which is to compare the most significant digits. This error is caused by the specific recursive procedure of our choice. Since we assume that the model would easily figure out the difference in the numbers of digits, we did not add explicit steps to count the digits. Although the model detects the difference in most cases, it turns out to be a relatively challenging operation. A possible solution is to teach the model to explicitly count the number of digits and compare them. 32\n\nI C ONTEXT L ENGTH D ISTRIBUTION\n\n In Figure 4 we present the distributions of context lengths for each problem type. We compare the context lengths of RoT and CoT. For each configuration, we randomly sample 10K contexts from the training distribution and plot the histogram of their lengths. The graphs show that the context sizes of CoT are many orders of magnitude larger than RoT. In theory, the total number of tokens to generate for each problem is identical in both RoT and CoT (if we do not count the THINK tokens). However, RoT\u2019s context sizes are much smaller since it utilizes multiple contexts. Another advantage of RoT is the utilization of dynamic programming. Since we can easily cache the duplicate computations of RoT as explained in Appendix G, we can drastically reduce the amount of token generation if there is a redundant structure in the problem. The amount of tokens to generate for each problem is plotted in Figure 5. The benefit is especially prominent in the algorithmic problems. For example, finding the LCS of two 32-digit sequences results in more than 10 18 tokens if we naively use CoT or RoT. If we use dynamic programming with RoT, we can efficiently solve the same problem with much less cost.\n\nJ T RANSFORMERS A RE P OWERFUL S ORTING M ACHINES\n\n In fact, the first algorithmic task that we tested is sorting since it has been widely used as a benchmark for algorithmic reasoning (Reed & de Freitas, 2016; Cai et al., 2017; Pierrot et al., 2019). However, we find that Transformers are incredibly good at sorting, even in the WT setting. Figure 6 shows the sorting experiment. For CoT and RoT, we train the merge sort algorithm. Interestingly, WT easily achieves a perfect score in sorting 64 three-digit numbers. Also the training converges much faster than RoT. The Transformer architecture, more specifically the attention mechanism, seems to be perfectly suited for the sorting operation.\n\nK T HE E XACT V ALUES OF F IGURE 3\n\n Table 2-5 show the exact values of the graphs in Figure 3. Except for the GPT-3 experiments in Table 2, we report the average and the standard deviation of eight runs. Each GPT-3 experiment is done only once. 33 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u001c\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014 \u0000\u0014\u0000H\u0000\u0014\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018 \u0000\u0014\u0000H\u0000\u0014\u0000\u001b \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0018\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015 \u0000\u0014\u0000H\u0000\u0014\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0011\u0000\u0018 \u0000\u0018\u0000\u0011\u0000\u0013 \u0000\u001a\u0000\u0011\u0000\u0018 \u0000\u0014\u0000H\u0000\u001a \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 34 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001c\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000$\u0000G\u0000G\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00006\u0000X\u0000E\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0017\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000X\u0000O\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000'\u0000L\u0000Y\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0010\u0000G\u0000L\u0000J\u0000L\u0000W\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0014 \u0000\u0014\u0000H\u0000\u0014\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0019\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018 \u0000\u0014\u0000H\u0000\u0014\u0000\u001b \u0000/\u0000&\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0014\u0000\u0019\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015 \u0000\u0014\u0000H\u0000\u0014\u0000\u0013 \u0000/\u00003\u00006\u0000\u0003\u0000\u0012\u0000\u0003\u0000O\u0000H\u0000Q\u0000J\u0000W\u0000K\u0000\u0003\u0000\u0016\u0000\u0015\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000.\u0000Q\u0000D\u0000S\u0000V\u0000D\u0000F\u0000N\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000L\u0000W\u0000H\u0000P\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0019\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013 \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u00005\u0000R\u00007 \u0000\u0015\u0000\u0011\u0000\u0018 \u0000\u0018\u0000\u0011\u0000\u0013 \u0000\u001a\u0000\u0011\u0000\u0018\u0000\u0014\u0000H\u0000\u001a \u00000\u0000&\u00000\u0000\u0003\u0000\u0012\u0000\u0003\u0000\u0014\u0000\u0015\u0000\u0003\u0000P\u0000D\u0000W\u0000U\u0000L\u0000F\u0000H\u0000V\u0000\u0003\u0000\u0012\u0000\u0003\u0000&\u0000R\u00007 RoT can utilize dynamic programming to reduce . 35 \u0000\u001b \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0017 \u0000\u0016\u0000\u0015 \u0000\u0017\u0000\u0013 \u0000\u0017\u0000\u001b \u0000\u0018\u0000\u0019 \u0000\u0019\u0000\u0017 \u0000\u0006\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000,\u0000W\u0000H\u0000P\u0000V \u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0014\u0000\u0011\u0000\u0013 \u0000$ \u0000F\u0000F \u0000U \u0000D\u0000F \u0000\\ \u00006\u0000R\u0000U\u0000W\u0000L\u0000Q\u0000J \u0000:\u0000L\u0000W\u0000K\u0000R\u0000X\u0000W\u0000\u0003\u00007\u0000K\u0000R\u0000X\u0000J\u0000K\u0000W \u0000&\u0000K\u0000D\u0000L\u0000Q\u0000\u0003\u0000R\u0000I\u0000\u0003\u00007\u0000K\u0000R\u0000X\u0000J\u0000K\u0000W \u00005\u0000H\u0000F\u0000X\u0000U\u0000V\u0000L\u0000R\u0000Q\u0000\u0003\u0000R\u0000I\u0000\u0003\u00007\u0000K\u0000R\u0000X\u0000J\u0000K\u0000W Problem Difficulty 36 37 38", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces Recursion of Thought (RoT), a model-agnostic framework that enables language models to solve complex multi-step reasoning problems by recursively dividing them into smaller subproblems and solving each subproblem in a separate context.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RoT grants language models the ability to control the recursion process by introducing special tokens that trigger the creation and utilization of multiple contexts.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "On arithmetic and algorithmic reasoning tasks, the authors demonstrate that RoT dramatically improves the reasoning capabilities of language models like GPT-3, allowing them to solve extremely complex problems requiring hundreds of thousands of tokens, which would exceed the maximum context size of a single context.", "topic_class_1": "Summary of the result", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Moreover, RoT enables even tiny randomly initialized Transformers or LSTMs to perform complex reasoning.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- RoT provides an effective and scalable approach to enable language models to perform multi-step reasoning on problems that would otherwise exceed the maximum context size.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The idea of recursively dividing problems and solving subproblems in separate contexts is well-motivated and aligns with how humans approach complex problems through abstraction and recursion.", "topic_class_1": "Opinion on the motivation", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper demonstrates the generality and effectiveness of RoT on a diverse set of arithmetic and algorithmic reasoning tasks.", "topic_class_1": "Opinion on the experiments", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- RoT leads to dramatic improvements in reasoning capabilities, not just for large pre-trained models like GPT-3, but also for small randomly initialized Transformers and LSTMs.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- RoT currently requires supervision in the form of ground truth intermediate reasoning steps for each task.", "topic_class_1": "Opinion on the method", "topic_class_2": "Weakness / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reducing this expensive supervision could make RoT more widely applicable.", "topic_class_1": "Opinion on the method", "topic_class_2": "Weakness / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper does not explore how RoT might facilitate length generalization, e.g. training on shorter problems and generalizing to longer ones.", "topic_class_1": "Opinion on the method", "topic_class_2": "Weakness / Experiments", "reliability": "No", "explanation": "This is not the focus of the paper and thus not a must-have experiment", "error_type": "Out-of-scope"}, {"segment_text": "- Some additional analysis on the limitations of RoT and failure cases could provide further insights.", "topic_class_1": "Opinion on the analysis", "topic_class_2": "Weakness / Analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is generally well-written and the key ideas of RoT are clearly explained.", "topic_class_1": "Opinion on writing", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The extensive experiments on multiple reasoning tasks demonstrate the effectiveness and versatility of the proposed approach.", "topic_class_1": "Opinion on the experiments", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The results convincingly show that RoT leads to significant improvements in reasoning capabilities.", "topic_class_1": "Opinion on the experiments", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Recursively combining language models with multiple contexts for reasoning is a novel paradigm that addresses a key limitation of existing approaches constrained by a single context length.", "topic_class_1": "Opinion on the idea", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors plan to release source code which would facilitate reproducibility.", "topic_class_1": "Opinion on code", "topic_class_2": "Strength / Reproducibility", "reliability": "No", "explanation": "The paper states that it provides code in the supplementary materials.", "error_type": "Neglect"}, {"segment_text": "Some of the experimental details could be elaborated further.", "topic_class_1": "Concerns on experiments", "topic_class_2": "Strength / Reproducibility", "reliability": "No", "explanation": "It is too general and does not specify which parts could be improved.", "error_type": "Vague Critique"}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a significant contribution by introducing Recursion of Thought (RoT), a novel framework that enables language models to perform complex multi-step reasoning by recursively dividing problems and utilizing multiple contexts.", "topic_class_1": "Summary of review", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RoT dramatically improves the reasoning capabilities of language models and allows them to solve problems that are infeasible with a single context.", "topic_class_1": "Summary of review", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments demonstrate the effectiveness of RoT on a range of arithmetic and algorithmic reasoning tasks.", "topic_class_1": "Summary of review", "topic_class_2": "Summary of Strength on Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a high-quality paper that proposes a novel and impactful approach to language model reasoning.", "topic_class_1": "Suggested paper decision\n", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The key limitation is the need for expensive supervision which could be a useful direction for future work.", "topic_class_1": "Opinion on the limitation", "topic_class_2": "Summary of Limitation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper introduces Recursion of Thought (RoT), a novel framework for enhancing the reasoning capabilities of language models (LMs) by enabling them to divide and conquer complex problems recursively.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By generating multiple contexts, RoT overcomes the context size limitation inherent in state-of-the-art LMs like GPT-3 and PaLM.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The framework is model-agnostic and has been shown to dramatically improve the performance of both large-scale pre-trained models and tiny, randomly initialized models on arithmetic and algorithmic reasoning tasks, achieving near-perfect accuracy on problems that require generating hundreds of thousands of tokens.", "topic_class_1": "Summary of the method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RoT addresses a critical limitation in LMs regarding context size and demonstrates a significant improvement in solving complex reasoning tasks.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The framework is model-agnostic, allowing its application across various models, including Transformers and LSTMs.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper provides extensive experiments and comparisons, showing that RoT can enable even small models to perform complex reasoning at a level that may be challenging even for humans.", "topic_class_1": "Opinion on the experiments", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach requires supervised learning for each task, potentially limiting its applicability to problems where such detailed supervision is feasible.", "topic_class_1": "Concerns on the method", "topic_class_2": "Weakness / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper does not explore the generalizability of models trained with RoT to tasks or problem sizes not seen during training.", "topic_class_1": "Concerns on the experiments", "topic_class_2": "Weakness / Method", "reliability": "No", "explanation": "Out-of-scope", "error_type": "Out-of-scope"}, {"segment_text": "Despite its potential, the approach's dependence on recursive problem-solving may not be suitable for all types of reasoning tasks.", "topic_class_1": "Concerns on the generalisability", "topic_class_2": "Weakness / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-structured, presenting a clear methodology and comprehensive experimental results.", "topic_class_1": "Summary of writing", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The quality of research is high, supported by rigorous experiments and a broad application of the proposed framework.", "topic_class_1": "Summary of quality", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty of the approach lies in addressing the context size limitation by leveraging recursive problem-solving, a significant step forward in the field.", "topic_class_1": "Opinion on novelty", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The inclusion of source code for reproducing experiments enhances the paper's reproducibility.", "topic_class_1": "Opinion on code", "topic_class_2": "Strength / Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a promising approach to overcoming the context size limitations of current language models.", "topic_class_1": "Summary of the review", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The Recursion of Thought framework not only demonstrates the potential for dramatically improved performance in complex reasoning tasks but also opens new avenues for research in natural language processing and artificial intelligence.", "topic_class_1": "Summary of the review", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While there are limitations, such as the need for task-specific supervision, the strengths and contributions of this work significantly outweigh its weaknesses.", "topic_class_1": "Suggested paper decision", "topic_class_2": "Summary of Weakness on Method ", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes Recursion of Thought (RoT), a novel framework for improving the reasoning capabilities of language models (LMs) by overcoming the limitations imposed by context size.", "topic_class_1": "Summary of the method", "topic_class_2": "summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RoT introduces special tokens that allow the model to recursively divide complex problems into smaller subproblems, solve them in separate contexts, and combine the results.", "topic_class_1": "Summary of the method", "topic_class_2": "summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This divide-and-conquer approach is inspired by human reasoning and aims to address the issue of Chain of Thought (CoT) becoming impractical for highly complex problems due to context limitations.", "topic_class_1": "Summary of the motivation", "topic_class_2": "summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper demonstrates RoT's effectiveness on various arithmetic and algorithmic reasoning tasks, showing significant performance improvements even for small, randomly initialized LMs.", "topic_class_1": "Summary of the experiments", "topic_class_2": "summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: RoT presents a novel approach to reasoning with LMs, addressing the critical issue of context limitations.", "topic_class_1": "Opinion on novelty", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Model-agnostic: RoT can be applied to any sequence model capable of autoregressive generation, making it widely applicable.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Method", "reliability": "No", "explanation": "Experiments are only conducted on GPT-3 and LSTM. The approach might be suitable for some other decoder-only LMs, such there are some exceptions might need further checking, e.g., T5, GPT4", "error_type": "Experiment"}, {"segment_text": "Divide-and-conquer: The recursive approach allows for efficient and scalable reasoning, enabling LMs to tackle problems requiring extensive intermediate steps.", "topic_class_1": "Opinion on the method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Performance: RoT demonstrates significant performance improvements on various tasks, achieving near-perfect accuracy even for small LMs.", "topic_class_1": "Opinion on the performance", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides the source code for full experiment reproduction.", "topic_class_1": "Opinion on code", "topic_class_2": "Strength /Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Supervision: RoT currently requires supervised training data with ground truth intermediate steps, which can be expensive to obtain.", "topic_class_1": "Concerns on the method", "topic_class_2": "Weakness / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Length generalization: RoT does not facilitate length generalization, meaning a model trained on a specific problem size might not generalize to larger sizes.", "topic_class_1": "Concerns on the method", "topic_class_2": "Weakness / Method", "reliability": "No", "explanation": "Partially correct. It is unclear whether a model trained on a specific problem size will generalize to larger sizes well or not.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity: Some aspects of the paper, particularly the technical details of the recursive procedures, could benefit from improved clarity and elaboration. Providing more detailed explanations and examples of the recursive procedures would improve clarity.", "topic_class_1": "Concerns on writing", "topic_class_2": "Weakness / Paper Writing", "reliability": "No", "explanation": "Technical details are in Appendix, as mentioned in Section 3.3", "error_type": "Neglect"}, {"segment_text": "Exploring methods for reducing the reliance on supervised training data would be valuable for future work.", "topic_class_1": "Concerns on the method", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is generally well-written, but some sections describing the recursive procedures and training data distribution could be made clearer.", "topic_class_1": "Concnerns on writing", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-motivated, presents a significant contribution, and is supported by extensive experiments.", "topic_class_1": "Opinion on quality", "topic_class_2": "Strength / Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: RoT is a highly novel approach to reasoning with LMs, offering a unique solution to the context size limitation.", "topic_class_1": "Opinion on novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides the source code, allowing for full experiment reproduction and facilitating further research.", "topic_class_1": "Opinion on novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and impactful approach to reasoning with LMs.", "topic_class_1": "Summary of review", "topic_class_2": "Summary of Strength on Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RoT offers a promising solution to the context size limitation, enabling LMs to tackle complex problems that were previously intractable.", "topic_class_1": "Summary of review", "topic_class_2": "Summary of Weakness on Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper demonstrates significant performance improvements on various tasks, even for small models.", "topic_class_1": "Summary of the performance", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While some aspects could benefit from improved clarity and the current reliance on supervised training data is a limitation, the overall contribution is highly significant and warrants acceptance.", "topic_class_1": "suggested paper decision", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Thrust: Adaptively Propels Large Language Models with External Knowledge", "body_text": "\n\nT HRUST : A DAPTIVELY P ROPELS L ARGE L ANGUAGE M ODELS WITH E XTERNAL K NOWLEDGE\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Large-scale pre-trained language models (PTLM) have achieved great success in various natural language processing (NLP) tasks. Much evidence shows that PTLMs already encode rich knowledge themselves, but knowledge stored in PTLMs can be opaque and static, making external knowledge retrieval neces- sary. However, there are two major challenges when using external knowledge. First, knowledge indexing and retrieving on large-scale knowledge bases are time costly. Second, knowledge retrieved could be noisy and sometimes misleading. Motivated by the observation that external knowledge is not always required by PTLMs, we investigate an effective and ef\ufb01cient way to apply knowledge only when the knowledge is essential. Speci\ufb01cally, we propose instance-level adap- tive propulsion of external knowledge (IAPEK), where we score each instance on whether the PTLMs need the support of external knowledge. To achieve this goal, we design a novel metric, Thrust , which leverages the distribution estimation on seen/training instances. Extensive experiments demonstrate that we can achieve signi\ufb01cantly higher cost-ef\ufb01ciency through Thrust compared to the naive usage of external knowledge on 88% of the evaluated tasks with 26% average perfor- mance improvement. Such \ufb01ndings further shed light on the real-world practice of knowledge-enhanced LMs with a limited budget for knowledge seeking due to computation latency or costs 1 .\n\n1 I NTRODUCTION\n\n Knowledge plays an important role in solving natural language processing (NLP) tasks, where en- cyclopedic or commonsense knowledge is commonly required to answer questions from various tasks (Yin et al., 2022). In recent years, the emergent advance of pre-trained language models (PTLM) has demonstrated great improvement on various tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020). Evidence also show that PTLMs con- tain rich encyclopedic (Petroni et al., 2019) or commonsense (Kocijan et al., 2019) knowledge them- selves. However, such implicit knowledge embedded in the model\u2019s hidden states can be opaque, static, and inef\ufb01cient to utilize (Khattab et al., 2022). These issues motivate the common practice on seeking external knowledge (Xu et al., 2021; Verga et al., 2021; Paranjape et al., 2022) in NLP. A typical line of work focuses on retrieval-based methods, where knowledge is retrieved by a stand- alone retriever from external knowledge bases and then used to augment the inference models (i.e., Reader) such as PTLMs (Karpukhin et al., 2020; Gao & Callan, 2021; Khattab & Zaharia, 2020). However, there are several limitations with the usage of external knowledge: (i) performance on the downstream tasks is not commonly revealed. Metrics of the common benchmarks (e.g., MS- MARCO (Nguyen et al., 2016), BEIR (Thakur et al., 2021)) measure the quality of retrieval (e.g., Recall@50, nDCG@10). Although retrieving the relevant content may positively relate to the down- stream performance, not reporting the downstream performance, especially for the out-of-domain tasks, limits the exploration of how to utilize the external knowledge in practice; (ii) the external knowledge can be noisy or unnecessary. On the retriever side, though concurrent retrievers achieve great performance on various tasks, the noise can still exist. For instance, ColBERT v2 (San- thanam et al., 2022) achieved 68.9 Success@5 on Natural Question (Kwiatkowski et al., 2019), which suggests that gold documents do not appear in the top 5 retrieved documents for 31.1% of 1 The code and data will be released after acceptance. 1 the queries. Considering the limited input token length, the most useful documents may not be in- cluded during prediction. Not to mention that others can be noise to the model. On the model side, PTLMs with growing capacity, from millions (e.g., BERT (Devlin et al., 2019)) to billions of param- eters (e.g., OPT (Zhang et al., 2022b)), may solve the queries directly without external knowledge, which makes it unnecessary to seek external knowledge and signi\ufb01es the noise issue. The example shown in Figure 1 demonstrates the noise and inef\ufb01ciency issues. OPT (the version with 175 billion parameters) can directly give the correct answer without any external knowledge. However, with top external knowledge retrieved by DPR (Karpukhin et al., 2020) from Wikipedia paragraphs, the external knowledge can be useless or even lead to wrong predictions. Intuitively, a solution to the noise and inef\ufb01ciency issues is only seeking external knowledge when it is necessary. In this work, we capture this intuition by proposing I nstance-level A daptive P ropulsion of E xternal K nowledge (IAPEK) to reduce the effect of noise in external knowledge and improve the cost-ef\ufb01ciency of knowledge augmentation. In detail, for each instance of a given task, we com- pute a con\ufb01dence score measuring how likely it can be solved directly with respect to a given model and reject the use of external knowledge when the score is high. We design a simple and lightweight metric Thrust to serve such a purpose by leveraging the estimation of the instance distribution in the eyes of the target models. To comprehensively understand the effectiveness of Thrust , we \ufb01rst create a large-scale benchmark examining the downstream performance of the task-plus-knowledge paradigm with (i) tasks with different formats and types (e.g., multiple-choice classi\ufb01cation (MC classi\ufb01cation) and open-domain question answering (open-domain QA)); (ii) knowledge with dif- ferent formats and from different resources (e.g., knowledge graphs, Wikipedia paragraphs, and human annotations). Next, with models that can utilize external knowledge, we evaluate the ef- fectiveness of Thrust by showing that it can boost the performance of various tasks under various settings, such as injecting external knowledge to different portions of the test instances. Extensive experiments show that Thrust can improve the cost-ef\ufb01ciency of seeking and using exter- nal knowledge on 88% cases with 26% average performance improvement through identifying the instances that mostly require knowledge. We can also observe that, with Thrust , we can achieve higher performance than injecting external knowledge for all the instances, where models are bene- \ufb01ted from both the performance and ef\ufb01ciency aspects.\n\n2 O UR METHOD 2.1 I NSTANCE - LEVEL ADAPTIVE PROPULSION OF KNOWLEDGE\n\n We \ufb01rst de\ufb01ne IAPEK as follows: for each query q i in a given test set D = { q (1) , q (2) , . . . } , let f ( q ) denotes the scoring function of the necessity of external knowledge, we extract the corresponding scores S = { f ( q ) (1) , f ( q ) (2) , . . . } . With S , we re-rank the test set into D \u2032 = { q \u2032 (1) , q \u2032 (2) , . . . } . Given any threshold t \u2208 R , we sample a subset D k = { q (1) k , q (2) k , . . . } as that with highest knowledge need, where for each q k \u2208 D k , f ( q k ) > t . Empirically, we can set t as a particular percentile of S , e.g., top 25% of S . Next, for each instance in D k , we seek for external knowledge pieces and 2 In the controversial and no knowledge cases, the knowledge contained internally is less likely to answer the query successfully. In contrast, if the model \ufb01nds the query similar to various seen instances, the internal knowledge is likely to exist. augment each query q k to q k + . We combine updated D k + and original unsampled instances D \\ D k to the new knowledge augmented dataset D + to apply to inference models.\n\n2.2 T HRUST\n\n We then introduce a novel empirical method, Thrust , to perform the proposed instance-level adaptive propulsion of external knowledge (IAPEK) . We design Thrust to measure how likely the given query can be solved by the internal knowledge of the target model. Intuitively, there are two cases where models can fail to answer a query with internal knowledge: (i) the model has no relevant knowledge and is not familiar with the query semantics or inference types; (ii) the model faces controversial knowledge, where the query may have similar semantics with different kinds of seen questions that potentially require different reasoning to solve. As shown in Figure 2, from a cluster view of the instance distribution, we can use the distance be- tween the query representation and the instance cluster centers to measure if a query can be solved with internal knowledge. Motivated by that, the Thrust score of a given query can be computed as: (i) we \ufb01rst estimate the instance distribution in the view of the target model by casting a set of instances (in our experiments, we use the training data as de\ufb01ned in Section 3.2) into the representa- tion space 2 ; (ii) to improve the ef\ufb01ciency during the testing, we conduct K-means clustering on the instance vectors and extract a set of clusters C , where |C| is relative to the size of estimated instances; (iii) during test time, for the general cases, for each instance query q , we calculate vectorized thrust ( t v ) as follows: we \ufb01rst extract the unit vectors pointing from the query vector to the center of each cluster; then we calculate t v as the length of the sum vector of these directed vectors weighted by the size of the each cluster over the square of the Euclidean distance between the query vector and center vectors. For binary classi\ufb01cation, motivated by (Murty et al., 2021), we leverage the property of binary labels and design binary thrust ( t b ) as a variant, which is calculated directly as the absolute value of the sum of the weights (cluster size over distance) multiplied by the corresponding numer- ical labels (i.e., +1 or \u2212 1 ). In short, if we denote r as the distance between a query and a cluster c , c 0 as the center of cluster c , and l c as the label of cluster c , t v and t b can then be written as: For t v , there is no need for the labels of the estimating or testing examples. For t b , there is still no need for the labels of the test examples. Such characteristics suggest that the Thrust scores can be computed at any stage during learning and inference. On the other hand, since we use the static hidden states from the given models, there is no involvement of any additional \ufb01ne-tuning. We simply use the training examples from the split of the original work to estimate the query distribution 2 In our experiments, if not speci\ufb01ed, we use the last layer hidden states of the models to represent each query instance. For T5-based models, we use the last layers of the decoders . 3 for the purpose of avoiding any sort of data leak. The training set is not used for updating the weights during computing Thrust scores.\n\n3 E XPERIMENT\n\n In this section, we \ufb01rst introduce the experiment setups. Next, considering that a preliminary condi- tion of applying Thrust is that the model can get bene\ufb01t from external knowledge, we \ufb01rst investigate the effectiveness of external knowledge on various models with the collected benchmark. Upon that, we evaluate the effectiveness of Thrust from the perspective of cost-ef\ufb01ciency as a simulation of the real-world usage, where the knowledge can be costly to acquire knowledge, i.e., with a speci\ufb01c portion of instances to be augmented with external knowledge, how much the proposed method can improve the performance.\n\n3.1 S ETUP\n\n We evaluate T5 (Raffel et al., 2020), GPT-J (Wang & Komatsuzaki, 2021), OPT (Zhang et al., 2022b), and Uni\ufb01edQA (Khashabi et al., 2020). We test both the zero-shot learning setting (models take the prompt with or without knowledge directly) and the transfer-learning setting (models are \ufb01ne-tuned with examples containing external knowledge) to examine the proper way to use external knowledge. For the zero-shot setting, we test on T5, GPT-J, and OPT (30 billion parameter version). For the transfer-learning setting, we test on Uni\ufb01edQA with different scales. After that, we evaluate the effectiveness of Thrust , where we try to simulate the real-world case where we have limited bandwidth or budget to retrieve external knowledge. Speci\ufb01cally, we are only allowed to conduct the knowledge retrieval for 25%, 50%, and 75% of instances. As intro- duced in Section 2.1, we use Thrust to rank the instances by their need for external knowledge and select the instances from the most necessary ones. We compare the performance difference between Thrust and random sampling. For MC classi\ufb01cation tasks, we follow previous works to use accuracy as the evaluation metric. For open-domain QA, we report F1 scores of the models under different settings. The F1 score for question answering measures the max uni-gram overlap between the model prediction and all gold answer candidates. Denoting the gold answer set as G and uni-gram tokens of each query and each corresponding gold answer as T q and T g , the F1 score can then be written as F 1 = max g \u2208G ( T q \u2229 T g ) / ( T q \u222aT g ) . We test on two settings: without knowledge and with knowledge . For the former one, we directly pass the prompt-decorated queries to the model to retrieve the choices with the highest probability or the answer to the questions. For the latter one, we add the prompt-decorated queries, knowledge, and Answer: into three lines and pass them all to the models.\n\n3.2 D ATASETS\n\n We \ufb01rst prepare a benchmark containing knowledge-intensive tasks from two main types: MC clas- si\ufb01cation and open-domain question answering, with seven and \ufb01ve tasks, respectively. We unify the instances into the same format, where each contains: (i) a query: a piece of text containing a question or the sentences to be classi\ufb01ed; (ii) an answer: either the label words or the answers to the questions in the query; (iii) knowledge: potentially helpful knowledge for the query, which is either inherently relevant due to the task design, annotated by humans, or retrieved from Wikipedia paragraphs with DPR. Details of the selected datasets are as follows: Multiple-choice classi\ufb01cation. For MC classi\ufb01cation, each query q includes a sentence or a ques- tion and requires models to extract one correct answer from a set of candidate answers (e.g., yes or no ). The selected tasks are (i) AGNews (Zhang et al., 2015): AGNews is a classic text clas- si\ufb01cation task in NLP, where models are required to classify if a piece of news belongs to which among political, sports, business, or technology categories. We regard the titles of the news as the queries since they may already contain suf\ufb01cient information for classi\ufb01cation (e.g., Olympic his- tory for India belongs to Sports ); Then, the content of the news is considered as the gold external knowledge extracted from the task design; (ii) e-SNLI (Camburu et al., 2018): e-SNLI is a natural language inference (NLI) task exploiting the role of explanations for the task of textual entailment. We concatenate the original two sentences with a blank and add a question about if entailment exists 4 to form the query. Naturally, the human-providing explanations are considered a strong source of external knowledge; (iii) StrategyQA (Geva et al., 2021): StrategyQA is a challenging multi-hop reasoning dataset that requires models to answer creative questions (e.g., Did Aristotle use a lap- top? ) through strategical inference from implicit reasoning steps. We regard the original creative questions as queries and human-written explicit facts (e.g., Aristotle was born in ... ) as external knowledge, which is again powerful and expected to contain little noise; (iv) CIKQA (Zhang et al., 2022a): CIKQA is a commonsense inference task in the format of multiple-choice question an- swering, combining the tasks of pronoun coreference resolution, commonsense QA (Talmor et al., 2019), COPA (Roemmele et al., 2011), and questions mined from ATOMIC knowledge graph (Sap et al., 2018). We regard the original questions as queries and the supporting commonsense knowl- edge extracted from knowledge graphs (KGs) in the original work as the external knowledge; (v) BoolQ (Clark et al., 2019): BoolQ is a question that contains encyclopedic questions that require models to answer yes or no . Following (Khashabi et al., 2020), we use the Wikipedia paragraphs retrieved by DPR as the external knowledge, which can be potentially noisy; (vi) ARC-E & ARC- C (Clark et al., 2018): ARC is a challenging multiple-choice question answering dataset that re- quires powerful knowledge understanding and reasoning, which is partitioned to an Easy set and a Challenge set (denoted as ARC-E & ARC-C, respectively), where the Challenge set questions are answered incorrectly by the retrieval-based or co-occurrence-based algorithms tested by the original authors. Similarly, we use the Wikipedia paragraphs retrieved by DPR as external knowledge. Open-domain QA. For open-domain QA, each query q contains an open question that typically requires solving an encyclopedic or commonsense inference. The generated answers can either be a few phrases or a single sentence. The involved datasets are HotpotQA (Yang et al., 2018), Natural Questions (NQ) (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013), Curated TREC (Baudi\u0161, 2015), and TriviaQA (Joshi et al., 2017). We use Wikipedia paragraphs retrieved by DPR as the external knowledge as a common practice (Yin et al., 2022), except for HotpotQA, where we use the passages the queries are generated from as a gold knowledge resource. The statistics of the involved datasets are reported in Table 1. We collect a benchmark with var- ious datasets of different types, formats, and knowledge sources, where we will then evaluate the effectiveness of IAPEK on it.\n\n3.3 U SING E XTERNAL K NOWLEDGE\n\n Table 2 presents the model performance on both the MC classi\ufb01cation and open-domain QA tasks. For the MC classi\ufb01cation tasks, we can observe that: (i) for the zero-shot setting, models do not con- sistently get bene\ufb01t from external knowledge. In addition, the ability to utilize external knowledge is also not clearly improved as the parameter size grows, which indicates that simply using larger models may not be the solution for better using the knowledge; (ii) for the transfer-learning setting, although AGNews , e-SNLI , CIKQA , and StrategyQA are not seen during the training of Uni\ufb01edQA 5 -tuned on the Uni\ufb01edQA dataset. models, we can observe that models achieve better performance than vanilla T5 models at differ- ent sizes. Under the with knowledge case, the Uni\ufb01edQA models achieve signi\ufb01cant improvement for utilizing external knowledge compared to the zero-shot models, Uni\ufb01edQA-3b achieves the best performance on all the tasks, which indicates that models can learn and transfer the ability to utilize external knowledge with examples containing external knowledge. For open-domain QA datasets, we can observe that: (i) similarly, models fail to get bene\ufb01t from external knowledge in 11 out of 25 cases under the zero-shot setting. Surprisingly, the smallest model T5-base gets bene\ufb01ts for all the tasks, but the largest model (OPT-30b) gets worse perfor- mance with knowledge for all tasks. The reason behind this can be that, since T5-base does not have enough internal knowledge, any relevant external knowledge can help. As a comparison, the OPT-30b model already contains rich knowledge, and thus the external knowledge may only be in- troducing extra noise if the model does not learn to utilize knowledge; (ii) under the transfer-learning setting, Uni\ufb01edQA-based models get signi\ufb01cant bene\ufb01t from the external knowledge, again showing the effectiveness of helping models to learn to use knowledge. In conclusion, we \ufb01nd that \ufb01ne-tuning on instances containing external knowledge is an effective way to help models gain performance increase from external knowledge at test time.\n\n3.4 P ERFORMANCE OF T HRUST\n\n Since a pre-condition of Thrust is that the external knowledge is useful, we evaluate the effective- ness of Thrust with the Uni\ufb01edQA models. From the results in Table 3, we can observe that: (i) Thrust consistently contributes to the performance from the base to the 3B model. Through cluster- ing the instances, we acquire the whole instance distribution in the eyes of the models. Then with distance to the cluster, Thrust represents how well the model can categorize a new query vector and \ufb01nd its similarity with others on the task. Leveraging such information, Thrust identi\ufb01es the no knowledge and controversial knowledge cases well and puts the knowledge into the most necessary ones; (ii) the gain is higher when the portion of augmented instances is smaller. For example, for Uni\ufb01edQA-3b, the gains from Thrust with 25% instances augmented with knowledge are 6.1%, 13.56% on MC classi\ufb01cation and QA tasks, respectively, while for the 75% case, the gains are 2.8% 6 We \ufb01nd that for many cases Thrust can outperform full knowledge usage with fewer examples augmented with external knowledge. and 6.8%. Such observation shows that Thrust is most effective on identifying the most necessary cases. One potential reason is that Thrust is sensitive to the distance change so the isolated examples ( no knowledge case in Figure 2 ) can be easily identi\ufb01ed; (iii) we also observe consistent failure case on ARC-C. The reason can be that the queries are designed as open questions, and the answers are usually about plans or ideas, not facts, so that it is hard for the small-size models to extract useful information from the seemingly unrelated Wikipedia document. For example, a query from ARC-C is: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation? . The correct and wrong options are Record the details of the investigation and Choose different objects to roll . For questions of this style, it is even hard for humans to \ufb01nd a relevant Wikipedia page that can help. The failure case further sheds light on the pre-condition of Thrust : we assume that external knowledge is useful.\n\n4 A NALYSIS 4.1 C OMPARISON WITH F ULL K NOWLEDGE U SAGE\n\n We denote simply using external knowledge for all instances as a costly but straightforward way of leveraging external knowledge. Since the big models might be suf\ufb01cient for certain instances and the external knowledge might introduce extra noise, we hypothesize that, in some cases, Thrust can help identify instances requiring (or not) knowledge and achieve higher overall performance on the whole dataset compared to seeking and adding knowledge indiscriminately. Table 4 presents the compar- ison between adaptive and indiscriminate knowledge propulsion. Thrust here denotes the best per- formance achieved when less than 90% of instances use external knowledge. We could observe that, for 50% and 30% tasks for Uni\ufb01edQA-base and Uni\ufb01edQA-3b, respectively, Thrust achieves better performance than using knowledge for all instances. Such results indicate that Thrust can help avoid potential noise. On the other hand, we can also observe that for the e-SNLI , Web Questions , and NQ , 7 the full knowledge setting performs better than Thrust . The reason behind this can be that external knowledge is essential and of high quality (e.g., the used knowledge is manually written rather than retrieved). As long as the model can comprehend the external knowledge, seeking and adding more high-quality knowledge always bene\ufb01t the models.\n\n4.2 D ISTRIBUTION OF T HRUST ACROSS T ASKS\n\n To further understand how Thrust leads to adaptive knowledge propulsion, we select the top 3 tasks with the highest improvement after applying IAPEK with Thrust from MC classi\ufb01cation and open-domain QA tasks, respectively. The selected tasks are: e-SNLI , BoolQ , CIKQA , HotpotQA , TriviaQA , and NQ . Figure 3 demonstrates the distribution of Thrust scores for each of the involved tasks. The scores are cast to [0 , 1] with respect to the extremum, and the distribution is normalized by Kernel Density Estimation. From the \ufb01gure, we can observe that low scores (i.e., the query needs external knowledge) appear commonly for the open-domain QA tasks such as HotpotQA and TriviaQA . CIKQA queries, which are designed to require commonsense knowledge to solve, also need external knowledge for many cases, as predicted by Thrust . On the other hand, for e-SNLI and BoolQ , external knowledge is not always necessary. Such \ufb01ndings demonstrate the potential of using Thrust to investigate the characteristics of tasks from the knowledge aspect.\n\n4.3 L AYER ABLATION\n\n Since we cast instances into the representation space, a crucial contributing factor for Thrust is the layer of the PTLM to use. To investigate the effect of which layer to use, we conduct experiments on Uni\ufb01edQA-3b with the same setting as in Section 3.1. Figure 4 presents the performance of adding 25%, 50%, 75% knowledge-augmented examples with Thrust using the hidden states of different layers. We can observe that, for most tasks, there is no signi\ufb01cant difference across lay- ers, which shows the robustness of Thrust and potential capacity to accelerate the computation of Thrust of using lower layers. However, for some tasks, such as StrategyQA and Web Questions , the middle-layer representation may worsen the overall performance. The reason can be that: early layers in the model contain rich semantic information, and later layers contain task-speci\ufb01c informa- tion (Lovering et al., 2021), so that both can act as good representation of the instances. However, in the middle layers, rich semantic features are abandoned during extracting task-speci\ufb01c features and task-speci\ufb01c features are also not fully extracted and expressed yet.\n\n5 R ELATED W ORK\n\n PTLM with external knowledge. The paradigm of retrieving knowledge from knowledge bases, augmenting PTLMs, and solving downstream tasks has been widely explored in the community of NLP. The knowledge bases can range from knowledge graphs (Xu et al., 2021), documents (Paran- jape et al., 2022), pre-processed vectors (Verga et al., 2021), other PTLMs (Shwartz et al., 2020), 8 . search engines (Nakano et al., 2021), to Wikipedia documents as used in this work. To augment the PTLMs, common practice includes creating synthesizing datasets (Wu et al., 2021), adding knowl- edge to the prompts (Wei et al., 2022; Nye et al., 2021), create demonstrations (Brown et al., 2020), and extending feature vectors (Khattab & Zaharia, 2020). In this work, we pioneer the study by adding the knowledge in the plain text format. However, since computing Thrust requires only the queries, not labels nor gold answers, during the test time, the method can be extended to any of these aforementioned settings. Hardness and Con\ufb01dence Estimation in PTLMs. Much previous work studies the estimation of dataset hardness and model con\ufb01dence under the context of PTLMs. For dataset hardness, RDA (Perez et al., 2021) measures the hardness as the cumulative area under the loss curves of cross-fold validation on the test set. Point-wise V -Usable information (Ethayarajh et al., 2022) com- putes the hardness as entropy difference between the feature-provided case and the blank feature case. Sensitivity Measurement (Hahn et al., 2021) measures the dataset difference by computing the variance of loss of the correct labels on a set of neighbor sentences extracted from generative models with masked original sentences as the inputs. These methods achieve great correlation with the model performance. However, all these methods focus on analyzing the test set performance, thus the test set labels are required and can not be applied when predicting the answers. Another line of work focuses on estimating the expected calibration errors (ECE) for classi\ufb01cation (Kong et al., 2020), QA (Jiang et al., 2021), and math (Lin et al., 2022) datasets, as a re\ufb02ection of model certainty on the correct answers. ECE can be considered as an orthogonal evaluation metric to measure the model\u2019s capability of understanding the tasks, compared to common metrics such as accuracy. Most of the previous work can be considered a posterior analysis of the model capability. In this work, instead, we estimate the pragmatic con\ufb01dence at the test time to empirically increase the performance with limited budget or bandwidth to acquire knowledge.\n\n6 C ONCLUSION\n\n In this work, we propose I nstance-level A daptive P ropulsion of E xternal K nowledge (IAPEK) as a solution to propel model performance when the external knowledge is useful but noisy. Accordingly, we propose a simple and effective instance-wise metric, Thrust , to perform the adaptive knowledge injection. Extensive experiments show that Thrust can improve the performance of utilizing external knowledge under various settings. Understanding the delicate usage of potentially noisy knowledge for PTLMs can further enable the models to conduct inference beyond the limitation of implicit internal knowledge. 9\n\nR EFERENCES\n\n 10 11 ?id=Vr_BTpw3wz 12 13 14", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes IAPEK (Instance-level Adaptive Propulsion of External Knowledge), a method to selectively augment pre-trained language models (PTLMs) with external knowledge only when necessary.", "topic_class_1": "paper summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors introduce a novel metric called Thrust which scores each instance based on whether the PTLM needs external knowledge to answer it.", "topic_class_1": "paper summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Thrust leverages the distribution of training instances in the PTLM's representation space.", "topic_class_1": "method", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on a range of classification and QA tasks show that using Thrust to adaptively inject external knowledge leads to significant performance improvements while being more cost-efficient than always using external knowledge.", "topic_class_1": "paper summary", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The findings shed light on how to effectively use knowledge in real-world applications of LMs with limited knowledge-seeking budgets.", "topic_class_1": "contributions", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper addresses an important problem - how to efficiently and effectively augment PTLMs with external knowledge only when needed.", "topic_class_1": "contributions", "topic_class_2": "Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This is critical for real-world applications.", "topic_class_1": "contributions", "topic_class_2": "Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The Thrust metric is simple yet effective at identifying instances that require external knowledge.", "topic_class_1": "contributions", "topic_class_2": "Effectiveness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It is a novel contribution.", "topic_class_1": "contributions", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Extensive experiments convincingly demonstrate the effectiveness of the IAPEK approach using Thrust, leading to performance gains while being more cost-efficient.", "topic_class_1": "experiments", "topic_class_2": "Effectiveness", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The work provides valuable insights into the judicious use of external knowledge in LMs to balance performance and efficiency.", "topic_class_1": "contributions", "topic_class_2": "Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the Thrust metric is evaluated on various tasks and datasets, it would be good to see even more comprehensive experiments, e.g.", "topic_class_1": "experimental suggestions", "topic_class_2": "Lack of Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "on additional benchmarks and knowledge sources.", "topic_class_1": "experimental suggestions", "topic_class_2": "Lack of Experiments", "reliability": "No", "explanation": "The authors have already tested their method on 12 benchmarks. I don't think they need more benchmarks. Also this suggestions is very blurred, without pointing out specifc datasets.", "error_type": "Out-of-scope"}, {"segment_text": "- Some details on Thrust could be explained further, e.g.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "how hyperparameters like number of clusters are chosen, runtime cost of computing Thrust.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Qualitative examples illustrating when Thrust decides to seek external knowledge would provide additional insights.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Case Study", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Theoretical analysis or justification of why Thrust is an effective scoring function would strengthen the work.", "topic_class_1": "analytical suggestions", "topic_class_2": "Missing Theoretical Analysis", "reliability": "No", "explanation": "I agree this would strengthen the work, but without analytical justiication, this work is already solid enough. ", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and the proposed method is well-explained.", "topic_class_1": "writing", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The overall quality of research is high, with the authors performing comprehensive experiments.", "topic_class_1": "contributions", "topic_class_2": "Extensive Experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The Thrust metric and overall IAPEK approach are novel contributions.", "topic_class_1": "contributions", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Key details are provided to allow reproducibility, though the code is not open-sourced yet (but the authors intend to release it).", "topic_class_1": "reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a valuable contribution by proposing IAPEK and the Thrust metric to adaptively augment LMs with external knowledge.", "topic_class_1": "contributions", "topic_class_2": "Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Thrust effectively identifies instances needing knowledge, enabling IAPEK to outperform baselines while being more efficient.", "topic_class_1": "contributions", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Extensive experiments highlight the benefits.", "topic_class_1": "contributions", "topic_class_2": "Extensive Experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some additional details, analysis and even more comprehensive evaluation could further strengthen this already solid work.", "topic_class_1": "suggestions for improvement", "topic_class_2": "Missing Details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper proposes a novel and impactful method to address an important problem.", "topic_class_1": "paper summary", "topic_class_2": "Overall Evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents an innovative approach, titled \"THRUST: Adaptively Propels Large Language Models with External Knowledge,\" focusing on enhancing the efficiency and effectiveness of large pre-trained language models (PTLMs) by selectively utilizing external knowledge.", "topic_class_1": "paper summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The core of this approach is the Instance-Level Adaptive Propulsion of External Knowledge (IAPEK), which evaluates the necessity of external knowledge for each instance using a novel metric called Thrust.", "topic_class_1": "paper summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Thrust is based on estimating the distribution of seen instances and deciding on the use of external knowledge accordingly.", "topic_class_1": "paper summary: method", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study demonstrates that this method significantly improves cost-efficiency in knowledge retrieval and utilization across a variety of NLP tasks, achieving an average performance improvement of 26% on 88% of evaluated tasks.", "topic_class_1": "paper summary", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Innovation and Relevance: The methodology addresses a crucial limitation of PTLMs, which is the opaque and static nature of their encoded knowledge.", "topic_class_1": "paper summary: targeted problem", "topic_class_2": "Novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By introducing a selective approach to external knowledge utilization, it paves the way for more efficient and effective NLP solutions.", "topic_class_1": "contributions", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical Evidence: Extensive experimentation validates the approach's effectiveness, showcasing significant improvements in performance across diverse tasks.", "topic_class_1": "paper summary", "topic_class_2": "Extensive Experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The promise to release code and data after acceptance encourages reproducibility and further research.", "topic_class_1": "reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Lack of Comparison: The paper does not thoroughly compare its approach with existing methods in the domain of external knowledge utilization, which could have strengthened the argument for its novelty and effectiveness.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Baselines", "reliability": "No", "explanation": "This suggestions is very blurred, without pointing out specifc approaches.", "error_type": "Vague Critique"}, {"segment_text": "Potential for Bias: The reliance on the novel metric Thrust for determining the necessity of external knowledge could introduce bias if the metric itself is not universally applicable across different types of tasks and domains.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Analysis for Bias", "reliability": "No", "explanation": "Too generic and blurred. The suggested weakness can be applied to many DL papers.", "error_type": "Vague Critique"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written, presenting a novel approach to a significant problem in NLP.", "topic_class_1": "writing", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, supported by rigorous experimentation.", "topic_class_1": "contributions", "topic_class_2": "High quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the novelty is somewhat diminished by the lack of comparison with existing methods.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Baselines", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The commitment to releasing code and data post-acceptance is commendable and will aid in reproducibility.", "topic_class_1": "reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces a novel and impactful approach to optimizing the use of external knowledge in large language models, addressing a critical issue of efficiency and effectiveness.", "topic_class_1": "contributions", "topic_class_2": "Strength Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method, supported by extensive empirical evidence, marks a significant advancement in the field.", "topic_class_1": "contributions", "topic_class_2": "Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, a more comprehensive comparison with existing methods could have further highlighted its novelty and superiority.", "topic_class_1": "suggestions for improvements", "topic_class_2": "Missing Baselines", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The commitment to open-source principles enhances the paper's value to the research community.", "topic_class_1": "reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes THRUST, a novel metric for instance-level adaptive propulsion of external knowledge (IAPEK) in large language models (LLMs).", "topic_class_1": "paper summary", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "THRUST scores each instance based on its need for external knowledge, allowing the model to selectively utilize external knowledge only when necessary.", "topic_class_1": "paper summary: method", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This approach aims to address the challenges of time-consuming knowledge retrieval and potential noise introduced by irrelevant knowledge.", "topic_class_1": "paper summary: targeted problem", "topic_class_2": "Motivation Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper evaluates THRUST on a diverse benchmark of multiple-choice classification and open-domain question answering tasks, demonstrating improved cost-efficiency and performance compared to naive knowledge usage.", "topic_class_1": "paper summary", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The concept of IAPEK and the THRUST metric are novel and address the important challenges of efficiently utilizing external knowledge in LLMs.", "topic_class_1": "contributions", "topic_class_2": "Novelty and Impact", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Motivation: The paper clearly motivates the need for IAPEK by highlighting the limitations of existing approaches that rely on indiscriminate knowledge usage.", "topic_class_1": "contributions", "topic_class_2": "Well Motivated", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical Results: Extensive experiments demonstrate the effectiveness of THRUST in improving cost-efficiency and performance across various tasks and model sizes.", "topic_class_1": "contributions", "topic_class_2": "Extensive Experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Benchmark: The paper introduces a comprehensive benchmark covering diverse tasks and knowledge sources, facilitating future research in this area.", "topic_class_1": "contributions", "topic_class_2": "Extensive Experiment", "reliability": "No", "explanation": "Although evaluated in diverse datasets, this paper does not introduce new data.", "error_type": "Unstated statement"}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overclaiming: The paper's claim that the proposed model \"internalizes\" the task through distillation might be overstated, as it essentially boils down to improved fine-tuning.", "topic_class_1": "writing", "topic_class_2": "Overclaiming", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Missing Baseline: The evaluation lacks a crucial baseline - a student model instruction-tuned without distillation.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Baseline", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This makes it difficult to isolate the gains specifically attributable to the distillation process.", "topic_class_1": "experimental suggestions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Inconsistency in Evaluation: The selection of datasets for evaluation in some experiments appears inconsistent and lacks clear justification.", "topic_class_1": "experimental suggestions", "topic_class_2": "Dataset Justification", "reliability": "No", "explanation": "The authors talks about what is the knowledge for each dataset in Section 3.2", "error_type": "Neglect"}, {"segment_text": "Limited Analysis of Failure Cases: While the paper acknowledges the failure case of ARC-C, a deeper analysis of why THRUST fails in such scenarios would be beneficial.", "topic_class_1": "analytical suggestions", "topic_class_2": "Missing analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Including the missing baseline of an instruction-tuned student model without distillation is crucial to accurately assess the effectiveness of the proposed distillation method.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Baseline", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Providing a more detailed explanation for the selection of datasets in certain experiments would improve the clarity and consistency of the evaluation.", "topic_class_1": "need more explanation or details", "topic_class_2": "Dataset Justification", "reliability": "No", "explanation": "The authors talks about why they choose those datasets and what is the knowledge for each dataset in Section 3.2.", "error_type": "Neglect"}, {"segment_text": "A deeper analysis of the failure case of ARC-C would be valuable in understanding the limitations of THRUST and potentially guiding future improvements.", "topic_class_1": "analytical suggestions", "topic_class_2": "Missing analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarifying the details regarding model size, training/test data usage, and distillation on the test dataset would enhance the paper's clarity.", "topic_class_1": "reproducibility", "topic_class_2": "Missing Details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, THRUST is a valuable contribution to the field of LLM research.", "topic_class_1": "contributions", "topic_class_2": "Overall Evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addressing the outlined concerns and incorporating the suggested improvements would further strengthen the paper and increase its impact on the community.", "topic_class_1": "suggestions for improvement", "topic_class_2": "Weakness Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is generally well-written and presents the core idea clearly.", "topic_class_1": "writing", "topic_class_2": "Clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, some experimental details could be elaborated upon for better clarity.", "topic_class_1": "experimental suggestions", "topic_class_2": "Missing Details", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research quality is good, with a novel approach and extensive empirical evaluation.", "topic_class_1": "contributions", "topic_class_2": "Novelty and Comprehensive Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors mention releasing code upon acceptance, which will enable reproducibility.", "topic_class_1": "reproducibility", "topic_class_2": "Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "THRUST presents a novel and promising approach for selectively utilizing external knowledge in LLMs.", "topic_class_1": "contributions", "topic_class_2": "Method Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The empirical results are encouraging, demonstrating improved cost-efficiency and performance.", "topic_class_1": "contributions", "topic_class_2": "Experiment Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, certain aspects like overclaiming, missing baselines, and inconsistent evaluation require further attention.", "topic_class_1": "suggestions for improvement", "topic_class_2": "Weakness Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addressing these concerns and providing a deeper analysis of failure cases would strengthen the paper significantly.", "topic_class_1": "suggestions for improvement", "topic_class_2": "Weakness Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Cramming: Training a language model on a single GPU in one day", "body_text": "\n\nC RAMMING : T RAINING A L ANGUAGE M ODEL ON A S INGLE GPU IN O NE D AY\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.\n\n1 S CALING U P AND S CALING D OWN\n\n Large-scale training of machine learning models with transformer architectures has lead to ground- breaking improvements in many sub-fields of natural language processing including language un- derstanding and natural language generation (Vaswani et al., 2017; Dosovitskiy et al., 2021; Radford et al., 2019). The nowadays accepted (but historically surprising) key behavior of these systems is that they reliably scale \u2013 they continuously improve in performance when the number of model pa- rameters and amount of data grow. These increases in performance are well-described by various power laws as studied by Kaplan et al. (2020). This sets up a dominant paradigm in which scaling is the key to performance improvement (Sutton, 2019). The power of scale has set off a race to produce extremely large models, which in turn has created an environment where few researchers or practitioners feel that they are capable of training a language model. The original BERT model Devlin et al. (2019), which became a cornerstone transformer for many practical applications in natural language understanding, already required a significant amount of computation to train. Yet, the reproduction and improvements in Liu et al. (2019) further increased its performance by cranking up the level of computation by orders of magnitude. As these pre-trained checkpoints became popular for a range of downstream applications (Wolf et al., 2020), the competition for the largest language model became a focal point for industrial labs. This led to training runs that improved the performance of pretrained language models at the expense of computation at the zettaFLOP scale (Raffel et al., 2020; Yang et al., 2020; Zaheer et al., 2021) and later at the extremely large yottaFLOP scale (Brown et al., 2020; Black et al., 2022; Chowdhery et al., 2022; Rae et al., 2022). Our goal is to turn this trend on its head and investigate how to best scale down language model training and what trade-offs emerge when doing so: What downstream performance can be achieved by a modest researcher when training from scratch with a single GPU for a single day? The ability to train a language model to the performance level of BERT with such modest resources has several interesting implications. For one, if scaled-down model pretraining is a viable analogue of large-compute pretraining, then this opens up a host of further academic investigations that 1 are currently hard to realize for large-scale models. For example, research questions about the differences between existing and new pre-training tasks, tracing model predictions to data points (Ilyas et al., 2022), security questions such as membership inference (Carlini et al., 2022) and data poisoning (Geiping et al., 2021), and a wide range of empirical investigations into topics such as stability or generalization that arise during training (Nagarajan & Kolter, 2019; Jiang et al., 2019). At the same time, we can imagine situations in which legal requirements make it unclear whether models trained on public data with uncertain origin are permissible, and where a practitioner is interested in retraining their language models using a specialized or trustworthy data source (Wilka et al., 2017; Gold & Latonero, 2017). In addition, we are motivated to benchmark the overall conceptual progress of research in this area over the last years, beyond simply turning the scaling knob. The goal of achieving BERT-like perfor- mance with modest training resources would have seemed unthinkable in 2018, and yet with modern advances and transformer training techniques this may now be possible. To answer these questions, we consider a challenge we call \u201cCramming\u201d \u2013 learning a whole language model the day before the test. Our studies begin by investigating many facets of the training pipeline to see which modifications actually improve performance in the scaled-down scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. An unsurprising consequence of these laws is that scaling down is hard; while smaller model architectures enable speeding up gradient computations, overall rates of model improvement over time remain nearly constant. Nonetheless, we can find changes to the training pipeline that exploit scaling laws to yield improvements by improving the effective rate of gradient computations without compromising model size. In the end, we are able to train models that achieve respectable performance \u2013 often close to and sometimes exceeding BERT on GLUE tasks \u2013 on a shoestring budget.\n\n2 T YING OUR HANDS BEHIND OUR BACK : A SETUP WITH LIMITED COMPUTE\n\n Before we start this investigation, we want to outline the extent of limitations we are interested in. The rules for cramming are as follows: \u2022 A transformer-based language model of arbitrary size is trained with masked-language modeling, completely from scratch. \u2022 Existing pretrained models cannot be included in any part of the pipeline. \u2022 Any raw text (excluding downstream data) can be included for training. This means that one can achieve speedups by making judicious choices about how and when to sample data, provided the sampling mechanism does not require a pre-trained model. \u2022 The downloading and pre-processing of raw data is exempted from the total compute bud- get. Pre-processing may include CPU-based tokenizer construction, tokenization, and fil- tering, but cannot include representation learning (i . e. pre-training a word embedding is not allowed, unless it is counted towards the final runtime). \u2022 Training proceeds on a single GPU for 24 hours. \u2022 Downstream performance is evaluated on GLUE (Wang et al., 2018). Downstream finetun- ing on GLUE is limited to brief training with only the training data of the downstream task (We consider 5 epochs or less) and needs to work with hyperparameters set globally for all GLUE tasks. Downstream finetuning is excluded from the total compute budget. In our implementation, we analyze both a setup with a classical rtx2080ti GPU (released September 2018) and a separate setup with a more modern rtxa6000 GPU (October 2020). We pair each unit with 4 CPU cores and 32GB of RAM. Why these limitations? We are principally interested in re-investigating the original BERT setup of Devlin et al. (2019) with limited compute. The optimal architecture of the transformer is not fixed, as the optimal size and shape depends on scaling laws (Kaplan et al., 2020). The limitations on usage of existing models rule out distillation from an existing model (Turc et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020b; Kaliamoorthi et al., 2021) and data filtering based on existing large models (Golchin et al., 2022), both of which ultimately answer questions about compression and 2 transfer of already processed information. Further, we do not want to limit data to the original dataset used to train BERT, wanting to allow for possible improvements through better data curation and quality. The rtx2080ti GPU is a natural candidate for this experiment, given that it was released before Devlin et al. (2019), but the more recent rtxa6000 is also interesting, being arguably the limit of a single-user workstation. At the finetuning stage we want to mimic the original BERT finetuning and evaluation setup, but provide additional limits to prevent gains based on tuning of only the downstream procedure, for example via computationally extensive downstream training (Bahri et al., 2021a), use of multiple downstream datasets (for example continued pretraining with MNLI before finetuning other tasks (Izsak et al., 2021)), and extended hyperparameter optimization for each GLUE task (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019).\n\n3 R ELATED W ORK ON E FFICIENT T RANSFORMERS\n\n How long does it take to train BERT? In general, this question is hard to answer, due to wildly varying hardware and software setups and differing measures of efficiency (Dehghani et al., 2021). An upper bound on the compute of a training run can be established by finding the total number of (low-precision) floating point operations available over the wallclock budget of the run. This peak of total FLOPs in a given time interval is generally not reached in actual compute, even for highly opti- mized models (Chowdhery et al., 2022), but represents the paid budget required to realize a training run. We summarize budgets for a few select training runs in Table 1. After the original training run for BERT on TPUs, initial reactions estimated up to 11 days of compute for comparable results on GPUs (Dettmers, 2018). However, sustained improvements, especially in software, have reduced the upper limit significantly (You et al., 2019; Narasimhan, 2019). Yet, recipes and implementations generally require entire server nodes (for GPUs) or TPU slices and target larger BERT architectures. Other work discussing improvements to BERT targets compute settings closer to the original BERT, for example SqueezeBERT (Iandola et al., 2020) employs 8 Titan RTX cards for four days. Sel- lam et al. (2022) note that the original BERT training run is an outlier and doubling its training time more reliably reproduces the original results. Our central point of comparison for BERT training with limited resources is the work of Izsak et al. (2021) who also attempt the goal of training BERT within 24 hours with overall similar limitations, but use a full server node with 8 V100 GPUs. Izsak et al. (2021) choose a BERT LARGE architecture variant and train with sequence length of 128 , including a range of tweaks such as modified learning rates schedules, large batch sizes, sparse prediction and packed sequences. We re-evaluate this setup as a baseline setting for our own compute budget (which is about 15x smaller). Studies of Efficient Transformers Recent years have seen a flurry of research working to improve and modify the transformer architecture proposed in Vaswani et al. (2017) and we refer to Treviso et al. (2022) for a recent categorization and review of research in this area. Several meta-studies have investigated proposed improvements and modifications: Narang et al. (2021) evaluate a large range of architectural modifications applied to the T5 model pipeline of Raffel et al. (2020) on tasks in 3 both language understanding and translation. The encoder-decoder structure of T5 is closer in spirit to the original transformer setup, but is understood to behave similarly to BERT when using the encoder component (Liu et al., 2021a). Evaluating modifications with 1.75 days of compute on TPU slices they find that most improvements do not reliably materialize gains in final accuracy. Tay et al. (2021) work in the same setting and evaluate the optimal shape of T5 derived architectures and its relative effects on downstream performance as models are scaled. Further exploration of the scaling behavior of various architectural improvements in Tay et al. (2022a) find that only few modifications outperform the original architecture of Vaswani et al. (2017) at all scales, especially when evaluating downstream accuracy. The meta-study investigating improvements in preparation for extreme-scale training in Scao et al. (2022) focuses on minor modifications to layout, positional embeddings and data sources for autoregressive models, and other extremely-large scale training runs have so far been similarly conservative in their settings (Brown et al., 2020; Black et al., 2022; Rae et al., 2022). In general though, these evaluations target larger compute settings than we intend to use, and are concerned with whether improvements (often from academic sources and proposed with evaluations on small scales) translate to larger scales. In this work, we set aside the question of (up)scaling and focus only on the limited compute. Scaling Laws The difficulty in finding tangible improvements is echoed in the scaling laws of Kaplan et al. (2020). Over a wide range of transformer model shapes, Kaplan et al. (2020) find only model size (as number of parameters in non-embedding layers) strongly predicts performance. Further, for a fixed compute budget, an optimal model size can be derived, but performance is only mildly connected to model size - larger models processes less data per unit of compute, but improve faster by almost the same margin. While the precise coefficients and shape of these scaling laws continue to be iterated on (Hoffmann et al., 2022) and adapted for related settings (Bansal et al., 2022; Clark et al., 2022; Bahri et al., 2021b), their overall logic appears hard to escape, even if power laws fit observations somewhat less well on small scales.\n\n4 I NVESTIGATIONS\n\n For our experimental evaluation we implement and test a considerable number of proposed modifi- cations to the setup of Devlin et al. (2019) for their merits in our limited compute setting as described in Section 2. We first clarify the common implementation and initial data setup, and then investigate architectural, training and dataset improvements.\n\n4.1 I MPLEMENTATION D ETAILS\n\n We implement everything in PyTorch (Paszke et al., 2017) and to limit our gains from the \u201dsoft- ware lottery\u201d (Hooker, 2021) we do not use specialized implementations (e.g. as proposed for attention mechanisms in Ivanov et al. (2021); Dao et al. (2022)), which would further bias results to- wards well-established components. We keep everything on the implementation level of the PyTorch framework, allowing only automated operator fusion (Sarofeen et al., 2022) that can be applied to all components. We run all experiments and ablation studies with the same setup of automated mixed precision (Micikevicius et al., 2018) for standard 16- and 32-bit floating point precision (over full 32-bit float, scaled 16-bit (Rasley et al., 2020) and pure bfloat16 (Wang & Kanwar, 2019). We find no benefit from offloading (Ren et al., 2021; Rasley et al., 2020) in our setting.). Initial Data Setup We start our investigation with a close analogue to the original raw text sources of Devlin et al. (2019), using a recent dump of the English Wikipedia ( 20220301.en ) and En- glish bookcorpus , noting the commentary of Tan (2019); Bandy & Vincent (2021). We force all text into lower-case, strip accents and non-ascii characters and create an English tokenizer from scratch based only on this data. We choose WordPiece with a vocabulary size of 2 15 = 32768 (Wu et al., 2016). We found no significant change in performance with BPE (Sennrich et al., 2016) or SentencePiece with Unigrams (Kudo, 2018; Kudo & Richardson, 2019). Smaller vocabulary sizes ( 2 12 , 2 13 , 2 14 ) resulted in worse performance, while larger vocabulary sizes ( 2 16 ) we not reliably better. We pack tokenized data into randomized sequences of length 128 and separate unrelated fragments by <sep> The performance impact from dropping this separator was minimal. No im- pact was observed from including a <cls> token in pretraining. The shorter sequence length is 4 sufficient for the downstream applications that we are targeting and simplifies attention computa- tions. Packing data into full sequences limits us to simpler sequence losses, but uses the available compute optimally Liu et al. (2019); Izsak et al. (2021). For the targeted compute settings, this sequence length results in micro-batch sizes of 64 to 96 for most variations of the base BERT ar- chitecture on gtx2080ti , which we will accumulate into larger batch sizes. With our limited compute budget, this produces enough samples to run single-epoch training (Komatsuzaki, 2019; Hernandez et al., 2022) where no data point is revisited.\n\n4.2 M ODIFYING THE A RCHITECTURE\n\n The most obvious way to efficiently scale down training is by modifying the model architecture; in- tuitively, it seems likely that smaller/lower capacity models will be optimal in the cramming regime. In this section, we study the relationship between model type and training efficiency. We see that scaling laws create a strong barrier to scaling down. Per-token efficiency of training depends strongly on model size, but not transformer type. Furthermore, smaller models learn less efficiently, and this largely mitigates any throughput gains. Fortunately, the fact that training efficiency is nearly con- stant across models of the same size means that we can boost performance by finding architecture modifications that speed up gradient computation while keeping the parameter count nearly con- stant. This makes architecture selection fairly straightforward as we can make design choices based primarily on how they affect computation time for a single gradient step. Scaling laws hold in the low-resource regime A large corpus of research in recent years has developed architectural improvements to speed up the original transformer. Many of these methods have not been found to improve training for the large-scale T5 architecture Narang et al. (2021); Tay et al. (2022a). But, in the low compute setting where data throughput is of utmost importance, maybe this is the way forward? Scaling laws have been observed by Kaplan et al. (2020) in the high- resource regime, and seem to hold strongly in the limit as resources grow. Surprisingly, these laws also hold in the limit of extreme compute down-scaling, and they create a barrier to low-cost training. We exemplify the effect of scaling laws for many fast transformer variants form the literature in Figure 1, where we train each architecture variant with the optimal training hyperparameters as described below in Section 4.3. We apply these architecture variants to a shared baseline model that incorporates Pre-Normalization and rotary embedding. Figure 1 visualizes the progress of MLM loss versus the number of tokens ingested in total and all architectures run with the same time budget. We observe that varying the transformer type and size has only minimal impact on the final loss after 24 hours. Models with more parameters learn more efficiently, as their MLM loss decreases faster on a per-gradient basis. However, smaller architectures make up for their slower learning efficiency by having cheaper gradients, and thus processing more tokens within the limited budget. Figure 1 shows that different architectures are unpredictable throughout an initial stage of training (the first 1B tokens), after which the per-token efficiencies differ by only a multiplicative constant 5 (a horizontal shift due to the log axis). This constant depends almost entirely on the model size, not model type, so that all choices reach a MLM loss around 1 . 9 at the end of training. Exploiting the scaling law. The scaling laws seem to bar us from making large gains via major changes to the transformer size and type, as per-token performance is tightly coupled to model size. As a result, we find no improvements when using a funnel-transformer architecture (Dai et al., 2020; Nawrot et al., 2022), when dropping FFN layers (Sridhar et al., 2022), or when using recurrent layers (Lan et al., 2019), even when trained with BPTT as in Schwarzschild (2021). Rescaling architectures to be deep-narrow (Tay et al., 2021; Wies et al., 2021) provides no gains. While this principle closes one door for scaling down efficiently, it opens another; Because per- gradient efficiency remains nearly constant for all models of the same size, we can exploit scaling laws by quickly searching for architectural choices that speed up computation while keeping model size roughly constant. A number of obvious optimizations fall into this category, and we describe them below, in addition to several other tweaks that provide marginal but worthwhile/free gains. Attention Block: We disable all QKV biases (Dayma et al., 2021). This exploits the scaling law by removing a layer of computation, making the forward and backward pass somewhat faster, while keeping the model size nearly constant. We find that we can decrease gradient costs by reducing the number of attention heads (Merity, 2019; Araabi & Monz, 2020; Liu et al., 2021b; Javaheripi et al., 2022), as this parallelizes better on the GPU and provides a slight performance boost. We find no benefits from replacements to the softmax operation (Richter & Wattenhofer, 2020). We further keep the original multi-head self-attention mechanism. A large amount of work has been focused on efficient attention (Sukhbaatar et al., 2019; Beltagy et al., 2020; Wang et al., 2020a; Liu et al., 2021c) and studies of efficient attention (Tay et al., 2020a;b). But, because we set the maximal sequence length to 128 , attention complexity is less of a concern in our setting. To verify this, we implement the recently proposed FLASH mechanism (Hua et al., 2022), but find no benefits. We further experiment with Fourier attention as proposed in Lee-Thorp et al. (2021), but find no improvements. We supplement the attention with rotary embeddings (Su et al., 2021; Black et al., 2022), which we find to provide small benefits. Feedforward Block: We find empirical gains from disabling all linear layer biases (Dayma et al., 2021). Just as for the attention layers, this leverages the scaling law by accelerating gradient com- putation without noticeable impacts on model size. As a result, we get higher throughput without compromising the rate at which the model improves. We keep the original feedforward block largely unchanged, finding no benefits from changing to another activation than GELU. We do see small improvements from re-ordering the block into a gated linear unit (Dauphin et al., 2017). In contrast to other work, e.g. (Black et al., 2022), we do not increase the number of parameters in the FFN block to compensate for the halving of the hidden dimensionality due to gating. Embedding: We implement scaled sinusoidal positional embeddings as described in Hua et al. (2022), finding incremental benefits over learned or unscaled sinusoidal embeddings. We see no im- provements from decoupling the input and output embeddings (Chung et al., 2020). The suggestion from Lan et al. (2019) to factorize the input embedding provides no gains in our setting. We include a layer normalization at the end of the embedding block. Layer Structure: As observed in many studies, we find that pre-normalization with Layer Norms is beneficial over post Layer Norms (Baevski & Auli, 2018; Xiong et al., 2020). We see no ad- ditional benefit from other variants of this modification, such as (Liu et al., 2020b; Shleifer et al., 2021). Further, replacing Layer Normalization with RMS Normalization provides no gains (Zhang & Sennrich, 2019). We note that the key effect of pre-normalization is to stabilize training and enable larger learning rates and reduced warmup, and we see limited benefits from including it by itself. We see no benefits from stochastic dropping of entire layers as described in (Zhang & He, 2020). Head Block: We find that we can remove the nonlinear head without ill effect. We can further drop the decoder bias (Radford et al., 2019) and gain in memory using sparse token prediction (Liu et al., 2019; Izsak et al., 2021). We add a final Layer Norm to stabilize training further. 6\n\n4.3 M ODIFYING THE T RAINING S ETUP\n\n We study the impact of training hyper-parameters on the BERT-base architecture. The original BERT training recipe understandably results is poor model performance in the cramming setting, and so we revisit a number of standard choices. Objective: We train with only masked language modeling on fully packed blocks of tokens with a masking rate of 15% and the original setup of Devlin et al. (2019) where 10% of all masks are filled with random words and 10% unchanged. We see no improvement from masking at larger rates, e.g. at 40% as proposed in (Wettig et al., 2022). We see no difference enabling or disabling the mentioned 20% rule. We evaluate other functions for the masked-language objective, such as mean-squared error (Hui & Belkin, 2021) or L1 loss, but find no benefits. Choice of Optimizer: We keep Adam (Kingma & Ba, 2015) as the optimizer of choice, with weight decay of 0 . 01 as described in (Loshchilov & Hutter, 2017), \u03b2 1 = 0 . 9 , \u03b2 2 = 0 . 98 and \u03b5 = 10 \u2212 12 . We find no noticeable change in varying these parameters in reasonable amounts, e.g. \u03b5 = 10 \u2212 6 , \u03b2 1 = 0 . 9 , \u03b2 2 = 0 . 999 . We test other first-order adaptive optimizers (Shazeer & Stern, 2018; Liu et al., 2020a) but find no advantages in our setting. We further find no advantages using higher- order optimizers (Yadav, 2020; Anil et al., 2021), but note that especially for higher-order optimizers there is a greater amount of variability in implementation. Learning Rate Schedule and Peak: Following the advice of Izsak et al. (2021), we re-scale the learning rate schedule so that it is tied to our budget and the learning rate decays as the budget reduces to zero. Interestingly, we observe in Figure 2 that while globally a large number of learning rate shapes lead to similar reductions in loss, we find that we can make some gains through the choice of schedule. We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10 \u2212 3 leads to minimal pretraining loss within our budget. Batch Size Schedule: A particularity of our setting is that, due to being limited to a single GPU, the micro-batch size that finds its way onto this GPU ( 96 for most experiments) is several times smaller than the optimal batch size. We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss and 4032 for maximal downstream performance, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively. Fortunately, we can find small speedups by using an aggressive batch size schedule; we increase the number of averaged micro-batches linearly over the course of training. This results in more progress earlier in training, and leads to a small benefit to performance. We also experiment with automatic and adaptive batching rules (De et al., 2017; Bollapragada et al., 2018a;b), but find that the best results from these adaptive schedules resemble the fixed linear schedule. For simplicity we just stick to the simpler linear schedule. Dropping Dropout The original BERT model of Devlin et al. (2019) includes dropout as in Vaswani et al. (2017), which prevents overfitting when training data is small relative to total compute 7 budget. While it can be helpful as a regularizer, dropout effectively reduces the number of gradient updates seen by each parameter, as updates do not occur when the associated feature is dropped. At the same time, update runtime is not strongly effected by the presence of dropout, and so dropout results in a net reduction in updates per second. In the cramming setting, training data is large compared to compute. Overfitting is not possible due to the single epoch schedule, and we disable dropout during pretraining (Brown et al., 2020) to maximize the number of parameter updates. We re-enable dropout during downstream fine-tuning with a dropout value of 0 . 1 . Further, we experiment with length curricula (Li et al., 2022) and token dropping (Hou et al., 2022), but find no gains.\n\n4.4 O PTIMIZING THE D ATASET\n\n We found above that scaling laws create a barrier to making major gains (beyond computational ef- ficiencies) with architectural modifications. However, scaling laws do not preclude us from training on better data. Once we have exhausted our ability to train on more tokens per second, we should seek to train on better tokens. We consider two data based pathways to better down-scaling. First, we can filter, process, or sort the existing data in various ways. Second, we can swap our data source. To this end, we experiment with several subsets of The Pile (Gao et al., 2020), containing raw text from only Gutenberg , Books3 and Wikipedia (en) . From these Pile datasets we tokenize the first 4 \u00d7 10 6 entries to generate enough tokens for our single pass. Another popular source of data is C4, the colossal, cleaned version of Common Crawl (Raffel et al., 2020), from which we stream the first 20 \u00d7 10 6 entries. For each data source we regenerate its own WordPiece tokenizer as described in Section 4.1. Of these four sources, we find the Pile to perform best in terms of downstream MNLI performance. However, it turns out we can further improve especially the C4 datset through additional processing. We first evaluate deduplication as described in Lee et al. (2022) via exact substring deduplication, but find this not to help in downstream performance in our case. We then test filtering for un- compressible data. We use the tokenizer itself to remove all training sequences from C4 set that cannot be compressed well; we simply set a threshold t , e.g. t = 0 . 3 , and drop all entries from the dataset where the number of tokens in the entry is larger than t times the number of raw characters. This removes, for example, sequences consisting of hard-to-compress HTML or markdown code. Surprisingly, this results in a measurable improvement on C4, summarized in Table 2. We then see some further improvements from two directions. First, sorting all tokenized se- quences by some metric, and second, increasing the final batch size. For filtering we sort all tokenized sequences by their average (unigram) token prevalence, so that likely sequences occur 8 first. This has some positive effect and can be strengthened slightly by drawing from a larger cor- pus as the unlikely sequences never get reached. Finally increasing the batch size to 4032 at the end of training (as mentioned in Section 4.3) is disproportionally effective on C4, but less so on bookcorpus-wikipedia . We believe that both modifications ultimately reduce the likelihood of training being hindered by fluctuations in the data distribution.\n\n5 F INETUNING P ERFORMANCE ON GLUE\n\n Finally we evaluate performance on the GLUE benchmark of Wang et al. (2018), minus WNLI as in Devlin et al. (2019). We note that we only use MNLI (m) during the previous sec- tions and do not tune hyperparameters based on the full GLUE scores. We finetune both the pre- trained BERT-base checkpoint and our models under the same constraints laid out in Section 2. For BERT-base, we finetune all datasets for 5 epochs with a batch size of 32 and learning rate of 2 \u00d7 10 \u2212 5 . For the crammed models, we find that this is not optimal and minor improvements can be gained from a batch size of 16 and learning rate of 4 \u00d7 10 \u2212 5 with cosine decay (this setup does not improve the pretrained BERT checkpoint). Table 3 and Table 4 describe the performance of this setup on the GLUE downstream tasks (as me- dian over 5 trials). There we compare the original BERT-base checkpoint, a reproduction of the BERT pretraining settings stopped after our budget is reached, and the modified recipe, evaluated with the single day rtx2080ti setup and the single day A6000 setup. Overall, performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where down- stream finetuning can smooth remaining differences. However, even the smaller datasets mostly work. The average is brought down however by a massive drop on CoLA (corpus of linguistic ac- ceptability) (Warstadt et al., 2019). This behavior is intriguing and we offer two hypotheses. First, it is conceivable that the chosen global hyperparameters for finetuning are a bad fit for CoLA in particular. CoLa performance can be brittle with respect to hyperparameter, with Jiao et al. (2020) training longer only on CoLA or Joshi et al. (2020) training less only on CoLA. Nevertheless, for BERT, a set of global hyperparameters exists, pointing at a deficiency in the crammed model. As a second hypothesis, it is conceivable that these models need to process more text before they memo- rize enough data to do well on CoLA. This would be in contrast to Liu et al. (2021d) who find that CoLA is learned relatively quickly compared to other downstream tasks when probing intermediate BERT checkpoints. On the other hand, deficiencies on CoLA in particular are also common in ap- proaches that distill BERT into smaller architectures (Sun et al., 2019; Turc et al., 2019; Mukherjee et al., 2021), which might come with limited capacity for linguistic acceptability.\n\n6 C ONCLUSIONS\n\n We discuss how much performance a transformer-based language model can achieve when crammed into a setting with very limited compute, finding that several strands of modification, such as espe- cially training recipe and data setup lead to decent downstream performance on GLUE. Overall though, cramming language models appears hard and we believe this to be only the first step in this direction. Why is cramming hard? We discuss scaling laws in the low compute regime and find that cramming is hard because the conclusions of Kaplan et al. (2020) so easy to reproduce. 9\n\nR EPRODUCIBILITY S TATEMENT\n\n We provide code to reproduce all experiments.\n\nR EFERENCES\n\n 10\n\nMishra,\n\n\n\n, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n\n\n\nShi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\n\n\n\n, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\n\n\n\n- skaya, Sanjay\n\n\n\n, Sunipa Dev, Henryk\n\n\n\n, Xavier Garcia, Vedant Misra,\n\n\n\n, Liam\n\n\n\n, Denny Zhou,\n\n\n\n, David Luan,\n\n\n\nLim,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\n,\n\n\n\nIn Advances\n\n Information Processing Systems volume 33, . 4271\u20134282 sociates Inc. URL https://papers.nips.cc/paper/2020/hash/ 2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html 11 12 13 14 15\n\nNarang\n\n 1 . 16 arXiv:2005.09561 [cs, stat] , May 2020. URL http://arxiv.org/abs/2005.09561 . Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, Tao, Yu, Phuong Dao, Christopher Fifty, Zhifeng Chen, and Yonghui Wu. N Grammer: . arxiv:2207.06366[cs] , July 2022. doi: 10.48550/arXiv.2207.06366. URL http://arxiv. org/abs/2207.06366 . Christian Sarofeen, Piotr Bialecki, Jie Jiang, Kevin Stephano, Masaki Kozuki, Neal Vaidya, and Stas Bekman. Introducing nvFuser piler for PyTorch, August 2022. URL https://pytorch.org/blog/ . Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M. Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy \\ & https:// . / ?id=rI7BL3fHIZq . URL https://github.com/aks2203/ 17 : // openreview. net /forum? id =rJ4km2R5t7 . 18 19 //arxiv.org/abs/2102. 08098 .\n\nA A PPENDIX B L IMITATIONS\n\n In this work, we limited our investigation to transformer-based architectures trained with MLM objectives. However, we do think that the general task of cramming posed in Section 2 is interesting even when relaxing these constraints. There have been a number of modifications proposed to the objective in particular (Joshi et al., 2020; Bao et al., 2020; Bajaj et al., 2022; Tay et al., 2022b). While Artetxe et al. (2022) and Wang et al. (2022) find MLM still to hold up well as a pretraining objective, other suggestions such as ELECTRA (Clark et al., 2019; 2020; He et al., 2021) could be employed which might be beneficial for crammed models. Also, the optimal architecture might not be transformer-based (Merity, 2019; Fusco et al., 2022; Peng, 2021)\n\nC O THER M ODIFICATIONS\n\n A few recent developments not included in this study are Roy et al. (2022), Shen et al. (2022), and Mindermann et al. (2022). Modifications further not included in this study are more involved initial- ization (Zhu et al., 2021), additional objective modifications (M\u00a8uller et al., 2019), progressive growth (Gu et al., 2021; Shen et al., 2022), convolutional variants (Iandola et al., 2020; Chelombiev et al., 2021; So et al., 2021), sequence recurrence (Lei et al., 2022) and TUPE embeddings (Ke et al., 2020). 20", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper investigates how much performance can be achieved by training a transformer-based language model from scratch on a single GPU in just one day, a challenge the authors call \"Cramming\".", "topic_class_1": "Summay of paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They analyze nearly all components of the pretraining pipeline for this low-resource scenario and provide a modified pipeline with performance close to BERT.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They investigate why scaling down is hard and which modifications actually improve performance in this constrained setting.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors provide evidence that even with limited compute, performance closely follows scaling laws observed in large-compute settings.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Through the lens of scaling laws, they categorize recent improvements to training and architecture and discuss their merit and practical applicability for the limited compute setting.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper tackles an important and practical problem of training language models with very limited compute resources, which is relevant for most researchers and practitioners.", "topic_class_1": "Strengths on contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors perform a comprehensive analysis of the pretraining pipeline, architectures, training setup, and datasets to find the best configuration for the low-resource setting.", "topic_class_1": "Strengths on experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The empirical results showing that models trained in the limited compute regime can achieve performance close to BERT on GLUE tasks are convincing and significant.", "topic_class_1": "Strengths on results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Analyzing the results through the lens of scaling laws provides useful insights into what matters most for performance in the constrained setting.", "topic_class_1": "Strengths on contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the paper is very thorough, some additional baselines could have strengthened the evaluation, such as comparing to other efficient models like TinyBERT, MobileBERT etc.", "topic_class_1": "Concerns over adding additional baselines", "topic_class_2": NaN, "reliability": "No", "explanation": "TinyBert and MobileBert share similar structure with Bert, so they are not very necessary in this case", "error_type": "Inexpert Statement"}, {"segment_text": "- The poor performance on the CoLA task is not sufficiently explained and requires more analysis to understand the limitations of the crammed models.", "topic_class_1": "Concerns over results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Finetuning and evaluation is done only on the GLUE benchmark.", "topic_class_1": "Concerns over datasets used", "topic_class_2": NaN, "reliability": "No", "explanation": "For this paper, evaluation on GLUE is sufficient.", "error_type": "Out-of-scope"}, {"segment_text": "Expanding to a wider range of tasks would provide a more comprehensive assessment of the crammed models' capabilities.", "topic_class_1": "Explanations of why more baselines and datasets are needed.", "topic_class_2": NaN, "reliability": "No", "explanation": "It is true, but every paper can benefit from wide range of tasks and it is out-of-scope for this paper.", "error_type": "Out-of-scope"}, {"segment_text": "- More discussion on how the findings can guide future research on efficient training of large language models would be useful.", "topic_class_1": "Suggestions on adding more discussions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is clearly written and easy to follow.", "topic_class_1": "Strength on Writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The training setup, model architecture and evaluation are described in sufficient detail.", "topic_class_1": "Strength on Writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The empirical methodology is sound and the results are convincing in demonstrating the feasibility of cramming language models.", "topic_class_1": "Strength on results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The idea of compressing the training of BERT-like models in a single-GPU single-day setup is novel and tackles a problem not sufficiently addressed in prior work.", "topic_class_1": "Strength on motivation", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors plan to release code which would allow reproducing the results.", "topic_class_1": "Strength on code pulication", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments do not require any special hardware or data that is hard to obtain.", "topic_class_1": "Strength on design ", "topic_class_2": NaN, "reliability": "No", "explanation": "This applied to general papers, nothing special here", "error_type": "Superficial Review"}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a valuable contribution by demonstrating that it is possible to train transformer language models that perform close to BERT while significantly compressing the time and compute requirements to a single GPU and a single day.", "topic_class_1": "Summary of review", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Through a comprehensive empirical analysis, the authors provide a training recipe that achieves this goal.", "topic_class_1": "Summary of review", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also provides useful insights into what matters most for model quality in this constrained setting by examining the results through the lens of scaling laws.", "topic_class_1": "Summary of review", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The limitations are that evaluation is restricted to GLUE, and poor CoLA performance needs more analysis.", "topic_class_1": "Summary of limitations", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a significant result that would be of interest to the NLP community.", "topic_class_1": "Summary of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some suggestions to further strengthen the paper would be to include additional efficient model baselines, expand evaluation beyond GLUE, and discuss more how the findings can guide future work on efficient training of large language models.", "topic_class_1": "Summary of suggestions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical novelty and significance": " 3", "Empirical novelty and significance": " 3", "Flag for ethics review": " NO.", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the feasibility of training a transformer-based language model from scratch on a single GPU within a day, aiming to achieve performance close to BERT's.", "topic_class_1": "Summay of paper topic", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By re-analyzing and modifying the pretraining pipeline components, the study examines the challenges of scaling down, identifies modifications that enhance performance in this constrained setting, and evaluates the impact of these changes on downstream tasks.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The findings indicate that performance in such a limited compute environment closely follows the scaling laws observed in larger-compute settings.", "topic_class_1": "Summay of findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research contributes to the understanding of efficient language model training and opens up opportunities for further academic inquiries with modest computational resources.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Innovative Approach: The paper creatively addresses the challenge of resource-efficient language model training, providing a valuable perspective opposite to the prevailing trend of scaling up.", "topic_class_1": "Strength on motivation", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive Analysis: It thoroughly investigates various components of the training pipeline and their impact on model performance, offering deep insights into efficient training strategies.", "topic_class_1": "Strength on analysis", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical Evidence: The study is well-supported by empirical results, showing that models trained under these constraints can achieve respectable performance on downstream tasks, close to that of BERT.", "topic_class_1": "Strength on experiment", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited Scope: The research mainly focuses on transformer architectures and the MLM objective, which may overlook potential benefits from other models or training objectives.", "topic_class_1": "Weakness on limitation", "topic_class_2": NaN, "reliability": "No", "explanation": "BERT is a transformer based model and it is pre-trained on the MLM objectivte. BERT has been the default baselines for many NLP tasks nowdays. Therefore, it is crucial and beneficial for researching resource-efficient model training for BERT. This contribution itself is significant enough.\n\nThis is the main model and main objective used by community. ", "error_type": "Inexpert Statement"}, {"segment_text": "Generalization of Findings: While the study provides valuable insights into scaling down, the specific modifications and their impacts might not generalize across different datasets, tasks, or larger computational settings.", "topic_class_1": "Weakness on generalization of the findings", "topic_class_2": NaN, "reliability": "No", "explanation": "Nothing insightful, can be applied to many papers.", "error_type": "Superficial Review"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-structured and clear, with a detailed explanation of the methodology and findings.", "topic_class_1": "Strength on Writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the complexity of some descriptions might require prior knowledge in the field for full comprehension.", "topic_class_1": "Weakness on require prior knowledge", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is of high quality, with a rigorous experimental setup and analysis that contribute to the understanding of efficient language model training.", "topic_class_1": "Strength on quality", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The study introduces novel insights into scaling down language models effectively, challenging the prevalent scaling-up paradigm and offering a new direction for research with limited computational resources.", "topic_class_1": "Strength on novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper includes a reproducibility statement and provides sufficient details for others to replicate the experiments, which is crucial for validating and building upon the findings.", "topic_class_1": "Strength on reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents an insightful investigation into training transformer-based language models with limited computational resources, demonstrating that it is possible to achieve performance close to BERT's under such constraints.", "topic_class_1": "Summary of paper", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research's strengths lie in its innovative approach, comprehensive analysis, and empirical support, though its scope and generalizability of findings could be expanded.", "topic_class_1": "Summary of strengths", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, the study makes a significant contribution to the field of efficient language model training and opens up new avenues for research with modest computational budgets.", "topic_class_1": "Summary of contributions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the feasibility of training a transformer-based language model from scratch on a single consumer GPU within one day, a challenge the authors call \"Cramming.\"", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper explores various modifications to the pre-training pipeline and analyzes their impact on downstream performance on the GLUE benchmark.", "topic_class_1": "Summay of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors find that while scaling down is inherently difficult due to the nature of scaling laws, certain adjustments to the architecture, training setup, and dataset can lead to models with performance approaching that of BERT.", "topic_class_1": "Summay of findings", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel and timely research question: This work challenges the dominant paradigm of scaling language models by exploring the potential of limited-compute training.", "topic_class_1": "Strength on motivation", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This opens up possibilities for researchers with restricted resources and facilitates investigations into aspects like interpretability and security.", "topic_class_1": "Strength on motivation and possible follow up work", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Extensive experimentation: The authors systematically analyze various architectural, training, and data-related modifications, providing valuable insights into their effectiveness in the low-resource setting.", "topic_class_1": "Strength on experiments", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Promising results: The Crammed LM achieves surprisingly good performance on GLUE, often nearing or even surpassing BERT, despite the significant resource constraints.", "topic_class_1": "Strength on results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited exploration of alternative objectives: While the paper focuses on MLM, other pre-training objectives like ELECTRA could potentially be more beneficial for cramming and warrant investigation.", "topic_class_1": "LImitation on on training with one objective", "topic_class_2": NaN, "reliability": "No", "explanation": "BERT is trained with MLM objective so it is natural to train on the same objective instead of exploring other pbjectives.\n\nMLM is the main loss used. Other objectives such as ELECTRA are not commonly used, and should not be a reason for rejection", "error_type": "Inexpert Statement"}, {"segment_text": "Uncertainty regarding the impact of downstream corpora: The paper primarily focuses on the effect of data processing and filtering, leaving the question of whether using downstream corpora for pre-training significantly contributes to the results.", "topic_class_1": "Concerns over impact of downstream copora", "topic_class_2": NaN, "reliability": "No", "explanation": "For pretraining, it is common practice to only use general corpus such as wikipeida.", "error_type": "Inexpert Statement"}, {"segment_text": "Lack of zero/few-shot evaluation: Given the paper's focus on knowledge transfer in low-resource settings, evaluating the Crammed LM on zero/few-shot tasks would provide valuable insights.", "topic_class_1": "Concerns over missing results", "topic_class_2": NaN, "reliability": "No", "explanation": "Zero/few-shot is generally used for decoder-only LLMs.", "error_type": "Inexpert Statement"}, {"segment_text": "CoLA performance: The significant drop in performance on CoLA suggests potential limitations of the Crammed LM, either in terms of hyperparameter sensitivity or the ability to capture linguistic acceptability with limited training data.", "topic_class_1": "Concerns over results", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is generally well-written and clear, although the discussion of scaling laws could be made more accessible to a broader audience.", "topic_class_1": "Strength on Writing", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-motivated, and the experiments are designed and conducted rigorously.", "topic_class_1": "Strength on quality", "topic_class_2": NaN, "reliability": "No", "explanation": "The reviewer pointed that there are limitations in weaknesses but here stated the experiments conducted rigorously.", "error_type": "Contradiction"}, {"segment_text": "Novelty: The research question and the specific approach are novel and contribute to the understanding of low-resource language model training.", "topic_class_1": "Strength on novelty", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors provide code to reproduce the experiments, enhancing the transparency and reliability of the findings.", "topic_class_1": "Strength on reproducibility", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section Title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a valuable contribution to the field of NLP by exploring the potential of training language models with limited compute resources.", "topic_class_1": "Summary of paper", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The Crammed LM shows promising results on GLUE, suggesting that achieving BERT-like performance with modest resources is possible.", "topic_class_1": "Summary of contribution", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, further investigations are needed to address the weaknesses identified and explore the full potential of cramming language models.", "topic_class_1": "Suggestions", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Mapping Language Models to Grounded Conceptual Spaces", "body_text": "\n\nM APPING L ANGUAGE M ODELS TO G ROUNDED C ON - CEPTUAL S PACES\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n A fundamental criticism of text-only language models (LMs) is their lack of grounding \u2014that is, the ability to tie a word for which they have learned a rep- resentation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, gener- ate \ufb02uent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed re\ufb02ects the conceptual structure of the non-linguistic world\u2014which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual do- main (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word \u201cleft\u201d means using a textual depiction of a grid world, and assess how well it can gener- alise to related concepts, for example, the word \u201cright\u201d , in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations \u201cfrom scratch\u201d, it is pos- sible that large text-only models learn a suf\ufb01ciently rich conceptual structure that could allow them to be grounded in a data-ef\ufb01cient way.\n\n1 I NTRODUCTION\n\n Large pre-trained language models (LMs) trained on text corpora have shown remarkable progress on a range of natural language understanding tasks (Radford et al., 2019; Brown et al., 2020). Such models have demonstrated their ability to generate \ufb02uent dialogue (Brown et al., 2020), make com- monsense inferences Zellers et al. (2019), and reconstruct taxonomies and word relations (Chen et al., 2020). However, it has been argued that true meaning cannot be learned from the form of language alone (i.e., from text) because it is a word\u2019s use in the non-linguistic world that imparts it meaning (Bender & Koller, 2020; Bisk et al., 2020). For example, although LMs might learn from textual co-occurrences that the words north and south are opposites, the grounded meaning of these words, i.e., the direction that you should travel if you are told to go north , is something which these models, by de\ufb01nition, do not have access to during training. While it is indisputable that text-only models do not learn representations of concepts that are grounded in the non-text world, it is possible for the structure of relations between concepts in text form to be identical to what a grounded model would learn. In principle, it is therefore possible for a text-only model\u2019s conceptual space to be isomorphic to the \u201ctrue\u201d, grounded conceptual space (Merrill et al., 2021). In this work, we investigate whether this is the case by asking whether models can learn to ground an entire domain (e.g., direction) after grounding only a subset of the points in that domain (e.g., left ). Speci\ufb01cally, for generative LMs that have been trained only on large text corpora, we \u201corient\u201d the models by showing them how some word forms (that they have learned during training) are used in simple text worlds\u2014for example, what the direction north maps to in a textual representation of a grid world (see Figure 1). We then evaluate two types of generalisation. First (\u00a73.1), we evaluate generalisation to unseen worlds. For example, if the model has seen several 1 realisations of the word north in different grid worlds, can it correctly identify north in an unseen world (e.g., one of a different size or shape)? Second (\u00a73.2), we evaluate generalisation to unseen but related concepts. For example, if the model has been shown grounded representations of north and east , can it correctly identify south and west , even though it was never shown to them? We \ufb01nd that although the small language models (GPT-2 models that contain an order of 100M param- eters) cannot perform either generalisation well, the largest model (a GPT-3 model containing 175B parameters) can indeed learn groundings in the conceptual worlds we build. We analyse the predic- tions and errors made by models (\u00a73.3) and \ufb01nd that the errors made by the small models often come from them struggling to produce in-domain references and defaulting to random words. In contrast, the errors made by the largest model are quite intuitive, as they predict in-domain concepts that are often close to the true concept in the world (e.g., maroon versus dark red ).\n\n2 E XPERIMENTAL D ESIGN 2.1 M ODELS\n\n We test \ufb01ve autoregressive Transformer language models (Vaswani et al., 2017) of varying size, speci\ufb01cally the GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) models. The small- est model we use contains 124M parameters, and the others follow increasing model sizes (355M, 774M, 1.5B and 175B parameters). All models are pre-trained on the OPENAI-WT dataset, com- posed of 40 GB of English web text available on the internet. We generate up to 5 tokens per prompt and, to improve the robustness of our analyses, generate 3 samples for every prompt. We use a temperature of 1 during generation and sample from the softmax probabilities produced at each time step using nucleus sampling (Holtzman et al., 2019) with p = 0.85. We include detail on all model aspects, as well as discussion on their training data in Appendix A.\n\n2.2 I N -C ONTEXT L EARNING\n\n Several studies (Brown et al., 2020; Reynolds & McDonell, 2021) have shown that instead of \ufb01ne- tuning generative LMs\u2013i.e., a process that updates parameters learned by the model during pre- training, it is possible to achieve competitive performance by giving the model a small number of training examples within the prompt . This is often referred to as \u201cin-context learning\u201d or \u201cfew- shot prompting\u201d. Speci\ufb01cally, a prompt includes n task examples that include a question pre\ufb01x (e.g., \u201cWorld:\u201d ) followed by the question, and an answer pre\ufb01x (e.g., \u201cAnswer:\u201d ) followed by the answer to the question. After giving the model n examples in this manner, the prompt ends with a new question and only an answer pre\ufb01x after which the model is expected to generate an answer to the last question, following the prompt format it has seen. By enumerating over all questions in the test set, we can obtain a model-generated answer for every test set question that we wish to evaluate. There are no gradient updates to any model parameters using this approach. RGB: (255, 0 0) Answer: red RGB: (143, 0, 255) Answer: violet Spatial Terms Cardinal Directions RGB Colours World: [0. 0. 0. 0. 0.] [0. 0. 0. 1 . 0.] [0. 0. 0. 0. 0.] Answer: right World: [0. 0. 1 .] [0. 0. 0.] [0. 0. 0.] Answer: northeast FINAL\n\n2.3 G ROUNDED C ONCEPT D OMAINS\n\n The models that we test, by construction, can receive only text inputs. Thus, we focus on a set of grounded domains for which it is possible to faithfully represent the grounded meaning in text form. We brie\ufb02y describe these domains below, summarise them in Figure 1, and describe in detail, 2 the prompt creation process for each generalisation task in Sections \u00a73.2 and \u00a73.1. We discuss the possibility of expanding this set of concepts in future work in Section \u00a74. Spatial Terms We consider 6 spatial concepts: left, right, up, down, top, bottom . Each of the above concepts can be represented in a grid world using the position of a special character (here, a \u2018 1 \u2019) 1 in the world. To do this, we create grid world environments of varying sizes (where the number of rows and columns ranges from 1 to 8 ), where each world consists of \u2018 0 \u2019s and a single \u2018 1 \u2019. Cardinal Directions We consider eight cardinal directions: north, south, east, west, northeast, northwest, southeast, southwest . These are similar to spatial terms, except that they include compo- sitional terms (e.g., northeast ). We use grid-worlds of the same format as spatial terms to represent these concepts. Colour Terms We consider colour terms in a three-dimensional space, using a dataset of 367 RGB colours (Abdou et al., 2021) that contains colour names (e.g., red, cyan, forest green ) each associated with an RGB code (e.g., (255 , 0 , 0) ). Therefore, the world representation in this case is not a grid-world, but an RGB code associated with every colour name. Figure 1 shows example RGB codes and colours that serve as part of a prompt given to the models we test.\n\n2.4 R OTATED W ORLDS TO C ONTROL FOR M EMORISATION\n\n Motivation The GPT-x models that we use have been trained on the CommonCrawl corpus (Rad- ford et al. (2019)), a collection of documents that contains web text freely available on the internet. Since we provide instantiations of grounded concepts in text form, it is very plausible that the do- mains described above have been encountered verbatim during training. For example, for spatial terms such as left , a model might have seen instances of matrices and linear algebra terms with the word left in close proximity; for colour terms, tables that map RGB colour codes to colour names are pervasive in web-text. We therefore include a control task in our experimental setup such that the model cannot succeed using simple memorisation. Rather, success on a task requires the model to truly perform a conceptual mapping between the ungrounded and grounded representations. Isomorphism as a Control We use the concept of isomorphism to control for memorisation. In- tuitively, imagine a situation where you are lost in the woods. Once pointed in the direction of north, you instantly know which way is south. However, this ability is not dependent on having been correctly pointed north\u2013if someone were to incorrectly point east and tell you this was north, you would readily infer west to be south. This is because your reasoning depends on your knowledge of the relation between north and south, and between north and the world, rather than on having memorized the \u201ctrue\u201d grounding of each concept independently. By the same logic, if a model is learning a grounding function, this should be dependent on the world that it is being grounded in. For example, in different worlds where the word red grounds to different points in space, the word blue , by analogy, shares a \ufb01xed conceptual relation to the word red . Therefore, it should ground in correspondingly equidistant ways in the two worlds. A model\u2019s ability to learn two different grounding functions f vs. g , should not be dependent on what the actual points ground to, as long as the structural relations between concepts in the space are preserved. Further, this should hold for all such isomorphic transformations that preserve the structure of the space, and importantly, it should not hold for random perturbations that distort the structure of the space, since such distortions would break the assumption that the relation between red and blue in ungrounded space is analogous to that in grounded space. Mechanism In the colour domain, since colour concepts exist in a 3D world of RGB codes, we rotate each point around a \ufb01xed axis by a certain degree to create a new isomorphic world. We repeat this control three times (for 90 \u25e6 , 180 \u25e6 and 270 \u25e6 rotations) and average over the rotations in our evaluations. For cardinal directions, we rotate each cardinal concept by 90 \u25e6 in two dimensions, and do this three times and average over rotations. Since the spatial terms exist as pairs, we simply swap the groundings of alternate terms ( e.g., left and right ). For random worlds that do not preserve 1 As a control, we perform the same experiments by varying the special character, e.g., replacing a 1 with a 2 or replacing them with letters such as a or b . This does not effect our results, see Appendix B 3 the structure between word forms, we randomly assign a concept name (e.g., red ) to a point in the world, and we do this for all concept names and categories to obtain a random world for each. Figure 2 shows example transformations of colours on rotating by 90 \u25e6 , as well as randomly rotating points.\n\nIsomorphism and Rotated Worlds\n\n blue\n\n2.5 E VALUATIONS\n\n Experimental Logic We report model performance in three settings: the original (\u201ctrue\u201d) world (e.g., that in which red maps to the actual RGB code for red), an average over three rotated worlds (e.g., worlds in which red maps to some other RGB code, but relations between colors are pre- served), and a random world (in which relations between mappings are not preserved). If a model is performing the conceptual mapping in the desired way, we expect that performance should be high in the true world and that there should be no signi\ufb01cant degradation in performance when moving to rotated worlds. We also expect that performance should be low in the random world. Metrics When given a prompt, a generative LM is free to generate any number of tokens until having generated the EOS token that halts further generation. Since classi\ufb01cation tasks usually correspond to labels that contain only a few words, the standard approach is to let the model generate an entire sequence of text and to then cut off the generation to the \ufb01rst n tokens (where typically n < 10 ). From the prompting mechanism, the model should learn to follow the prompt format to generate one label (e.g., instead of every label in succession), and since it does not receive any gradient updates during this learning, there is no incentive for it to predict all related labels within a n -token generation. We see that this is true in practice and show example generations in Appendix E. We set n = 5 and then report the following metrics.\n\nT OP -1 A CCURACY\n\n If the ground-truth answer or any substring thereof 2 lies in the generated an- swer (i.e., the \ufb01rst n tokens of the full generation), the model gets perfect accuracy. If the ground- truth answer does not exist in the generated answer, or exists in the generation outside of the cutoff limit, the model gets a 0 accuracy. For example, for the ground truth answer deep tuscan red , if the model generated answer is tuscan red or red the model gets a perfect accuracy, but if the model generated answer is deep red or wine or vermilion , the model gets an accuracy of 0.\n\nT OP -3 A CCURACY\n\n This metric is analogous to Top-1 except that instead of only considering the most probable generation from the model, we collect the second and third most probable answer sequences as well. The model gets a perfect accuracy if the correct answer exists in any of the three generated answers. If the correct answer does not exist in any of the 3 generated answers, or exists in any of the generations outside of the cutoff limit, the model gets an accuracy of 0. 2 We report results on exact match, instead of substring matches, for the best-performing model as well. 4\n\nG ROUNDING D ISTANCE\n\n For analysis purposes (\u00a73.3), we wish to assess how far off models are, in cases where they are wrong. For this, we need to quantify the distance between the model\u2019s predicted answer and the ground truth answer. For example, in the colour domain, the distance between two points can be computed as the Euclidean distance between two RGB codes. For every answer generated by the model (e.g., the word pink in the colour domain), if the answer is an acceptable grounded term that exists in the world, we can compute its distance to the true grounding (e.g., the colour red ). However, if the generated answer was an unrelated word (e.g., the word cat ) that does not exist in the same domain, no distance can be computed in the world. Therefore, we calculate a distance metric as the Euclidean distance between two points in space when the generated answer falls in the domain. When the generated answer does not fall in the domain, we set the distance to a number signi\ufb01cantly higher than the largest distance between two in-domain concepts. We provide equations and details on calculation of this metric in Appendix C. Baselines Given that the language models are free to generate any word that exists in their vocab- ulary as a potential answer, we choose two random baselines over vocabulary words against which to compare model performance. We use R-IV (i.e., random in-vocabulary) to denote a baseline that randomly selects from among all words in the model\u2019s vocabulary. We use R-ID (i.e., random in- domain) to denote a baseline that randomly selects from amongst only the in-domain words (e.g., from colour terms); this is 6, 8 and 367 words respectively for the spatial, cardinal and colour cate- gories. Note that the generative LMs do not have such a domain restriction over words, as they are free to choose any token from the full vocabulary (like R-IV), thus making R-IV, a fairer baseline than R-ID.\n\n3 R ESULTS 3.1 G ENERALISATION TO U NSEEN W ORLDS\n\n Our \ufb01rst investigation looks into how well models generalise concepts that have been taught to them to unseen worlds. For example, if a model has seen a few examples of a concept (such as left ) as depicted in some grid worlds, can it correct identify an instance of the same concept in a different grid world (e.g., one with a different size or orientation)? Note that we can only conduct this type of evaluation in the spatial and cardinal domains, since, for the colour domain, there is only ever one world, with one grounding for each concept (e.g., the colour red has exactly one grounding to a 3-digit RGB code in a 3-D RGB space). Data We create prompts that include 20 examples of grounded concepts in a set of grid worlds. For each domain (e.g., cardinal directions that contain 8 concepts, and spatial terms that contain 3 pairs of 2 concepts), we include a (roughly) equal sample of concepts among these 20 examples. 5 Then, for held-out grid worlds, we append it to the end of the prompt to test whether or not models generate the correct concept label for these unseen worlds. Figure 3 shows example prompts given to the model and example generations from three different models. We report results averaged over all generations in Table 1. In general, we see the desired trend in which model performance on the original and rotated worlds is well above that on the random world. We do not see consistent or signi\ufb01cant performance degradation when moving from the original to the rotated world, suggesting performance is not due to simple memorisation. Comparing across models, we see that the smaller models struggle to even learn concepts that were taught to them in a few-shot manner. For example, for the spatial category, the performance of the smallest model is below that of a baseline which guesses randomly among in-domain words, suggesting the model even fails to learn the general domain of the task (discussed more in Section 3.3). In contrast, the largest model (GPT-3), has a 45% Top-1 accuracy and a 76% Top-3 accuracy for the spatial category.\n\n3.2 G ENERALIZATION TO U NSEEN C ONCEPTS\n\n Our primary interest here is in the model\u2019s ability to map its ungrounded conceptual structure to a grounded world. Speci\ufb01cally, we want to see that the model is able to ground an entire conceptual domain when only taught how to ground a small subset of the points in that domain. To test this, we show models example concepts in a domain (e.g., north, east ), while holding out other concepts (e.g., south, west ). We then test them on instances of held-out concepts. Figure 4 shows, for the colour domain, an example prompt with training examples and the associated held-out concept. 6 Data For the spatial and cardinal domains, for every concept we wish to test (e.g., left ) we create prompts that contain 20 examples of all the remaining n \u2212 1 concepts in the domain. We ensure that each concept appears at least once in the prompt. Then, we append a grid-world containing an instance of the held-out concept and evaluate whether models can generalise to this concept. For each concept, we do this for grid-worlds of different sizes and then average over concepts to report one metric of generalisation performance. For the colour domain, since the total number of concepts (colours) are too large to hold-out just one concept, we include only 60 examples in a prompt and average results over the remaining colours. Here, the prompt contains 3 primary and 3 secondary colours, as well as 57 other randomly selected colours. For each held-out colour (i.e., one that does not appear in the prompt that the model is \ufb01ne-tuned on), we append the RGB code to the end of the prompt, to assess how well the model can generalise to unseen colours. We report results on all concept categories in Table 2. As before, we see that models do not appear to be exploiting simple memorisation (evidenced by similar performance in original vs. rotated worlds) and that only the largest models appear capable of getting reasonable performance on the task. That said, the largest GPT-3 model achieves impressive results given the dif\ufb01culty of the task. For example, it achieves over 40% Top-1 accuracy in the color domain, which requires generating a label like violet despite having never seen such a label during training (Figure 4). Original Rotated Random Original Rotated Random Original IV 0 0.00 0.00 0 00 0 00 0 00 0 00 0.00 0.00 ID 0.16 0.16 0.16 0.13 0.13 0.13 0 00 0.00 0.00 124 M 0 10 0.11 0 04 0 10 0 10 03 0 09 . We report both Top-1 and Top-3 accuracy. R-IV refers to a random baseline that randomly selects a word out of the total vocabulary, R-ID refers to a baseline that randomly selects a word out of the set of words in the concept category.\n\n3.3 E RROR A NALYSIS\n\n Given the results above, we investigate \u201chow wrong\u201d models are when they fail to produce the expected ground-truth label. One clear trend of the large model is the ability to generate \u201con topic\u201d (i.e., in-domain) responses, regardless of whether or not those responses are correct. For example, if the expected response to an input is \u201cleft\u201d , a model which produces right is wrong in a very different way than a model that produces nonsensical outputs such as \u201c[0[0\u201d or function words such as \u201cthe\u201d (see Figure 3). We see that the largest 175B parameter model almost always produces in-domain answers. We evaluate this by checking whether the generation lies in the set of related words for that category (e.g., all colours, spatial, cardinal words respectively). We see that the smaller models fail to do this i.e., their generations tend to be unrelated words that might have had high prominence (e.g., function words). On evaluating accuracy of generations being \u201cin-domain\u201d, we see that the smallest model has only a 53% accuracy while the largest has a 98% accuracy of generating in- domain answers. 7 Second, we evaluate grounded distance (\u00a72.5) to measure the degree of correctness of a response. There are often multiple reasonably correct ways to label a given instance of a grounded world. For example, in the colour domain, the colours dark red and wine are close enough in space that they might be intuitive alternate answers for one another. However, our top-1 and top-3 metrics only assess strict matches and do not account for this. Thus, we look at the distance between the color denoted by the predicted label (when it exists, i.e., when the model generated a legitimate color name) and the color the model was asked to label. The lower this distance is, the \u201cless wrong\u201d the model\u2019s prediction is. Table 3 reports evaluations measured by grounding distance for the colour domain. For every test instance, we compute the distance between the model predicted grounding and the true grounding as de\ufb01ned in Equation C.3. We then average over all computed distances to report one number that tells us how close, on average, model predictions are to the true grounding in the world. We show example visualisations of colours in RGB space that lie within a certain distance threshold of each other. We see that the distances of predicted groundings from the true groundings re\ufb02ect the models\u2019 ability on the task as shown in Tables 1 and 2 and offers intuitive insights into the models reasoning process. Speci\ufb01cally, for the largest model, the distances between model-predicted groundings from the true grounding are signi\ufb01cantly lower than random.\n\n4 D ISCUSSION\n\n Our empirical results suggest that very large LMs (speci\ufb01cally, GPT-3), even when trained only on text, can learn to ground words to conceptual spaces to some degree. The fact that these models succeed even in isomorphic rotated worlds, suggests that these models are not succeeding via naive memorisation. Rather, this suggests that they may be exploiting something about the conceptual structure of the space learned from text in order to map onto a new space that was not explicitly encountered during training. A limitation of our approach however, is that there are some grounded concepts (e.g., visual and sensory inputs) that cannot be easily encoded in text form. By construction, the LMs that we use are restricted to text-only inputs, thus our focus is on domains (e.g., colours and directions) that have a well-de\ufb01ned world-space. This is only a small set of all the potential grounded concepts we would wish to teach LMs. Although many forms of data can be coerced into text format (e.g., we represent color using discrete digits to represent RGB space), complex concepts may loose fundamental as- pects of their meaning when represented in this way. For example, for color, a coarse-grained notion of numeric proximity, derivable from text (Wallace et al., 2019; Naik et al., 2019), may be suf\ufb01cient to differentiate the concepts we explore, but for more complex visual inputs (e.g., the output of a CNN image encoder), a text-based numeric representation is unlikely to capture the necessary de- gree of nuance. Future work would need to consider ways of adapting GPT-3-like models to accept non-textual inputs while still exploiting the text-based conceptual structure. If such limitations were addressed, our results are suggestive of a potentially promising way in which text-only training could support general purpose, grounded models of meaning. Speci\ufb01cally, our results imply that the conceptual space a model learns from text might be nearly isomorphic to what it would learn from interacting in a grounded world, and that models can be taught to map between those conceptual spaces without requiring explicit grounding for every concept. This is 8 exciting, as domain-general text corpora are readily available, while domain general multimodal corpora\u2013e.g., containing suf\ufb01cient information on abstract concepts such as emotions ( happy ) or time ( used to )\u2013might be dif\ufb01cult or impossible to collect. If models like GPT-3 could be adapted to receive non-text prompts (as discussed above), our results suggest that the rich conceptual structure such models learn from text could be bootstrapped into powerful grounded models of language.\n\n5 R ELATED W ORK\n\n There is a signi\ufb01cant amount of work that focuses on understanding how language models represent and reason about concepts in the world, as well as work that directly attempts to build models that take text inputs and ground them to elements in the world. We situate our work within these two bodies of literature: one that investigates how LMs understand linguistic phenomena and word meaning, and another, that attempts to connect models of language to models of the world in order to solve tasks. We describe each body of work below. Meaning and Understanding in LMs With the advent of large LMs of increasing orders of mag- nitude, there has been increasing speculation on the capabilities of such models, and whether they truly understand the meaning of the words they are learning representations for. Several works that attempt to probe linguistic phenomena in LMs, show that the representations learned by such models encode syntactic dependencies and coreference information (Tenney et al., 2019; Hewitt & Manning, 2019) and word-sense information (Chen et al., 2020). Work that investigates the abil- ity of models to form word associations, \ufb01nds that large LMs can indeed perform such a task; for e.g., Hwang et al. (2020) reports that GPT-3 produces such tuples with 73% accuracy, suggesting that pre-trained LMs not only recognize that entities are related, but can differentiate how they are related. In this work, we speci\ufb01cally investigate whether or not such models can learn to ground conceptual domains (for e.g., a world of colours or spatial directions) and analyse when and where they fail to do so. Natural Language Grounding There is an increasing amount of work, usually at the intersec- tion of NLP and \ufb01elds like vision and reinforcement learning, that aims to use natural language to instruct agents about aspects of the world. In the case of vision, this could be in order to learn corre- spondences between language descriptions and pixels in an image (Kiela et al., 2018), for example, to learn how to caption an image (Vinyals et al., 2014). In the case of reinforcement learning or robotics, this could be to build agents that understand natural language instructions in order to take actions in a world that follow the instruction\u2014for example to navigate to a goal (Artzi & Zettle- moyer, 2013), or to solve tasks in different languages (Ku et al., 2020). Most of these tasks focus on training LMs from scratch, however usually with inputs that contain both textual information as well as grounded world information. Our work is different in that we attempt to take an LM that was previously trained only on text, and attempt to teach it a concept in the world without re-training it. With only a few samples of what the concept grounds to, we investigate how well large LMs can use the structure of language and associations between word forms in order to generalise to grounded concepts.\n\n6 C ONCLUSION\n\n This work investigates the extent to which large language models, trained only on text can be taught to map previously learned word forms onto conceptual worlds. We investigate several different state-of-the-art generative language models in colour and direction domains, represented as discrete grid worlds given to the model as text input. On showing models a few examples of such grounded instances, we see that although smaller models struggle to generalise from text to grounded concepts, the largest model does indeed learn the space of concepts that we test it on. We analyse where the models fail and see that although the smallest models often produce random, unrelated outputs, the errors made by the largest model are quite intuitive, for example, predicting colours very close in space to the true grounding. We discuss the limitations of focusing on text-only inputs to teach models grounded concepts, as well as the implications of such work, that can allow large language models to be adapted, in a data-ef\ufb01cient way, to points grounded in the world. 9\n\nR EFERENCES\n\n 10 ,\n\nA PPENDIX\n\n We provide, as supplementary material, additional information about the data generation, models used, examples of model generations, as well as additional results across all models.\n\nA M ODELLING D ETAILS\n\n We use a GPT-3 model Brown et al. (2020) and four GPT-2 Radford et al. (2019) models from the Hugging Face Transformer Wolf et al. (2019) library. Each of these is a pretrained autoregressive transformer model, trained on the OpenWT corpus, containing around 8 million documents. The top 15 domains by volume in WebText are: Google, Archive, Blogspot, GitHub, NYTimes, Wordpress, Washington Post, Wikia, BBC, The Guardian, eBay, Pastebin, CNN, Yahoo!, and the Huf\ufb01ngton Post. Individual model parameters and layers are shown in Table 5. The pretrained models use byte-pair encoding (BPE) tokens Sennrich et al. (2015) to represent frequent symbol sequences in the text, and this tokenisation is performed on all new input prompts to generate text from the model. We report the hyperparameters used by the pretrained model in Table 6.\n\nB D ATA G ENERATION\n\n In this section we describe how we create prompt data for the concept categories we wish to evaluate. We describe what the \u201cworlds\u201d look like for each category, as well as detail on the generation of 11 such worlds and division into train and test splits, for colours in Section B.1 and for both spatial and cardinal concepts in Section B 2.\n\nB.1 C OLOUR C ONCEPTS\n\n We draw from an existing dataset containing RGB codes associated with colour names, for 367 colours. In this concept category, a grounded realisation of a colour in the world, is simply its RGB code i.e., the point that it grounds to in 3-dimensional colour space. Therefore, all grounded examples in prompts follow the format of RGB: (x, y, z) followed by Colour: concept name , where the items in red are replaced with instances of RGB codes and names from the dataset of colours we useAbdou et al. (2021). This gives us a total of 367 samples of RGB codes paired with colour names. We create training and testing splits for different generalisation evaluations in the following way.\n\nB.1.1 G ENERALISATION TO U NSEEN C ONCEPTS\n\n Random Split We create prompts to perform this experiment in two ways. The \ufb01rst, as reported in \u00a73.2, creates a \u201ctrain split\u201d i.e., samples within the prompt, by \ufb01rst selecting the 3 primary and 3 secondary colours to always be in the prompt. We then sample 64 other colours from the set of colours to be part of the train split. The prompt therefore contains 70 samples of the question and answer pre\ufb01xes followed the RGB codes and colour names. For every sample in the test set, we create a new prompt that appends a question pre\ufb01x and the RGB code of that sample to the end of the prompt, followed by the answer pre\ufb01x (with no answer). For each prompt, the model is then required to generate an answer. Figure 4 shows the random sample of colours in the prompt and we report results on these in the main paper in Table 2. Sub-space Split We then perform another experiment to evaluate a models ability to learn con- cepts within a sub-space, and generalise to a whole new sub-space. Here, the prompt contains 3 primary and 3 secondary colours, as well as 57 other colours that we select in the following way. For each of the 6 primary and secondary colours, we consider a sub-space in the world to be a space of colours that lies within a Euclidean distance of 150 from that colour. 3 Since the model has seen the 6 primary and secondary colours, as well as a certain space of the colour world, we wish to evaluate how well the model can generalise to a new sub-space of the world, by using the colour word-form associations. We then report generalisation results averaged over the 6 sub-spaces. Fig- ure 5 shows the sample of colours within a sub-space centered around the colour red, that serve as training samples within the prompt. We report results on these in Appendix Table 9. 3 We show visualisations of such colour sub-spaces in the Appendix. 12\n\nB.2 S PATIAL AND C ARDINAL C ONCEPTS\n\n We consider a \u201cworld\u201d to be a 2D matrix \ufb01lled with 0s and containing exactly one 1, where the posi- tion of the 1 (i.e., the determining object) refers to the concept of interest (for e.g., a spatial location like \u201cleft\u201d or a cardinal direction like \u201cnorth\u201d . We create grid worlds \ufb01lled with 0 s and one 1 , of row and column ranges from (1 , 8) giving us 672 total grid worlds. 4 For each grid world, all grounded examples follow the format of World: [1. 0. 0. 0.] followed by Direction: concept name . where the items in red are replaced with instances of different grid worlds and corresponding concepts, based on the location of the 1 . We create training and testing splits for different generalisation evaluations in the following way.\n\nB.2.1 G ENERALISATION TO U NSEEN W ORLDS\n\n Our primary investigation here is to assess whether the models can generalise to new worlds that differ in size, or location of the determining object. Each of the 672 worlds we create differs from one another in either aspect i.e., either the size of the world is different, or, when worlds are of the same size, the location of the 1 is in a different position. Therefore, we randomly sample 20 worlds that contain instances of concepts to serve as part of the prompt. The prompt therefore contains a question pre\ufb01x followed by the grid world on a new line, followed by the answer pre\ufb01x and the concept name. We ensure that there is a roughly equal distribution of concepts in the train split (e.g., if there are 8 concepts split over 20 samples, we ensure that 2 of each concept exist in the prompt, and then \ufb01ll the remainder of the prompt by randomly sampling from the concepts). We then create a new prompt for every sample in the test set by appending the world representation and answer pre\ufb01x to the end. We report results on this in the main paper in Table 1.\n\nB.2.2 G ENERALISATION TO U NSEEN C ONCEPTS\n\n Since we wish to assess a model\u2019s ability to generalise to new concepts, here, we hold out concepts in the test set. Speci\ufb01cally, for every domain contain n concepts (e.g., 8 for cardinal directions), the train split contains an equal distribution of n \u2212 1 concepts. We then test on samples that contain the last held-out concept, as well as the seen concepts, to assess how well the model has learned seen concepts, and generalises to unseen concepts. We report results on this in the main paper in Table 2. 4 We exclude grid worlds where the position of the 1 is in an ambiguous position for that concept category e.g., directly in the center for a cardinal task, or in the middle row for a spatial task. 13\n\nB.2.3 G ENERALISATION TO S UB - SPACES\n\n Similar to the colours, for the cardinal directions, we also report (in Appendix Table 11) results on a sub-space generalisation task. Speci\ufb01cally, we show models examples of concepts in one sub-space of the world (e.g., north, east, northeast ) and then test them on concepts in an entirely different sub-space (e.g., south, west, southwest ). The training split therefore contains worlds annotated with one sub-space, and we test on the remaining held-out concepts. We do this for all sub-spaces and average over them. As seen in Table 11, we see that the largest model can perform this task to some degree, achieving a 60% accuracy.\n\nB.2.4 G ENERALISATION TO C OMPOSITIONS\n\n Here, we speci\ufb01cally test model performance when holding out any instance of compositions of directions. Therefore, the training split contains examples of worlds annotated with the concepts north, south, east, west , and the test split contains examples of compositions of concepts i.e., north- east, southeast, northwest, southwest . We report results in Table 10, and we see that here, even the largest model fails to perform the task. This means that when models have never seen any instance of a composition, they do not perform these compositions themselves, however, as seen in Table 10, and earlier results, when the training data contains some examples of compositions, models can indeed generalise to completely new compositions. 107\n\nC E VALUATION M ETRICS\n\n In this section we describe our evaluation metrics in detail, with examples of how certain generations might be scored by each metric. 14\n\nC.1 S UBSTRING M ATCH\n\n For every test instance, there is exactly one string answer that is correct (for e.g., the words \u201cleft\u201d or \u201cnortheast\u201d or \u201celectric blue\u201d ). However, language models are free to generate any number of tokens given a prompt\u2014i.e., they might generate exactly one token, or up to a 100 tokens for any given prompt. We consider the \ufb01rst 5 tokens generated by the model to be the generated answer\u2014 and consider a substring match metric to be one that looks at whether or not the model generated answer lies in the ground truth answer. To make this clearer, we provide examples below, denoting a model-generated answer in blue and the ground-truth answer in green. By the substring metric, electric green for electric blue would give an accuracy of 0, however electric or green would give an accuracy of 1. In practice, we do not see (the large) models generate half-answers (e.g,. simply saying electric ). Similarly green for dark green has an accuracy of 1, but dark pink for dark green would have an accuracy of 0.\n\nC.2 E XACT M ATCH\n\n For this metric, the model only gets a perfect accuracy if the model generated answer exactly matches the ground-truth answer (after stripping both for any trailing whitespaces). Therefore saying green for dark green , or electric green for light electric green would have an accuracy of 0.\n\nC.3 G ROUNDING D ISTANCE\n\n Equation C.3 shows our calculation of distances in the world. For a certain concept category C (for example, all colour names existing in the dataset), let c 1 be the model predicted concept name and c 2 be the true grounding. The grounding distance is the Euclidean distance between the two points when the predicted concept does exist in the space of concepts in the world, and is set to an arbitrarily high number (here, 500, which is higher than the maximum distance between any two points in space) when the predicted concept is some other word does not fall in the concept category.\n\nD P ROMPT S IZE\n\n The size of the input prompt to a model depends on its maximum content length, which is a hyper- parameter \ufb01xed before training the model to completion. The question-answer pairs that we use to teach models grounded concepts differ in their lengths based on the concept category. For example, since the spatial and cardinal concepts require grid worlds that could be up to 10 rows and columns wide, a prompt for these categories might contain a fewer number of samples. Since the colour groundings are 3-digit RGB codes, a larger number of samples can be given in a prompt. We report accuracy curves of models when given an increasing number of samples in the prompt in Appendix D. We see that past 20 samples for spatial terms, and 60 samples for colours, models have no signif- icant increase in performance. When we report results in Tables 2 and Table 1, we report numbers for a \ufb01xed number of samples in a prompt (e.g., 20 vs. 60 respectively) for all models. Interestingly, once we get past a certain number of prompts, there is no signi\ufb01cant increase in model performance on the task. This result hints at two things. First, the larger models seem to learn a task, and generalise to new samples, with only a small number of sample when \ufb01ne-tuned in a few-shot prompting regime. This seems to hold for the smaller models as well i.e., although they do not learn the task well, increasing the number of samples within a prompt does not signi\ufb01cantly increase model performance. Second, the signi\ufb01cant difference in performance based on model size hints at the fact that the limiting factor for good performance is the number of parameters of a model. 15 More concretely, the generalisation extent of each model seems to max out at some number of input prompts, with a trend of increasing performance on increasing model size. Therefore, a larger model (than the ones we have here) might be required in order to achieve better grounded generalisation.\n\nE M ODEL G ENERATIONS\n\n We show example generations from the model in Table 8 world to RGB: ( 43) light blue color the red blue hex red color green red 16\n\nF P RE - TRAINING D ATA F.1 W HAT HAVE MODELS SEEN IN THEIR TRAINING DATA THAT MIGHT BE RELATED TO GROUNDING ?\n\n Pre-trained language have been trained on large text corpora available on the internet\u2014a diverse source of information that covers many domains. There is speculation, that the remarkable gen- eralisation performance of these models stems from having seen instances of the task or related information at some point in their training data. However, this data has not been made public, which makes this a hypothesis that is hard to con\ufb01rm. In our domain of grounded concepts as well, it is unclear what aspects of similar data the model might have had access to during training, or what inductive biases from certain types of data it might have learnt, that allow it good performance on grounded tasks. We attempt to assess this in the following way. We consider all the concept words in every concept category, and all the world representations, and consider these the prompts for the models. For each word, we wish to assess the distribution of text generated, which should, in some sense, re\ufb02ect the distribution of text seen during training. For example, or the world representations (i.e., a 2D array), we assess how often the model might use a spatial or cardinal term when simply prompted with an array, thus giving us insights into how often the training data might have contain such an array in close proximity to such a word. Similarly, for the colour domain, we evaluate how often the model predicts a correct colour name when prompted with an RGB code. We see that when models are simply prompted with a world representation, they tend to generate similar- looking world representations, instead of conceptual words (such as \u201cleft\u201d ) that might be associated with it. When prompted with RGB codes, the GPT-3 model only has a 7 . 3% accuracy of correctly generating the corresponding colour name in a 5-token sequence after the prompt. 17 18", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates the extent to which large language models (LLMs) trained solely on text can learn to map words to grounded conceptual spaces, despite never being explicitly trained on such groundings.", "topic_class_1": "Summary", "topic_class_2": "Summary of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors focus on color and direction domains, representing the grounded spaces as text (e.g.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "RGB values for colors, 2D matrices for directions).", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They prompt LLMs with a few examples of groundings (e.g.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "the word \"red\" mapped to the RGB code for red) and test if the models can generalize to unseen instances and unseen but related concepts.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Crucially, they test the models on \"rotated\" conceptual spaces to control for memorization.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The key findings are:", "topic_class_1": "Summary", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "1) The largest model tested (GPT-3 with 175B parameters) can learn to ground words to conceptual spaces to a significant degree, while smaller models struggle.", "topic_class_1": "Findings", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "2) The models generalize to unseen instances in the original and rotated spaces, but not in random unstructured spaces, suggesting they are not merely memorizing.", "topic_class_1": "Findings", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "3) To some extent, the models can generalize to unseen but related concepts (e.g.", "topic_class_1": "Findings", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "grounding \"blue\" after being shown how to ground \"red\").", "topic_class_1": "Findings", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "4) Analysis of model errors shows the large model tends to predict incorrect but semantically related concepts.", "topic_class_1": "Findings", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper addresses the fundamental question of whether LLMs can learn grounded meaning solely from text, which has significant implications for language understanding.", "topic_class_1": "Strength", "topic_class_2": "Strength on the significance of the problem", "reliability": "No", "explanation": "The paper addresses the extent as opposed to the possibility of learning grounded meaning from text.", "error_type": "Contradiction"}, {"segment_text": "- The experimental design carefully controls for memorization using rotated conceptual spaces.", "topic_class_1": "Strength", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Testing for generalization to unseen concepts is a strong evaluation of the models' ability to learn conceptual structure.", "topic_class_1": "Strength", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The error analysis provides useful insights into the models' failures and successes.", "topic_class_1": "Strength", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The domains tested (colors and directions) are fairly simple and restricted to text representations.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the generality of method", "reliability": "No", "explanation": "The authors test spatial terms, cardinal directions, and colors. While simple tasks, these allow us to distinguish the performance of smaller and larger LLM sizes.", "error_type": "Out-of-scope"}, {"segment_text": "It's unclear if the findings would generalize to more complex grounded concepts.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the generality of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- There is no comparison to multimodal models trained on both text and perceptual data, to assess the gap between pure language models and grounded models.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the lack of compared baselines", "reliability": "No", "explanation": "Section 1 (Introduction) and Section 5 sufficiently describe the need to test textual models on grounded conceptual spaces. Multi-modal model performance on this task requires the construction of different probes and increases the complexity of the problem which is beyond the scope of this paper.  ", "error_type": "Out-of-scope"}, {"segment_text": "- More insight into what enables the largest model to succeed (e.g.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the limited insights provided", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "analyzing its representations) would strengthen the explanatory power of the work.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the limited insights provided", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and the methodology is described in enough detail to enable reproducibility.", "topic_class_1": "Clarity", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research question is novel and the results are significant, shedding light on the grounding capabilities of LLMs.", "topic_class_1": "Quality", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are carefully designed and analyzed.", "topic_class_1": "Quality", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The domains tested are simple but the paper provides a strong proof of concept.", "topic_class_1": "Quality", "topic_class_2": "Strength on the evaluation", "reliability": "No", "explanation": "conflict", "error_type": "Contradiction"}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper provides an insightful and well-executed investigation into the ability of language models to learn grounded meaning solely from text.", "topic_class_1": "Summary", "topic_class_2": "Summary of the topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The key finding that very large LLMs can learn to map words to conceptual spaces, and generalize to some extent to unseen concepts, is significant and novel.", "topic_class_1": "Summary", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are rigorous, controlling for memorization and testing generalization.", "topic_class_1": "Summary", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper opens up an exciting direction for building grounded language understanding from the rich conceptual structure captured by large LLMs.", "topic_class_1": "Summary", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The main limitation is that the tested domains are narrow, so the generality of the findings to more complex concepts remains an open question.", "topic_class_1": "Summary", "topic_class_2": "Summary of the limitation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is an important contribution that advances our understanding of the grounding capabilities of language models.", "topic_class_1": "Summary", "topic_class_2": "Overall summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper explores the capability of large pre-trained language models (LMs) to map learned linguistic structures onto grounded conceptual spaces without direct observation of the physical world.", "topic_class_1": "Summary", "topic_class_2": "Summary of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It investigates whether the rich conceptual structure learned from text is reflective of the conceptual structure of the non-linguistic world by teaching LMs to map linguistic concepts (e.g., colors, directions) onto representations in a simplified, textual \"grid world.\"", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Through experiments with generative LMs of varying sizes, including GPT-2 and GPT-3, the study demonstrates that while smaller models struggle with this task, the largest model, GPT-3, shows an ability to not only learn but also generalize grounded concepts to new, unseen instances.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This suggests that large text-only LMs could be grounded in a data-efficient manner, leveraging their inherent conceptual knowledge.", "topic_class_1": "Summary", "topic_class_2": "Summary of claim", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel Approach:The paper proposes an innovative method to explore grounding in LMs without the need for direct sensory input, suggesting a pathway to grounding LMs in more abstract, conceptual spaces.", "topic_class_1": "Strength", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Significant Empirical Evaluation:The thorough evaluation across several LMs and conceptual domains (colors, directions) provides strong evidence of the varying abilities of LMs to understand and generalize grounded concepts.", "topic_class_1": "Strength", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Insightful Analysis:Error analysis and investigation into the models' performances offer valuable insights into how LMs may leverage linguistic structures to understand grounded concepts.", "topic_class_1": "Strength", "topic_class_2": "Strength on the evaluation", "reliability": "No", "explanation": "Not accurate: \"leverage linguistic structures\" is not discussed in this paper.", "error_type": "Unstated statement"}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited Scope of Grounded Concepts:The paper focuses on relatively simple and easily text-representable concepts like colors and directions, which might not fully capture the complexity of grounding in more abstract or nuanced concepts.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the generality of method", "reliability": "No", "explanation": "The authors test spatial terms, cardinal directions, and colors. While simple tasks, these allow us to distinguish the performance of smaller and larger LLM sizes.", "error_type": "Out-of-scope"}, {"segment_text": "Dependency on Textual Representations:The approach relies on the assumption that textual representations can sufficiently capture the essence of grounded concepts, which may not always be the case, especially for more complex sensory experiences.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the generality of method", "reliability": "No", "explanation": "Section 1 (Introduction) and Section 5 sufficiently describe the need to test textual models on grounded conceptual spaces. Multi-modal model performance on this task requires the construction of different probes and increases the complexity of the problem which is beyond the scope of this paper.  ", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity:The paper is well-written, with clear explanations of the methodology, experiments, and findings.", "topic_class_1": "Clarity", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality and Rigor:The research is of high quality, with rigorous experimental design and thorough analysis of results.", "topic_class_1": "Quality", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty:The paper contributes novel insights into the possibility of grounding LMs in conceptual spaces using only textual information.", "topic_class_1": "Novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility:Detailed descriptions of the experimental setup and access to model details enhance the reproducibility of the results.", "topic_class_1": "Reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents an intriguing exploration of grounding language models in conceptual spaces without direct sensory input.", "topic_class_1": "Summary", "topic_class_2": "Summary of the topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It significantly contributes to our understanding of how large LMs can leverage their linguistic knowledge to understand and generalize grounded concepts.", "topic_class_1": "Summary", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the study is limited by its focus on simple, text-representable concepts and assumes the sufficiency of textual representations for grounding, its innovative approach, rigorous evaluation, and insightful analysis make a valuable contribution to the field.", "topic_class_1": "Summary", "topic_class_2": "Summary of the limitations", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The findings suggest that large text-only models like GPT-3 might be capable of being grounded in a data-efficient manner, opening new avenues for research into grounding LMs.", "topic_class_1": "Summary", "topic_class_2": "Summary of the findings", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper investigates whether large language models (LLMs) trained solely on text can learn to map words to grounded conceptual spaces.", "topic_class_1": "Summary", "topic_class_2": "Summary of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors test this by showing LLMs of varying sizes (including GPT-2 and GPT-3) how certain words are used in simple text worlds representing spatial terms, cardinal directions, and colors.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They then evaluate the models' ability to generalize to unseen worlds and unseen but related concepts within these domains.", "topic_class_1": "Summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel approach: The paper proposes a novel approach to grounding LLMs by leveraging their existing conceptual structure learned from text, rather than training them from scratch on multimodal data.", "topic_class_1": "Strength", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strong empirical results: The largest model (GPT-3) demonstrates impressive performance in generalizing to unseen worlds and unseen concepts, suggesting that it can indeed learn grounded representations.", "topic_class_1": "Strength", "topic_class_2": "Strength on the significant results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Control for memorization: The use of rotated worlds as a control helps rule out the possibility that the models are simply memorizing specific word-world mappings.", "topic_class_1": "Strength", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Detailed error analysis: The analysis of model errors provides valuable insights into how the models are reasoning and where they struggle.", "topic_class_1": "Strength", "topic_class_2": "Strength on the analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited domains: The study focuses on a limited set of grounded concepts that can be easily represented in text form. This raises questions about thegeneralizability of the approach to more complex concepts.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the domain", "reliability": "No", "explanation": "The authors test spatial terms, cardinal directions, and colors. While simple tasks, these allow us to distinguish the performance of smaller and larger LLM sizes.", "error_type": "Out-of-scope"}, {"segment_text": "Performance of smaller models: Smaller LLMs perform poorly on the tasks, suggesting that massive scale might be necessary for this approach to work effectively.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the bad performance of small models", "reliability": "No", "explanation": "Not a weakness.\nSmaller models do not show positive results while larger models do. However, this serves as one of the important findings of this paper. This is not a weakness of the paper whatsoever.", "error_type": "Misunderstanding"}, {"segment_text": "Unclear how to scale to more complex concepts: It is unclear how this technique could be extended to ground more complex visual or sensory concepts that are difficult to represent textually. It would be interesting to explore how the performance of the approach changes with different types of world representations and different levels of complexity within each domain. Investigating ways to adapt GPT-3-like models to accept non-textual inputs would be valuable for extending this approach to a wider range of grounded concepts. Exploring the relationship between the size of the LLM and its ability to learn grounded representations would be helpful for understanding the role of scale in this approach.", "topic_class_1": "Weakness", "topic_class_2": "Concern on the generality of method Concern on the domain Concern on the input modalities Concern on the size of LLM", "reliability": "No", "explanation": "Section 1 (Introduction) and Section 5 sufficiently describe the need to test textual models on grounded conceptual spaces. Multi-modal model performance on this task requires the construction of different probes and increases the complexity of the problem which is beyond the scope of this paper.  ", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is clearly written and well-organized, with concrete examples and detailed explanations of the methodology.", "topic_class_1": "Clarity", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-designed and executed, with appropriate controls and evaluation metrics.", "topic_class_1": "Quality", "topic_class_2": "Strength on the evaluation", "reliability": "No", "explanation": "confict with weaknessess", "error_type": "Contradiction"}, {"segment_text": "Novelty: The idea of grounding LLMs by leveraging their existing conceptual structure is novel and potentially impactful.", "topic_class_1": "Novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides sufficient details about the data generation process and model configurations to allow for reproducibility.", "topic_class_1": "Reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a promising new approach for grounding LLMs in a data-efficient way.", "topic_class_1": "Summary", "topic_class_2": "Summary of the topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the study has limitations in terms of the domains explored and the reliance on large models, it offers valuable insights into the potential of LLMs to learn grounded representations from text alone.", "topic_class_1": "Summary", "topic_class_2": "Overall summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further research is needed to explore thegeneralizability of this approach and to develop methods for grounding more complex concepts.", "topic_class_1": "Summary", "topic_class_2": "Future work suggestion", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Capturing Structural Locality in Non-parametric Language Models", "body_text": "\n\nC APTURING S TRUCTURAL L OCALITY IN N ON - PARAMETRIC L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models , which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model ef\ufb01cacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improved performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.\n\n1 I NTRODUCTION\n\n Language models (LMs) predict a probability distribution over sequences, and are most widely studied to model and generate natural languages (Bengio et al., 2003; Merity et al., 2018; Baevski & Auli, 2018; Brown et al., 2020). Advances in LMs bene\ufb01t many natural language processing downstream tasks, such as machine translation (Bahdanau et al., 2015), dialog systems (Sordoni et al., 2015), question answering (Yang et al., 2019; Raffel et al., 2019), and general representation learning for natural language (Devlin et al., 2018; Liu et al., 2019). Recently, LMs have also been adopted to model sequences other than text, such as source code written in programming language (Hindle et al., 2016; Hellendoorn & Devanbu, 2017; Alon et al., 2020; Karampatsis et al., 2020), which can enable useful downstream tasks like code completion (Raychev et al., 2014). Most current neural LMs are based on parametric neural networks, using RNN (Mikolov et al., 2010) or Transformer (Vaswani et al., 2017) architectures. These models make predictions solely using a \ufb01xed set of neural network parameters. Recently, more and more neural LMs also incorporate non-parametric components (Grave et al., 2017; Guu et al., 2018; He et al., 2020; Khandelwal et al., 2020), which usually \ufb01rst select examples from an external source and then reference them during the prediction. For example, Khandelwal et al. (2020) model the token-level probability by interpolating the parametric LM probability with a probability obtained from the nearest context-token pairs in an external datastore. Using such non-parametric components in LMs is bene\ufb01cial because the model no longer needs to memorize everything about the language in its parameters. For such non-parametric LMs, one important concept is a distance metric between the current context and other contexts in the datastore. One example of such metric is the (cid:96) 2 distance between context vectors calculated by the parametric model (Khandelwal et al., 2020). This distance can be used in both retrieval and probability calculation; items in the datastore that are less distant from the current context are more likely to be retrieved and have a higher in\ufb02uence on the \ufb01nal probability. However, given that non-parametric datastores are typically very large, containing a myriad of contexts from disparate sources, calculating a metric that accurately re\ufb02ects semantic similarities is non-trivial; as we demonstrate in experiments, there is much room for improvement in current practice. In this paper, we argue that the relevance of contexts may be correlated with not only contextual distance, but also structural characteristics of the underlying data. Speci\ufb01cally, we take advantage of a property we dub structural locality , the propensity of text to be divided into local groups sharing 1 Obama was awarded the Judea Pearl received his Terence Tao won the Judea Pearl received the DatastoreContext Geoffrey Hinton was awarded the Target Nobel Bachelor Turing Fields CS Awards Category Section Politics CS Math CS Awards Early Life Awards Awards query Distance \ud835\udc51 Nobel Fields Turing Bachelor 3 10 20 22 Modified Distance Turing 5 Nobel 15 Fields 22 Bachelor 50 \ud835\udc50 \ud835\udc64 \ud835\udc59 \ud835\udc54 \ud835\udc53(\ud835\udc50 \ud835\udc61 ) Current Context \ud835\udc50 \ud835\udc61 Normalize Aggregate \ud835\udc5d kNN (\ud835\udc64 \ud835\udc61 |\ud835\udc50 \ud835\udc61 ) Current Locality ? \ud835\udc64 \ud835\udc61 common hierarchical attributes. This property is ubiquitous across many kinds of texts and can provide additional information on how closely related two different examples are to each other. Throughout this paper, we will provide two case-studies of this phenomenon. First, in the domain of programs written in source code, if two source \ufb01les originate from the same project, they are more likely to be related than \ufb01les from other projects, and even more so if they are from the exact same package (Hellendoorn & Devanbu, 2017). Second, in natural language, two sections of Wikipedia text may be more related if they fall within the same topical domain, are from similarly titled sections, or even are from the same article (as in Figure 1). Notably this locality often manifests itself at different levels, such as the levels of \u201cproject\u201d, \u201csubdirectory\u201d, and \u201c\ufb01le\u201d cited above for source code. In this paper, we hypothesize that by using multiple levels of structural locality, we can better calibrate the distance metrics used to retrieve examples from non-parametric datastores, thereby improving LM performance. Speci\ufb01cally, we propose a simple-yet-effective approach that can easily be applied to non-parametric LMs: we use different levels of structural locality to de\ufb01ne functions that modify the contextual distance metrics used by the non-parametric module. We evaluate our method on two drastically different domains: Java programming language source code, and natural language Wikipedia articles, achieving noticeable LM performance gains in both by adding just 5 & 7 parameters respectively. Moreover, we perform an in-depth analysis showing how the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure, providing evidence for why adding the locality features is indeed useful. We also compare programming languages and natural languages to highlight several interesting differences in terms of how, and how much, the locality helps improve LM performance.\n\n2 N ON - PARAMETRIC L ANGUAGE M ODELS\n\n Given a linguistic context consisting of a sequence of tokens c t = ( w 1 , ...w t \u2212 1 ) , autoregressive parametric LMs estimate p ( w t | c t ; \u03b8 ) , the probability distribution over the next token w t . Such parametric LMs store information regarding the language being modeled in the parameters \u03b8 . The size of \u03b8 is \ufb01xed in advance based on the hyperparameters of the model architecture, in recent years typically a neural network (Grave et al., 2016; Baevski & Auli, 2018; Dai et al., 2019; Brown et al., 2020). In contrast, a non-parametric LM\u2019s number of parameters is not determined by just the model architecture, but also by the underlying data used to train the model. While non-parametric LMs using Bayesian statistics have existed for some time (Wood et al., 2011; Shareghi et al., 2017; He et al., 2020), they have recently seen increased prevalence through the introduction of neural LMs that retrieve relevant examples from an external datastore (Hashimoto et al., 2018; Guu et al., 2018). In particular, we focus on kNN-LMs (Khandelwal et al., 2020), a variety of such models that uses a nearest neighbor retrieval mechanism to augment a pre-trained parametric LM, achieving impressive results without any additional training. Neural network-based LMs usually map the context c to a \ufb01xed-length vector representation, with a trained function f ( c ) . In kNN-LMs, the non-parametric component consists of a collection ( D ) of contexts for the kNN to retrieve from. Denoting these contexts and their corresponding next token as 2 \u2013 ( c i , w i ) \u2208 D , we create a datastore ( K , V ) = { ( k i , v i ) } , which contains key-value pairs: ( i (1) During inference, the parametric component of the LM generates the output distribution over next tokens p LM ( w t | c t ; \u03b8 ) and the corresponding context representation f ( c t ) , given the test input context c t . Then the non-parametric component of the LM queries the datastore with f ( c t ) representation to retrieve its k -nearest neighbors N according to a distance function d ( \u00b7 , \u00b7 ) . We can then compute a probability distribution over these neighbors using the softmax of their negative distances. The model aggregates the probability mass for each vocabulary item across all its occurrences in the retrieved targets. This distribution is then interpolated with the parametric LM distribution p LM to produce the \ufb01nal kNN-LM distribution: In our experiments, we follow Khandelwal et al. (2020) in setting the interpolation factor \u03bb to 0.25.\n\n3 D EFINING S TRUCTURAL L OCALITY\n\n We de\ufb01ne structural locality as a categorical feature calculated between a pair of contexts ( c i , c j ) in a collection of data, that describes whether the pair share some common, potentially hierarchical, attributes (e.g., the section title of a Wikipedia article section, or the directory path of a source code \ufb01le). For each domain, a set of hierarchical attributes { l 0 , l 1 , ..., l n } can be de\ufb01ned based on prior knowledge of the domain. We denote l k ( c i , c j ) \u2208 { 0 , 1 } as the boolean locality feature value for the context pair, representing whether c i and c j share the same hierarchical attributes l k . Here, l 0 is reserved for \u201cno locality\u201d, in case the pair shares none of the attributes. Without loss of generality, we set a constraint that (cid:80) k l k ( c i , c j ) = 1 , as new features can be introduced by conjunction and negation of the attributes if needed. Speci\ufb01c Instantiations. We instantiate these features on our two case studies of Wikipedia text and Java source code, as summarized in Table 1. In Wikipedia, for every context c i , we de\ufb01ne four mutually exclusive hierarchical attributes, l 0 \u2212 l 3 . We calculate these features based on the Wikipedia article and section titles, using simple pattern matching. We then link each article to a set of categories (one article may belong to multiple categories) using the knowledge graph WikiData, 1 by aggregating all the category entities involving two properties: P31 (instance of) and P279 (subclass of). The criterion for \u201csame section title\u201d is exact string match (Hayashi et al., 2020). If there is at least one common category between the sets of categories for two articles, the pair is assigned the \u201csame article category\u201d. For Java source code, we de\ufb01ne 3 mutually exclusive attributes, l 0 \u2212 l 2 based on the location of the code. For each source \ufb01le, we use the full \ufb01le path to obtain the two attributes: project name and sub-directory path. 2 The criterion for both \u201csame project\u201d and \u201csame subdirectory\u201d is exact string match. Note that these features are strictly hierarchical, hence only two features are used to capture speci\ufb01c locality here. An Aside: Connections to Domain Adaptation. Domain adaptation typically refers to reusing existing information about a given problem (e.g., data or model) to solve a task in a new domain. 1 https://www.wikidata.org/ 2 For example, full path .../Journal.IO/src/main/java/journal/io/api/DataFile.java has project Journal.IO and sub-directory src/main/java/journal/io/api/ for package journal.io.api . 3 Domain adaptation for neural models generally focuses on \ufb01ne-tuning models on in-domain data (Sen- nrich et al., 2016; Chu et al., 2017) or making direct modi\ufb01cations to the model to consider domain information (Britz et al., 2017) or latent topic features (Khudanpur & Wu, 2000; Mikolov & Zweig, 2012; Wang & Cho, 2016). Most of these methods do not natively support new test-time contexts that were not seen at training time. In comparison, one immediate advantage of non-parametric LMs is the ability to adapt to different domains at test time without re-training (Merity et al., 2016; Grave et al., 2016; 2017; Khandelwal et al., 2020). For example, some adaptive LMs (Grave et al., 2016; 2017) make use of the previous hidden states of test documents dynamically during inference. Similarly, our proposed locality features do not require re-training on the training set. Note that within the scope of this paper, although connected, the proposed structural locality is a different concept from domain . We consider domains as higher-level classi\ufb01cations describing the text where one example belongs to one domain label; e.g., a section about Kim Kardashian\u2019s early life belongs to a category of texts describing celebrities. One the other hand, with the structural locality, a user could de\ufb01ne multiple levels of locality: to that same section, we can assign not only the domain label, but also, the section title \u201cEarly Life\u201d. The lightweight nature of our model combined with non-parametric LMs also makes adding more levels of features straightforward, as the features only need to be calculated for the top nearest neighbors, and the number parameters that need tuning in our proposed method (Section 5) is only about twice the number of locality features.\n\n4 S TRUCTURAL L OCALITY AND N EAREST N EIGHBORS\n\n In this section, we examine the relationship between distances derived from neural LM features d ( f ( c i ) , f ( c t )) , structural locality features l ( c i , c t ) , and the accuracy of the next-word prediction w i . Speci\ufb01cally, the underlying assumption of the kNN-LM is that less distant contexts will be more likely to accurately predict the next word w t . We would like to test whether this correlation between distance d ( \u00b7 ) holds uniformly across different locality levels l ( \u00b7 ) , or if locality provides additional information indicative of whether a particular context is useful for predicting w i beyond just that provided by the neural representations. Data. We use two different corpora from different domains to examine this question. W IKITEXT -103 3 is a standard language modeling benchmark (Merity et al., 2016) consisting of natural language text from English Wikipedia. It contains a 250K token, word-level vocabulary, with 103M tokens in the training set and 250K tokens in both the validation and test sets. J AVA G ITHUB 4 is a programming language corpus containing Java source code from Github (Allama- nis & Sutton, 2013) that is widely used in source code modeling (Hellendoorn & Devanbu, 2017; Karampatsis et al., 2020). It contains 1.44B tokens from 13,362 projects in the training split, 3.83M tokens from 36 projects in the validation split and 5.33M tokens from 38 projects in the test split. The splits are separated by whole projects. The dataset is tokenized with byte-pair encoding (Sennrich et al., 2015) using the vocabulary from Karampatsis et al. (2020) with 2,000 subtokens. Base Model. As the neural model used to calculate context features, we follow Khandelwal et al. (2020), 5 train an LM with the exact architecture and optimization described by Baevski & Auli (2018): a decoder-only Transformer (Vaswani et al., 2017), with 1024 dimensional hidden states for the W IKITEXT -103 dataset and 512 for J AVA G ITHUB . We set the number of retrieved nearest neighbors to be analyzed to 1024, and the distance metric to (cid:96) 2 following the default. Datastore. To capture the effect of our proposed locality features, the datastore should ideally be both closely related to the test examples, suf\ufb01ciently large to ensure precise kNN retrieval performance for a wide range of contexts, and not too sparse in terms of the prevalence of locality features. For W IKITEXT -103, we include the training set, as well as the validation/test set (excluding the text currently being evaluated) in the datastore. For the J AVA G ITHUB , due to the relatively large size of the validation/test set, and the unwieldy size of the training set, we include only the validation/test set (also excluding the current \ufb01le). 3 https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/ . 4 https://zenodo.org/record/3628665 . 5 https://github.com/urvashik/knnlm 4 Analysis. Consider k nearest neighbor contexts N t = { c r | r = 1 ...k } retrieved for any test context c t in the test set C , ordered by the ascending distance: \u2200 r : d ( c r , c t ) < d ( c r +1 , c t ) . We de\ufb01ne r \u2208 [1 , k ] as the \u201crank\u201d for the retrieved context c r . To study the quality of the retrieved contexts, we calculate the number of correctly retrieved tokens, de\ufb01ned as # { w r = w t gold } across C . We plot in Figure 2, from left to right: (1) Negative distances {\u2212 d ( c r , c t ) | c r \u2208 N t , c t \u2208 C} grouped into bins, vs. the retrieval accuracy of the bin avg (# { w r = w t gold . (2 Rank vs. the retrieval accuracy at rank r , (# . 3 . the negative distance plots . Naturally, the left-most sub-\ufb01gures re\ufb02ect that the lower the (negative) distance, the lower the accuracy on both datasets. Yet, interesting, on the Wikipedia domain (Figure 2a), as the negative distance gets close to 0 (perfect match), the retrieval accuracy for the next word does not always increase; the accuracy values in this range have very high variance and all 4 levels of locality show no clear trend. This partly indicates that context-based distance is imperfect, regardless of locality. Even so, at slightly lower distances, the trends stabilize and largely show a consistent picture: more speci\ufb01c locality features, especially those involving the same category ( l 1 & l 3 ) yield better predictions than the locality-insensitive component for identical distances. This is especially signi\ufb01cant at higher ranked retrievals (middle sub-\ufb01gure), where contexts that share the same section title and the same article category are substantially more likely to share the same completion. This suggests that the proposed locality features are not fully represented by, or correlated with the original distance metric, and thus implies that there is room for improvement by incorporating these features. In the Java source code domain (Figure 2b), we generally observe that the retrieval accuracy is much higher than that in the Wikipedia domain, suggesting that the kNN is doing a better job retrieving relevant contexts. This is largely due to higher repetitiveness of source code (Hindle et al., 2016); as we show later, the base Transformer model also performs much better here than on natural language text (Section 6.2). We also observe a more pronounced locality effect here: at the same distances close to 0 and at the same rank, neighbors that are local to the current context have far higher accuracy, indicating usefulness of locality features in the source code domain as well. However, as we can see 5 from the (right-most) plot of rank versus the negative distance, the average distances of the neighbors with higher locality levels are also signi\ufb01cantly smaller than the distance of those without locality. This suggests that the distance in the Java source code domain already correlates well with the level of locality, which may render incorporating locality features less bene\ufb01cial. We study the precise bene\ufb01t under one instantiation of this model next.\n\n5 I NCORPORATING S TRUCTURAL L OCALITY IN N ON - PARAMETRIC LM S\n\n Now that we have demonstrated that locality is additionally indicative of next-word prediction accuracy beyond context distance, we propose a method to incorporate this information into the non- parametric retrieval module. In the case of kNN-LMs (Section 2), recall that p kNN is calculated based on the softmax of the negative distance \u2212 d ( f ( c i ) , f ( c t )) . Assuming locality features { l 0 , l 1 , ..., l n } for each pair ( c i , c t ) consisting the retrieved nearest neighbor and the current inference context c t , we modify the formulation of p kNN (Equation 3) to consider these features as below: where g n ( d ( \u00b7 , \u00b7 ); \u03b8 n ) is a learnable function of the distance of the nearest neighbors, with parameter \u03b8 n for each type of locality feature l n . One can view function g ( \u00b7 ) as a \u201cmodi\ufb01ed\u201d distance for nearest neighbors after taking locality information into consideration. In our experiments, we adopt a linear form of g ( \u00b7 ) : 6) We omit the bias for g 0 ( \u00b7 ) by setting b 0 = 0 to remove one free parameter from the model and potentially make optimization easier. 6 To learn these functions, a user needs to provide just a small sample of annotated data in the same domain, as there are only 2 n + 1 parameters to optimize. In our experiments, we use the validation split for optimization. The parameters are trained to minimize the negative log-likelihood of the kNN prediction of the gold token: To optimize the parameters, we use the Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.0001 on the validation set for 200 epochs. It converges within 20 minutes for both datasets.\n\n6 H OW D OES S TRUCTURAL L OCALITY I MPROVE L ANGUAGE M ODELING ? 6.1 E XPERIMENTAL S ETUP\n\n Baselines. Since we base our model on kNN-LMs, this model will be our most directly comparable baseline. We also compare our model to the underlying parametric LM (Baevski & Auli, 2018), without the kNN module. For the J AVA G ITHUB dataset, we additionally compare to the recent state-of-the-art model from Karampatsis et al. (2020) on code language modeling, which uses BPE and LSTMs. In all experiments, the maximum number of tokens per input sample is 3,072. Evaluation. We evaluate the performance of the LM with the standard perplexity metric and token prediction top- k accuracy on the held-out data. 7 The top- k accuracy is calculated by checking if the ground truth token is among the predicted top- k list. This metric, primarily for k \u2208 { 1 , 5 } (with more k values in Appendix A.2), is commonly used to evaluate predictive models of source code (Hindle et al., 2016). In order to more easily incorporate and analyze the locality features, and also 6 In preliminary experiments, we also considered an adaptive variant that conditioned the weights and biases ( { w n } , { b n } ) on the current context representation f ( c t ) parameterized by a light MLP. However, this did not result in signi\ufb01cant improvement over directly optimizing w and b , perhaps because the context vector space is very large (512-1024 dimensions) compared to the relatively few data points from the validation set used to train. 7 For J AVA G ITHUB , the perplexity is calculated on full tokens by aggregating the likelihood of subtokens. The accuracy is calculated that all subtokens in a full token have to be predicted correctly . 6 following Karampatsis et al. (2020) we split the evaluation dataset into independent test examples to evaluate, where each of the example is an atomic unit in the locality hierarchy. For J AVA G ITHUB , each test example is a source code \ufb01le, and for W IKITEXT -103, each test example is a Wikipedia article section. 8\n\n6.2 R ESULTS\n\n The high-level results are shown in Table 2. At \ufb01rst glance, we can already see that the two datasets vary greatly in predictability. With a similar Transformer architecture, performance on J AVA G ITHUB is much better than on the W IKITEXT -103 across the board, partly due to the rigid nature of programming language syntax. With a Transformer model, we achieved a strong state-of-the-art language model on Java code, with low perplexity and very high prediction accuracy ( \u223c 75%). By adding kNN module onto the Transformer-based LMs, perplexity and accuracy in both domains improves by a large margin. This is expected and in line with previous experiments on kNN- LMs (Khandelwal et al., 2020). The Wikipedia domain enjoys less relative improvement in perplexity (16%) than the Java source code domain (29%). This is particularly interesting, considering that the datastore used for W IKITEXT -103 contains both the current held-out split and the training data ( \u223c 100M contexts), compared to that for J AVA G ITHUB with only the current held-out split ( \u223c 5M contexts). This re\ufb02ects the fact that source code is known to bene\ufb01t strongly from project- and package-speci\ufb01c locality (Tu et al., 2014; Hellendoorn & Devanbu, 2017). Adding proposed locality features and \ufb01netuning the pa- rameters on the validation set improves the performance further on both datasets, albeit with a smaller relative gain. This con\ufb01rms our hypothesis that incorporating locality into the non-parametric retrieval-based LMs is bene\ufb01cial. We also see that locality features in the Wikipedia domain result in fairly consistent gains, while the Java source code domain sees especially strong accuracy improvements. This echoes our analysis of the source code corpus in Section 4, where we found that distance was generally strongly correlated with accuracy, but that locality was particularly informative at low distances. There, it may help discriminate between top-ranked completion candidates (as also shown later in Tab. 4). It is notable that despite the fact that the perplexity and accuracy on J AVA G ITHUB are already very strong with the vanilla Transformer, we still see a noticeable relative error reduction of 4.9% by adding locality levels information. We next study how locality features guide towards a \u201cbetter\u201d distance distribution among nearest neighbors. We plot the relationship between the nearest neighbor ranks and \u201cmodi\ufb01ed\u201d distance g ( k i , c t ) in Figure 3. Table 3 shows the speci\ufb01c learned parameters for each level of g ( \u00b7 , \u00b7 ) . Evidently, the biases vary strongly, as the model tries to \u201ccorrect\u201d the original distance by emphasizing more 8 Note that because we predict W IKITEXT -103 section-by-section instead of article-by-article the perplexity numbers reported here are somewhat worse than other works. Article-by-article calculation is not inherently incompatible with our proposed method, but it would require additional implementation to use different locality features for different locations in the output. Hence, we used section-by-section calculation for expediency. 7 local contexts. Compared with the original negative distance \u2212 d ( k i , f ( c t )) depicted in Figure 2, the negative modi\ufb01ed distance is more separated between the different locality levels on either dataset, showing the relative importance of different locality more clearly. For W IKITEXT -103, comparing Figure 3a with Figure 2a, we can see that with the original distance different localities cluster together, and the modi\ufb01ed distance separates them much better. We can also see that if two contexts have the same article category and the same section title, then their distance on average is the closest, closely followed by those sharing article categories only. On the other hand, contexts that only share section titles are not as closely related. This is intuitively reasonable; the education section for a computer scientist and a musician can be very different in content, even if sharing some commonalities, like the phrases used to describe courses, grades, school locations, etc. This also underscores the usefulness of explicit locality features in providing interpretable insights into the relevance of domain knowledge. For J AVA G ITHUB , comparing Figure 3b with Figure 2b, we can see that the original distance is already more separated between different locality levels than that of W IKITEXT -103, again suggesting better learned representations (in terms of locality sensitivity) for the Java domain. However, the model still bene\ufb01ts somewhat from the contexts that are under the same subdirectory (more so than just the same project), especially for top nearest neighbors: the gap for ranks higher than 80 is more pronounced with the modi\ufb01ed distance. This again veri\ufb01es our hypothesis about the hierarchical nature of structural locality. It also indicates potential practical applications \u2013 if this model were deployed in a code editor, one could obtain representations of the \ufb01les in the same sub-directory as the current \ufb01le and use them, along with the proposed locality features, to bias auto-complete results. Table 4 shows a randomly sampled test context from each domain where p kNN for the gold token increases after using locality features. We can see that the nearest neighbor search using context representations performs reasonably well at capturing patterns and themed phrases, especially closer to the last token, \ufb01nding two very similarly rated candidates. However, in both examples, the second retrieved candidate has a wrong target token. Informed by locality features \u2013 in the W IKITEXT -103 example, a matching section and category for the \ufb01rst candidate \u2013 the more \u201clocal\u201d context enjoys a large boost in probability, while the non-local one\u2019s decreases slightly. We present additional examples in Appendix A.1. The J AVA example demonstrates the same effect; the second retrieved example shows resemblances in variable name and template structures, but the fact that the project is focused on Google API rather than Twitter API makes the original retrieval undesirable.\n\n7 C ONCLUSION\n\n In this paper, we propose a novel method of incorporating structural locality into non-parametric LMs that reference retrieved examples from a datastore. We evaluate this approach in both a natural language and programming language domain, and empirically explore the similarities and differences of how structural locality affects LMs in these settings. The improvement in perplexity and prediction accuracy across both domains show the effectiveness and ubiquity of such locality information. 8 in many parts enhanced cyclonic over part Paci\ufb01c their Section on the announced the arrival of a had washed away several houses and come inland about \" / \" googleapis Besides language modeling, we also envision that the method can bene\ufb01t other applications that could be enhanced using user-de\ufb01ned prior domain knowledge such as conditional generation or representation learning using retrieved information. Limitations. Our method applies to settings where locality effects are present, there is suf\ufb01cient data to build a reliable datastore for each locality level, and that locality is not already meaningfully captured by the model. While this may not apply to every domain, these features are common: besides source code & Wikipedia, domains including books (features: authorship & dewey decimal system information), research papers (venue, research area), product manuals (kind, sections), and online discussions (time, topic) are all plausible candidates. The features in our studied domains were selected based on availability and prior knowledge of the domain (e.g., for Java, Hellendoorn & Devanbu (2017)). While they did provide measurable improvements and were natural to interpret, these may not be the optimal choice, and other options are worth investigating. It is also possible that LM improvements will eventually lead to learned context representations that almost perfectly capture the relevant locality information. However, we believe this to be unlikely: in many practical settings, there is some inherent ambiguity in partial contexts that cannot be solved with the surface text only. For instance, in Java source code \ufb01les, it is common to declare a package, which will obviously match perfectly based on the \ufb01rst few tokens (e.g., package org. ) with many other contexts. Yet given the scoped nature of this declaration, locally retrieved continuations are inherently far more useful. 9\n\nE THICS S TATEMENT\n\n There are several ethical considerations regarding our proposed method. First, while language models have a large variety of positive uses in generating natural language (Li et al., 2021) or source code (Allamanis et al., 2018), there is also a potential for dual use. For example, previous work has cited the potential to generate fake news (Zellers et al., 2019) or undesirable/defamatory content (Wallace et al., 2019). Our methodology improves the accuracy of language models, which has the potential to increase their applicability not only in positive use cases but also in ethically questionable scenarios. In order to mitigate these risks, methods to detect machine generated content may be employed, although these are not perfect remedies (Zellers et al., 2019). Second, because our methodology explicitly references the training corpus in generation it may increase the likelihood of copying content more-or-less verbatim from the training text. This raises potential issues of copyright violation (Chen et al., 2021) or privacy violation (Carlini et al., 2021). However, at the same time, because non-parametric models increase traceability through direct references to the training corpus, it also provides a tool to identify the provenance of the original content, providing a tool to identify such verbatim copying compared to standard parametric models. Finally, there has been much recent interest in the energy and environmental impact of large language models (Strubell et al., 2019). Due to the necessity to look up information in a datastore, non- parametric models have additional computational overhead compared to parametric models. However, at the same time, as noted by Khandelwal et al. (2020), non-parametric models also provide a tool for quickly adapting to new domains through the use of domain-speci\ufb01c datastores, obviating the necessity for domain-speci\ufb01c \ufb01ne-tuning. Our work takes this a step further, allowing models to leverage locality of the datastores, potentially making this an even more attractive alternative for ef\ufb01cient adaptation.\n\nR EPRODUCIBILITY S TATEMENT\n\n The source code package containing a README document on how to reproduce the results and analysis and experiment scripts is available in the paper\u2019s supplementary material. The details about the dataset used, model hyper-parameters, and analysis performed are described in Section 4 and Section 6.1. All experiments are conducted on a single machine with a 48 core CPU and 8 NVIDIA V100 32GB GPU. For W IKITEXT -103 we use the pretrained model provided by (Khandelwal et al., 2020) for fair comparison. For J AVA G ITHUB the Transformer model is trained until it converges, requiring approximately 2 days. The datastore size is about 5GB for J AVA G ITHUB and 351GB for W IKITEXT -103.\n\nR EFERENCES\n\n 10 11 12 13\n\nA A PPENDIX A.1 A DDITIONAL E XAMPLES Test Context\n\n TestTarget Section: Design; Category: ship class In an effort to outmatch the American New York class, planners called for a ship armed with twelve 14-inch (36 cm) guns and faster than the 21 knots (39 km/h; 24 mph) of their rivals. Vickers \ufb01les show that the Japanese had access to the designs for double- and triple-gun turrets, yet opted for six double turrets over four triple turrets. The \ufb01nal design\u2014designated A-64 by the IJN\u2014called for a ... 09 DatastoreTarget Orig Prob Section: Design; Category: ship class Both ships were also given torpedo bulges to improve their underwater protection and to compensate for the weight of the additional armour. In addition, their sterns were lengthened by 7.62 metres (25 ft). These changes increased their overall length to 213.8 metres (701 ft), their beam to 31.75 metres (104 ft 2 in) and their draft to 9.45 metres (31 ft). Their ... 23 Section: History; Category: gun mount The British Admiralty ordered a prototype of Coles\u2019s patented design in 1859, which was installed in the ironclad \ufb02oating battery, HMS Trusty, for trials in 1861, becoming the \ufb01rst warship to be \ufb01tted with a revolving gun turret. Coles\u2019s aim was to create a ...\n\nTest\n\n Section: La Venta; Category: colossal statue When discovered it was half-buried; its massive size meant that the discoverers were unable to excavate it completely. Matthew Stirling fully excavated the monument in 1940, after clearing the thick vegetation that had covered it in the intervening years. Monument 1 has been ... moved -2 97 +1 22 Context Orig. Log-Prob. \u2206 Log- Prob. Section: San Lorenzo; Category: colossal statue The sculpture suffered some mutilation in antiquity, with nine pits hollowed into the face and headdress. San Lorenzo Colossal Head 10 (also known as San Lorenzo Monument 89) has been ... moved -4.18 +1.36 Section: San Lorenzo; Category: castle The excavations investigated the north of the fortress, searching for an entrance postulated by architect Eugene Viollet-le-Duc, but no such entrance was found. However, the excavation did reveal was that there was an addition to the north of the castle to enable the use of guns. Typologically, the structure has been ... dated -4.63 -0.11\n\nA.2 A DDITIONAL R ESULTS ON T OKEN P REDICTION A CCURACY\n\n 14 15", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper explores incorporating structural locality features into non-parametric language models that retrieve examples from an external datastore.", "topic_class_1": "summary", "topic_class_2": "Summary of paper", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors propose a simple approach of learning parameters that modify the distance metric used to retrieve examples based on locality information, to improve the likelihood of retrieving local examples.", "topic_class_1": "summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on Java source code and Wikipedia text demonstrate that adding locality features improves the perplexity and prediction accuracy compared to baseline models without these features. The authors also analyze how locality contributes to the improvements and why contextual similarity alone does not fully capture the locality structure.", "topic_class_1": "summary", "topic_class_2": "Summary of experiments", "reliability": "No", "explanation": "The summarization items seem correct, however, it is just replication of the paper abstract. The analysis is presented before introducing the structural locality with LMs and experiments", "error_type": "Duplication"}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper addresses an important problem of leveraging structural locality in non-parametric language models, which is a ubiquitous feature of real-world datasets. The proposed method is simple yet effective, requiring only a few additional learned parameters.", "topic_class_1": "Strength / Method", "topic_class_2": "Importance in the research topic Strength in method", "reliability": "No", "explanation": "The strengths are replicated compared with the abstracts, nearly with no rephrases.", "error_type": "Superficial Review"}, {"segment_text": "Experiments on two very different domains demonstrate the broad applicability of the approach. The authors provide an insightful analysis comparing the effect of locality in the two domains.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Strength in experiments", "reliability": "No", "explanation": "The strengths are replicated with no deep summary, even no mention of the name of two benchmarks", "error_type": "Superficial Review"}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The gains from adding locality features, while consistent, are relatively modest in terms of perplexity and accuracy.", "topic_class_1": "Weakness / Experiments", "topic_class_2": "Weakness of the experiments", "reliability": "No", "explanation": "The gain in perplexity and accuracy is significant as shown in Table 2.", "error_type": "Neglect"}, {"segment_text": "More discussion on the significance of the improvements would be helpful.", "topic_class_1": "Weakness / Experiments", "topic_class_2": "Weakness of the experiments", "reliability": "No", "explanation": "Already with analysis.", "error_type": "Neglect"}, {"segment_text": "The choice of locality features seems somewhat ad-hoc based on domain knowledge.", "topic_class_1": "Weakness / Method", "topic_class_2": "Weakness of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "More principled methods for defining the features could be explored.", "topic_class_1": "Weakness / Method", "topic_class_2": "Suggestion in future work", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limitations of the approach when locality information is not available or already captured by the model are not discussed in depth.", "topic_class_1": "Weakness / Experiments", "topic_class_2": "Weakness of discussion", "reliability": "No", "explanation": "Result of transformer with no locality information is presented and discussed.", "error_type": "Neglect"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written and easy to follow.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": "Opinion in clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method is described clearly and the experiments are comprehensive.", "topic_class_1": "Strength / Method and Experiments", "topic_class_2": "Opinion in quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The work is novel in studying the effect of locality in non-parametric language models and proposing a lightweight method to incorporate it.", "topic_class_1": "Strength / Method", "topic_class_2": "Opinion in novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The code and data are provided which aids reproducibility.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": "Opinion in reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The overall quality of the research seems high.", "topic_class_1": "Strength / Paper Quality", "topic_class_2": "Opinion in quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a valuable contribution in demonstrating how structural locality information can be leveraged to improve non-parametric language models.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method is straightforward but effective as shown through experiments on two domains.", "topic_class_1": "Summary of Strength on Experiments", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The analysis provides useful insights into the differing effects of locality.", "topic_class_1": "Summary of Strength on Analysis", "topic_class_2": "Summary of the review", "reliability": "No", "explanation": "It is unclear what differing effects of locality mean.", "error_type": "Vague Critique"}, {"segment_text": "While the gains are modest, the work opens up interesting avenues for further exploration of locality-aware retrieval-based language models.", "topic_class_1": "Summary of Weakness on Experiments", "topic_class_2": "Summary of the review", "reliability": "No", "explanation": "The gains are significant on the two benchmarks.", "error_type": "Misunderstanding"}, {"segment_text": "Some additional discussion on the choice and limitations of the locality features would further strengthen the paper.", "topic_class_1": "Summary of Suggestion on Discussion", "topic_class_2": "Summary of the revieww", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a solid paper that advances our understanding of an important problem.", "topic_class_1": "Summary of Recommendation", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces a novel approach to enhance non-parametric language models by leveraging structural locality, a feature inherent in datasets organized into local hierarchies.", "topic_class_1": "summary", "topic_class_2": "Summay of topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors propose a method to incorporate locality information into these models, aiming to improve the likelihood of retrieving examples from local neighborhoods.", "topic_class_1": "summary", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This is demonstrated through experiments on Java source code and Wikipedia text, showing that the introduction of locality features improves model performance.", "topic_class_1": "summary", "topic_class_2": "Summary of experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The analysis provides insights into why traditional contextual similarity metrics may fall short in capturing the essence of locality structures.", "topic_class_1": "summary", "topic_class_2": "Summary of results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Innovative approach: The paper presents a unique method to integrate structural locality into language models, addressing a gap in current modeling approaches.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength in novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical evidence: Demonstrated improvements on two diverse datasets underscore the method's effectiveness and general applicability.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Strength in experiment results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Significant contribution: This work sheds light on the limitations of existing similarity metrics in capturing locality, potentially influencing future research directions in language modeling.", "topic_class_1": "Strength / Method", "topic_class_2": "Opinion in general", "reliability": "No", "explanation": "The limitation is not subject to the metrics. But the existing vector stores in the embedding space.", "error_type": "Inexpert Statement"}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited domain analysis: While the paper provides insights into Java and Wikipedia datasets, the exploration of other domains where structural locality could play a crucial role is missing.", "topic_class_1": "Weakness / Experiments", "topic_class_2": "Weakness in experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Model complexity: The introduction of locality features adds complexity to the model, which might pose challenges in understanding and implementation for some practitioners.", "topic_class_1": "Weakness / Method", "topic_class_2": "Weakness in methods", "reliability": "No", "explanation": "Moaning for no reasons", "error_type": "Vague Critique"}, {"segment_text": "Clarity, Quality, Novelity, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written, clearly presenting the methodology, experiments, and findings.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": "Opinion on clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The quality of research is high, with thorough experimentation and analysis.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Opinion on quality", "reliability": "No", "explanation": "It contradicts to the weakness.", "error_type": "Contradiction"}, {"segment_text": "The novelty lies in the use of structural locality within non-parametric language models, a direction not extensively explored previously.", "topic_class_1": "Strength / Method", "topic_class_2": "Opinion on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The inclusion of source code and detailed experiment setups enhances the paper's reproducibility.", "topic_class_1": "Srength / Reprocudibility", "topic_class_2": "Opinion on reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper makes a compelling case for the inclusion of structural locality in enhancing non-parametric language models, supported by experiments across two domains.", "topic_class_1": "Summary of Strength on Method and Experiments", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Its strengths lie in its novel approach and the significant improvements demonstrated.", "topic_class_1": "Summary of Strength on Method and Experiments", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the exploration of additional domains and the complexity introduced by the locality features are areas that could be addressed further.", "topic_class_1": "Summary of Weakness on Applicability of the Method", "topic_class_2": "Suggestion of future direction", "reliability": "No", "explanation": "Well, if you think this is indeed complex", "error_type": "Misunderstanding"}, {"segment_text": "Overall, the paper presents a valuable contribution to the field of NLP, offering new insights and methodologies for future research.", "topic_class_1": "Summary of Recommendation", "topic_class_2": "Summary of the review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes a novel approach to enhance non-parametric language models (LMs) by incorporating structural locality information.", "topic_class_1": "summary", "topic_class_2": "Summary of the topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors argue that the relevance of contexts for predicting the next word depends not only on semantic similarity but also on the hierarchical structure of the data.", "topic_class_1": "summary", "topic_class_2": "Summary of the analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They introduce locality features based on domain knowledge (e.g., article category and section title for Wikipedia, project and sub-directory for Java code) and modify the distance metric used for nearest neighbor retrieval in kNN-LMs to leverage these features.", "topic_class_1": "summary", "topic_class_2": "Summary of the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper evaluates this approach on two diverse datasets \u2013 Wikipedia text and Java source code \u2013 and demonstrates improvements in perplexity and token prediction accuracy.", "topic_class_1": "summary", "topic_class_2": "Summary of experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The concept of incorporating structural locality into non-parametric LMs is novel and offers a promising direction for improving language modeling.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength in novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Effectiveness: The proposed method demonstrates consistent improvements in perplexity and accuracy across both Wikipedia and Java datasets, indicating its effectiveness and potential for broader applicability.", "topic_class_1": "Strength / Experiments", "topic_class_2": "Strength in experiments", "reliability": "No", "explanation": "In fact, the proposed method relies on careful extraction of domain feature, and  seems hard to be applied into broader domains. Conflict to Weakness 1-3.", "error_type": "Contradiction"}, {"segment_text": "Interpretability: The use of explicit locality features provides interpretable insights into the model's predictions and the importance of domain knowledge.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength in method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Lightweight: The method introduces only a small number of additional parameters, making it efficient and easy to integrate into existing kNN-LM frameworks.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength in method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Analysis: The paper provides a thorough analysis of the relationship between distance, locality, and prediction accuracy, offering valuable insights into the workings of the model.", "topic_class_1": "Strength / Method", "topic_class_2": "Strength in results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited scope: The current implementation focuses on two specific domains and requires manual definition of locality features.", "topic_class_1": "Weakness / Method", "topic_class_2": "Weakness in experiments", "reliability": "No", "explanation": "This contradicts to what the reviewer stated in the strengths.", "error_type": "Contradiction"}, {"segment_text": "Exploring automated feature extraction and extending the approach to other domains would broaden its impact.", "topic_class_1": "Weakness / Method", "topic_class_2": "Weakness in experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Datastore size: The datastore size for Wikipedia is quite large, raising potential concerns about computational efficiency and scalability.", "topic_class_1": "Weakness / Method", "topic_class_2": "Weakness in method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Investigating methods for optimizing datastore size and retrieval efficiency would be beneficial.", "topic_class_1": "Weakness / Method", "topic_class_2": "Suggestion in future direction", "reliability": "No", "explanation": "This is more like a future direction instead of the weakness", "error_type": "out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is generally well-written and clear, with detailed explanations of the methodology and analysis.", "topic_class_1": "Strength / Paper Writing", "topic_class_2": "Opinion in clarity", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, with rigorous evaluation and insightful discussion.", "topic_class_1": "Strength / Method", "topic_class_2": "Opinion in quality", "reliability": "No", "explanation": "Conflict to Weakness", "error_type": "Contradiction"}, {"segment_text": "The contributions are novel and significant, offering a new perspective on improving non-parametric LMs.", "topic_class_1": "Strength / Method", "topic_class_2": "Opinion in contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors provide a reproducibility statement and share the source code, facilitating replication and further research.", "topic_class_1": "Strength / Reproducibility", "topic_class_2": "Opinion in reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and effective approach for incorporating structural locality into non-parametric language models.", "topic_class_1": "Summary of Strength on Method", "topic_class_2": "Summary of review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method demonstrates promising results in two different domains, offering a valuable contribution to the field of language modeling.", "topic_class_1": "Summary of Strength on Experiments", "topic_class_2": "Summary of review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further exploration of its applicability to other domains and addressing the potential limitations would be valuable future directions.", "topic_class_1": "Summart of Suggestions on Experiments", "topic_class_2": "Summary of review", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance 3": " The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Empirical Novelty and Significance 3": " The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Flag for Ethics Review": " NO", "Recommendation 8": " accept, good paper", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Using Document Similarity Methods to create Parallel Datasets for Code Translation", "body_text": "\n\nU SING D OCUMENT S IMILARITY M ETHODS TO CREATE P ARALLEL D ATASETS FOR C ODE T RANSLATION\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Translating source code from one programming language to another is a critical, time-consuming task in modernizing legacy applications and codebases. Recent work in this space has drawn inspiration from the software naturalness hypothesis by applying natural language processing techniques towards automating the code translation task. However, due to the paucity of parallel data in this domain, super- vised techniques have only been applied to a limited set of popular programming languages. To bypass this limitation, unsupervised neural machine translation techniques have been proposed to learn code translation using only monolingual corpora. In this work, we propose to use document similarity methods to create noisy parallel datasets of code, thus enabling supervised techniques to be applied for automated code translation without having to rely on the availability or expen- sive curation of parallel code datasets. We explore the noise tolerance of models trained on such automatically-created datasets and show that these models per- form comparably to models trained on ground truth for reasonable levels of noise. Finally, we exhibit the practical utility of the proposed method by creating parallel datasets for languages beyond the ones explored in prior work, thus expanding the set of programming languages for automated code translation.\n\n1 I NTRODUCTION\n\n As the pace of software development increases and the famous adage \u201csoftware is eating the world\u201d (Andreessen, 2011) is borne out, there is a corresponding increase in the amount of source code and number of software artefacts in active use for which support has lapsed. At the same time, the number of software professionals and programmers who can support and understand such code is unable to keep pace with the rate at which it is produced. This problem, while important when it comes to relatively modern programming languages (such as Java and Python), becomes even more pressing when it come to legacy languages (like COBOL) that mission-critical applications and systems are written in (Charette, 2020). In recent years, there have been multiple instances of organizations struggling to maintain their legacy systems and making considerable investments to upgrade them. In 2021 the Commonwealth Bank of Australia upgraded its core banking platform originally written in COBOL: this ultimately took 5 years and more than 1 Billion AUD to com- plete (Irrera, 2017). During the COVID-19 pandemic, software systems implemented in COBOL slowed down the release of US unemployment stimulus checks (Kelly, 2020), leaving governments scrambling to \ufb01nd COBOL experts who were already hard to come by. A recent study by the United States Government Accountability Of\ufb01ce (Walsh, 2021) has identi\ufb01ed 65 critical federal legacy sys- tems in need of urgent modernization. Some of these systems are over 50 years old, and cost millions of dollars annually to operate and maintain. Parallel to these developments are recent efforts at the intersection of software engineering, machine learning (ML), and natural language processing (NLP), which have posited the naturalness hypoth- esis of software (Hindle et al., 2016). The hypothesis states that \u201c...Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools\u201d (Allamanis et al., 2018). This hypothesis has been used to extend breakthroughs and advances from various NLP sub-\ufb01elds to software engineering tasks such as code translation. Prior works in the code trans- lation domain have proposed the application of statistical, supervised, and unsupervised machine translation techniques to learn code translation models to varying degrees of success. 1 A key limitation of a majority of the proposed code translation approaches, however, is the lack of availability of parallel data for training. Unlike natural language, where a piece of text is verbatim translated in multiple languages \u2013 legal documents, parliamentary proceedings in multilingual soci- eties \u2013 code is rarely implemented as is in multiple languages; thus making it hard to create parallel datasets. A few limited datasets \u2013 such as Java \u2194 C# (Nguyen et al., 2013) and AVATAR for Java \u2194 Python (Ahmad et al., 2021b) \u2013 are currently available. However, these are extremely limited in the number of programming language they cover, and manually curating a dataset for a speci\ufb01c use-case is impractical. To bypass this limitation, unsupervised techniques have been applied to the code translation task. Unsupervised techniques come with their own limitations however; and often, supervised techniques can outperform them when the source and target corpora are from different domains, the source and target languages use different scripts, and on low-resource language pairs, among other concerns (Kim et al., 2020; Marchisio et al., 2020). It is for this reason that in this work, we focus on one of the main blockers impeding the application of supervised techniques to code translation: the availability of parallel corpora and datasets. Specif- ically, we propose to utilize document similarity methods to create parallel source code datasets that are noisy by design. In this work, we empirically demonstrate the effectiveness of document sim- ilarity methods in creating such parallel datasets with high levels of accuracy. Given that datasets created in this manner are bound to be noisy, we study the performance characteristics of models for code translation that have been trained on data with varying degrees of noise; and show that these models have considerable resistance to noise and perform well even with moderate amounts of noise. Finally, we demonstrate the practical utility of the proposed approach by training models to translate between 10 pairs of languages \u2013 a majority of which have not been looked at in prior work.\n\n2 R ELATED W ORK\n\n Code translation datasets: Typical methods for creating parallel datasets for code translation have either relied on the availability of open-sourced projects with implementations in multiple lan- guages, or on the existence of transpilers. The earliest widely-used large-scale dataset for code translation was for Java \u2194 C# (Nguyen et al., 2013) translation, created by indexing open-sourced projects implemented in both languages. Aggarwal et al. (2015) used the Python 2to3 1 transpiler to create a dataset; while Chen et al. (2018) used CoffeeScript\u2019s compiler (which compiles down to JavaScript) to create a parallel dataset. More recently, Ahmad et al. (2021b) released AVATAR \u2013 a parallel corpus of Java to Python manually curated through submissions on competitive program- ming websites. Publicly available datasets for code translation are however extremely limited, and manually curating these datasets for a speci\ufb01c use-case is expensive and often impractical. Source-to-Source translation: The earliest code translation models were rule-based systems, op- erating on handcrafted rules. These systems require a lot of effort to build, are not easily extend- able to other languages, and are also outperformed by neural techniques. Some of these systems are: Java2CSharp 2 , Java2Python 3 , SmallTalk to C (Yasumatsu & Doi, 1995), Cobol to Java (Mossienko, 2003), and Tangible Software Solutions 4 (VB.NET, C#, Java, C++, and Python). Moving away from rule-based systems, Nguyen et al. (2013), Karaivanov et al. (2014), and Nguyen et al. (2014) applied different versions of Phrase-Based Statistical Machine Translation to translate between Java and C#. Chen et al. (2018) proposed a tree-to-tree neural network to translate the parsed tree of the source code into the target code parse tree. The aforementioned supervised tech- niques have all been benchmarked on the Java \u2194 C# dataset, and are limited by the availability of parallel datasets. To bypass this limitation, Roziere et al. (2020) used unsupervised neural machine translation techniques to translate between languages using only monolingual corpora, and showed impressive results for translation between Java, C++, and Python. While Roziere et al. (2020) trained the model speci\ufb01cally for code translation, large language models \u2013 such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and Codex (Chen et al., 2021) \u2013 have also been shown to have some competence in generating code (Hendrycks et al., 2021). Parallel corpus mining: Prior work in natural language research has looked at various ways of creating parallel corpora from a non-parallel corpus. Munteanu & Marcu (2005) train a maximum 1 https:// docs.python . org/3/library/2to3 .html 2 https:// sourceforge .net/projects/ j2cstranslator / 3 https:// github.com/natural / java2python 4 https://www. tangiblesoftwaresolutions .com/ 2 entropy classi\ufb01er to identify if two given sentences are translations of each other. They extract parallel data from large-scale Chinese, Arabic, and English non-parallel newspaper corpora, and show improvement in model performance when trained with a combination of a small parallel corpus and the extracted dataset. Uszkoreit et al. (2010) describe a system that uses n-gram features to mine parallel documents from a billion-scale corpus. Smith et al. (2010) focus on aligning Wikipedia documents by creating features suitable for such documents. Artetxe & Schwenk (2019) utilize speci\ufb01c scoring functions based on multilingual sentence embeddings to create parallel corpora, and Hangya & Fraser (2019) rely on continuous parallel segments rather than word similarities to \ufb01nd parallel sentences. Ban\u00b4on et al. (2020) released the largest publicly available parallel corpora of sentences ( 223 million parallel sentences) by aligning sentences from data crawled over the web. There is a substantial precedence of parallel corpus mining in the natural language domain; however, such studies in the code translation domain are non-existent. Machine Translation using noisy data: Prior studies have aimed to study the impact of noise on the performance of machine translation systems. Formiga & Fonollosa (2012) study the impact of misspelled words on the performance of Statistical Machine Translation and suggest strategies to deal with them, while Goutte et al. (2012) study the impact of sentence alignment errors on the performance of SMT. Further, Khayrallah & Koehn (2018) de\ufb01ne 5 categories of arti\ufb01cial noise in Neural Machine Translation, and study the impact each of these types has on performance. We moti- vate our work from these prior efforts in order to study the impact that noise has on the performance of code translation models.\n\n3 P ROPOSED M ETHOD\n\n In this work, we propose to utilize document similarity methods to create noisy parallel datasets for code translation. We refer to the datasets created in this manner as \u201cnoisy\u201d because unlike manually curated datasets, there is no guarantee of a parallel implementation of the speci\ufb01c source \ufb01le/code being available in the corpus: this may result in near-similar code samples being paired as ground truth examples instead. Algorithm 1 presents the proposed approach as pseudocode. Algorithm 1: CreateParallelCorpora( D, D (cid:48) , M, \u03b4 ) 2: initialize P = {} 3: for i = 1 to | D | do 4: D sim = GetSimilarDocuments( d i , D (cid:48) , M ) 5: for ( d i , d (cid:48) j ) in D sim do 6: if ( \u00b7 , d (cid:48) j ) / \u2208 P and M ( d i , d (cid:48) j ) \u2264 \u03b4 then 7: P = P \u222a ( d i , d (cid:48) j ) 8: break 9: D res = sort (( d 1 , d 2 ) \u2208 P , key = M ( d 1 , d 2 )) 10: Return: D res The algorithm expects two non-parallel sets of documents D = { d 1 , \u00b7 \u00b7 \u00b7 , d n } and D (cid:48) = { d (cid:48) 1 , \u00b7 \u00b7 \u00b7 , d (cid:48) m } as input. Within the context of our work, the documents in these two sets represent code samples from two distinct pro- gramming languages. Along with the docu- ments, the algorithm also expects a similarity measure M ( d, d (cid:48) ) as input, to compare two given documents for similarity. A lower score from the similarity measure indicates higher similarity between documents. Finally, the algorithm expects a similarity threshold \u03b4 to help keep only suf\ufb01ciently similar documents in the resulting parallel corpus. Thereafter, the algorithm follows a simple procedure of iterating over all documents; \ufb01nding the most similar documents in the target set; and adding the newly found similar document pairs to the result only if the target document has not been paired be- fore, and if the similarity is below the threshold value. Once all the documents are iterated upon, the algorithm produces a list of unique pairs of code segments (documents) ordered by their similarity, ready to be used for downstream tasks.\n\n4 E XPERIMENTAL SETUP\n\n To better understand the effectiveness and practical utility of the method proposed in Section 3, we devise 3 research questions and design experiments to empirically answer them (see Section 5). In this section, we brie\ufb02y summarize the different document similarity methods, datasets, pre-trained models, and evaluation metrics we use in our experiments. 3\n\n4.1 D OCUMENT SIMILARITY METHODS TF-IDF:\n\n TF-IDF (Salton & Buckley, 1988) computes the product of the term frequency (TF) (fraction of times a term appears in a document) with the inverse document frequency (IDF) (log- arithm of the inverse fraction of documents a particular token occurs in). The cosine similarity between the document vectors thus created computes the similarity of documents.\n\nOkapi-BM25:\n\n The Okapi-BM25 model (Robertson et al., 1995) uses the following scoring func- tion (Equation 1) to score the importance of a word w in a document D . Here, IDF ( w ) represents the inverse document frequency of the word w , TF ( w, D ) represents the term frequency of the word w in the document D , | D | and D avg are the lengths of the current document and the average document lengths respectively, and k 1 and b are free parameters of the model. Latent Dirichlet Allocation (LDA): LDA (Blei et al., 2003) is a hierarchical generative Bayesian model that models each document as a \ufb01nite mixture over an underlying set of topics. The cosine similarity of the topic distribution of two documents computes their similarity. Latent Semantic Indexing (LSI): LSI (Deerwester et al., 1990) computes the Singular Value Decomposition of the Bag of Words representation of documents. The cosine similarity of the decomposed vectors of documents computes their similarity. Word Movers Distance (WMD): WMD (Kusner et al., 2015) models the document distance problem as a variant of the Earth Movers Distance (Monge, 1781; Rubner et al., 1998) and solves the optimization problem de\ufb01ned in Equation 2. subject to: Here, T \u2208 R n \u00d7 n is a \ufb02ow matrix where T ij \u2265 0 and denotes how much of word i in document d travels to word j in document d (cid:48) . c ( i, j ) = (cid:107) x i \u2212 x j (cid:107) 2 is the cost associated with travelling from one word to another, d and d (cid:48) are the nBOW representations of the documents, and X \u2208 R d \u00d7 n is the word embedding matrix where x i \u2208 R d represents the d -dimensional embedding of the i th word.\n\n4.2 D ATASETS\n\n For the experiments whose results are detailed in Section 5, we utilize the following datasets. We provide representative code samples and statistics from the datasets in Appendix A. Java \u2194 C#: The Java \u2194 C# dataset is one of the earliest large-scale datasets introduced for the code translation task (Nguyen et al., 2013; Zhong et al., 2010). It is created by indexing several open-source projects which have both Java and C# implementations, and pairing methods with the same \ufb01le name and method name. The earlier version of this data was created by indexing the db4o and Lucene projects. More recently however, Chen et al. (2018) indexed 6 open-sourced projects to create the dataset. We use the version provided by Chen et al. (2018) in our work. Java \u2194 Python, Java \u2194 C++, and Python \u2194 C++: Roziere et al. (2020) extracted parallel func- tions in C++, Python, and Java from the online competitive programming platform GeeksForGeeks 5 , 5 https://practice . geeksforgeeks . org/ 4 and used these code samples as validation and test sets. We, however, concatenate the two datasets and use the uni\ufb01ed dataset for our experiments. The code samples in this dataset are function-scope code samples that solve an algorithmic problem. CodeNet: Project CodeNet (Puri et al., 2021) is a recently released large-scale AI for Code dataset, created by indexing two online competitive programming websites. The dataset is organized into about 4000 different problem sets, and contains a little under 14 million total solutions in 55 pro- gramming languages. Besides providing the code samples, CodeNet also provides input-output pairs to evaluate solutions to the problem sets.\n\n4.3 M ODELS\n\n CodeBERT: CodeBERT (Feng et al., 2020) is a Transformer (Vaswani et al., 2017) based model, pre-trained on a unimodal data of function-level code samples, and a bimodal data of code and the associated documentation in natural language. The pre-training data contains code samples in Go, Java, JavaScript, PHP, Python, and Ruby, and is trained using the Masked Language Modeling (Devlin et al., 2019) (MLM) and the Replaced Token Detection (Clark et al., 2019) objectives. GraphCodeBERT: GraphCodeBERT (Guo et al., 2020) is a Transformer based model for code that also considers the inherent structure in code by integrating the data \ufb02ow in the pre-training stage. The model is trained on the CodeSearchNet dataset (Husain et al., 2019) using the MLM, Edge Prediction, and Node Alignment objectives. PLBART: PLBART (Ahmad et al., 2021a) is a BART (Lewis et al., 2020) based model pre-trained on over 700 million Java, Python, and natural language documents collected from open-sourced code on Github and posts on StackOver\ufb02ow. The model is pre-trained via denoising autoencoding, where the model learns to reconstruct input corrupted by a noise function. The authors use three noising strategies: token masking, token deletion, and token in\ufb01lling to create the corrupted inputs.\n\n4.4 E VALUATION METRICS\n\n BLEU score: BLEU score (Papineni et al., 2002) is a common automatic evaluation metric for machine-generated text, and exhibits a high correlation with human judgment of quality. BLEU score is computed as the overlapping fraction of n-grams between the machine-generated text and the reference text. The metric has however been shown to not be a reliable measure for source code (Ren et al., 2020; Allamanis et al., 2018; Austin et al., 2021). CodeBLEU score: Ren et al. (2020) propose the CodeBLEU score to leverage the tree structure and semantic information in code. It is computed as a combination of the standard BLEU score, weighted n-gram match, syntactic abstract syntax tree match, and the semantic data \ufb02ow match. Exact Match (EM): EM (Nguyen et al., 2013) evaluates if the generated code matches exactly to the reference code. Computational Accuracy @ k (CA@ k ): Recent work in code synthesis has adopted the CA@ k metric (Austin et al., 2021; Roziere et al., 2020) to evaluate code generation models. To compute CA@ k , k samples are generated from the model, and the problem is considered solved if any of the generated k samples pass the unit tests associated with the problem.\n\n5 R ESEARCH Q UESTIONS AND R ESULTS\n\n To validate the central hypothesis of this paper \u2013 using document similarity methods to create datasets for supervised training of code translation models \u2013 we de\ufb01ne and seek answers to the following research questions (RQ): RQ1: How accurate are document similarity methods in creating parallel datasets for code? RQ2: Given the created dataset will be noisy, what is the effect of varying degrees of noise on code translation models? RQ3: Can the proposed method be used in practice to train models for programming languages not explored in prior work? 5\n\n5.1 RQ1 : E FFICACY OF DOCUMENT SIMILARITY METHODS\n\n We start our analysis by examining how effective document similarity methods are in creating code translation datasets. For this experiment, we utilize 4 datasets with known ground-truth mapping between pairs of programming languages \u2013 Java \u2194 C#, Java \u2194 Python, Java \u2194 C++, and Python \u2194 C++. For each of these datasets, we create a parallel dataset using 5 different document similarity methods, and compute the match accuracy as the number of correctly matched code samples. We summarize the results for this experiment in Table 1 and observe that similarity methods that operate in a latent space (such as LDA and LSI) perform much worse than methods that operate in the original space (such as TF-IDF, Okapi-BM25 and WMD). We posit that because code is written in a more formal language than natural language, and each data sample in the datasets used in this experiment implements an independent unique function, there is likely no underlying topic or latent semantic associations that can be captured by LSI and LDA. Therefore these methods perform worse than methods that directly utilize the tokens in the original space. We note that the datasets used for the experiments in Table 1 are not true representatives of code we expect to \ufb01nd while using this method in practice. This is due to the fact that the 4 datasets used contain code samples for which a true parallel implementation in the other language exists. When trying to create a parallel dataset from code collected in the wild, we cannot be sure of the availability of a true parallel implementation, which might affect the performance of the proposed method. Thus, to account for this phenomenon, we conduct a similar experiment with the CodeNet dataset. Since code samples in the CodeNet dataset are not parallel implementations, this gives us a better idea of the effectiveness of document similarity methods in creating parallel datasets from code in the wild. We select 6 languages and randomly sub-sample 50 problems from the dataset. For each of the code sample in each of the 50 sampled problem sets, we create a parallel dataset using 3 different document similarity methods. Since we do not have a ground-truth parallel implementation available for the CodeNet dataset, we cannot compute the match accuracy like we did in the previous experiment. We therefore compute the pseudo-match accuracy instead. The pseudo-match accuracy computes True if the matched code sample is a solution of the same problem set, and False otherwise. We report the pseudo-match accuracy results for this experiment in Table 2. We note that while the TF-IDF and Okapi-BM25 methods performed well in the previous experiment, their performance varies greatly for this experiment. Datasets created using TF-IDF and Okapi-BM25 methods are matched to the correct problem set with as little as 30% accuracy and as high as 70% accuracy in some cases. Datasets created using the WMD method however achieve a high match accuracy for both experiments (Table 1 and Table 2). From the experiments designed to answer RQ1, we conclude that document similarity methods are capable of creating parallel datasets of code with a signi\ufb01cantly high degree of match accuracy. Speci\ufb01cally, the WMD metric seems to be quite adept at delineating datasets for code translation.\n\n5.2 RQ2 : N OISE TOLERANCE OF MODELS TRAINED ON CODE\n\n In the previous section, we showed that document similarity methods \u2013 and speci\ufb01cally the Word Movers Distance (WMD) \u2013 are quite adept at creating parallel datasets for code. However, datasets created in this manner contain errors; therefore in this section, we seek to understand the effect that varying the degree of such noise has on the performance of code translation models. 6 We use the CodeBERT and the GraphCodeBERT pre-trained models for this experiments, and \ufb01ne- tune these models on different pairings of the Java \u2194 C# dataset (Nguyen et al., 2013) created using the different document similarity methods. We compare the performance of models trained on these paired datasets with models trained on the ground-truth dataset, and a random baseline with random pairings of code samples from the two programming languages. Following Ahmad et al. (2021a), we compute the BLEU score, CodeBLEU score, and the Exact Match score. The results for this experiment are summarized in Table 3. We additionally refer the reader to Table 1 to see the corresponding match accuracy of the different document similarity methods. Interestingly, we \ufb01nd that models trained on noisy code datasets have a certain degree of resistance to noise; and while the performance drops with increasing levels of noise, the degradation is not sudden. Even with high levels of noise, the models perform considerably well. With a high-performing method such as the Word Movers Distance (WMD) \u2013 with about 90% match accuracy \u2013 the degradation in performance is roughly 1 percentage point across the three measures and for both directions of translation. For methods with a higher level of noise \u2013 such as LDA with 47.21% match accuracy \u2013 while the performance goes down signi\ufb01cantly, it is still signi\ufb01cantly higher than the performance of the random baseline. We posit that although noisy datasets create pairs of code with incorrect parallel implementations, much of the semantics is still retained by the dataset due to the formal nature of programming languages. For example, even if code samples are incorrectly paired, the syntax for function and variable de\ufb01nition, code blocks, and indentation stays the same and is preserved. This allows the model to learn the translation task to a certain degree. introduces a different amount of noise in the resulting dataset (see ) affecting While the preceding experiment allowed us to understand the performance of models trained on noisy datasets created using different document similarity methods, we wish to understand the per- 7 formance characteristics of models trained with varying levels of noise on a more granular level. Thus we create datasets for translation between Java and C# by arti\ufb01cially injecting noise of varying levels. For explanation, in a dataset with x % noise level, we randomly misalign x % of code sam- ples in that dataset, while keeping the remaining code samples correctly paired. We then train the GraphCodeBERT model on these datasets, and compute the BLEU score, CodeBLEU score, and the Exact Match (EM) score. Figure 1 shows the performance curve with levels of noise varying from 0% (ground-truth dataset) to 100% (complete random pairings). We observe that the degra- dation in the performance is gradual for initial levels of noise \u2013 compared to performance at 0% noise, the performance at about 30% noise goes down slowly by about 20 percentage points across all measures and both ways of translation. Post this 30% noise level, we see a sharper degradation in performance; and post 70% noise, we observe that the performance is just slightly better than the performance at 100% noise. Through the experiments in Table 3 and in Figure 1, we get a better idea of the performance charac- teristics of models for code under varying levels of noise. We conclude that the performance of these models is not severely affected with moderate amounts of noise; therefore when creating datasets from code in the wild where we expect a certain amount of noise, we can expect the models to perform reasonably well.\n\n5.3 RQ3 : T RANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES\n\n In Section 5.1, we concluded that document similarity methods are adept at creating parallel datasets for code with acceptable levels of noise; and in Section 5.2, we concluded that models trained on noisy datasets of code perform reasonably well under moderate levels of noise. In this section, we take advantage of these two \ufb01ndings and demonstrate the practical utility of the proposed method by creating noisy code translation datasets for languages not explored previously in the literature; and by training models for translating between these languages. For this experiment, we utilize the CodeNet dataset by creating noisy parallel datasets between the following 10 languages \u2013 C, C#, C++, Go, Java, JavaScript, PHP, Python, Ruby, and Scala. We choose these languages in order of their frequency in the CodeNet dataset, thereby maximizing the number of data samples we can potentially create. We also sub-sample about 2500 problem sets from the original 4000 problem sets from the CodeNet dataset for computational reasons. Thereafter, we match the solutions in one programming language to another for each of the 2500 sub-sampled problem sets using the WMD metric. Since the PLBART model can only translate sequences with a maximum of 512 tokens, we only match code samples with less than 512 tokens. We additionally use a similarity threshold of 3 . 0 and \ufb01lter samples accordingly. In Appendix B.3, we provide statistics of the \ufb01nal dataset along with some representative code samples and their corresponding similarity scores. To create the test set for the language pairs, we sub-sample 100 problems that are not seen in the training and the validation set, and randomly sample 5 different implementations in the source language from each of the 100 problem sets for a \ufb01nal test set size of 500 code samples. 8 C C# C++ Go Java JavaScript PHP Python Ruby Scala S o u r c e l a n g u a g e 80 24 40 C # Python We \ufb01ne-tune the PLBART model on the matched training data for each language pair, and evaluate the computational accuracy @ 5 on the test set. We compare the performance of this \ufb01ne-tuned model against a model \ufb01ne-tuned on a dataset created by randomly matching solutions from each problem set rather than using the WMD metric. To compare these two models fairly, we keep the dataset sizes, problem sets, and all the hyperparameters constant across the two training procedures. The CA@5 results for the model trained on the WMD-matched dataset are shown in Figure 2; while Figure 3 shows the difference in the performance of the model \ufb01ne-tuned using the WMD-matched data and the performance of the model \ufb01ne-tuned using the randomly matched data. We present some of the model generated code samples in Appendix B.4. Overall, we observe that models trained using the WMD-matched datasets achieve noteworthy per- formance across language pairs. More importantly, when compared with models trained on ran- domly paired data, we see substantial improvements for a majority of the language pairs. While the common language pairs, such as C \u2192 C++, Python \u2192 C++, Ruby \u2192 C++ see the biggest improve- ments, more obscure language pairs such as PHP \u2192 C++, PHP \u2192 Ruby, JavaScript \u2192 C++, and Python \u2192 Ruby also demonstrate substantial improvements over their random counterparts. This leads us to the conclusion that the proposed method is a viable way of creating high-quality datasets for code translation, thereby alleviating the paucity of training data in the domain.\n\n6 D ISCUSSION & C ONCLUSION\n\n Modernizing legacy applications into a new programming language is a process that requires a lot of time, intellect, and monetary investment. Automatic code translation techniques have the potential to speed up this process, and to reduce the human effort required by either working in tandem with humans or automatically translating legacy code to a modern language of choice. While multiple techniques have been proposed to improve the quality of code translation, their practical utility is hampered due to the limited availability of parallel data required to train these models between languages of choice. In this work, we proposed a simple technique to utilize document similarity methods to create noisy datasets for code translation; and demonstrated that models for code have a certain amount of tolerance for noise and perform well even under signi\ufb01cant amounts of noise. We speci\ufb01cally demonstrated the effectiveness of the Word Movers Distance (WMD) metric in creating parallel datasets between numerous language pairs that have not been explored in prior literature; and showed signi\ufb01cantly improved model performances as compared to models trained on randomly matched datasets. Future work will explore better metrics in terms of both match accuracy and computational ef\ufb01ciency, thereby further reducing the noise in the dataset; and incorporating the similarity score in the model to weight samples according to their computed similarity. 9\n\n7 E THICS S TATEMENT\n\n One of the major ethical points to consider when dealing with the automatic creation and translation of source code centers around the effects on humans: both humans who create and maintain code for a living; and humans that are affected by the decisions and outcomes produced by the execution of such code. For the former concern, our work merely seeks to align pre-existing bits of open- sourced code so that downstream data-hungry techniques may have more reasonable approximations of correct and on-purpose code to learn from. Our work does not replace jobs that humans are trained to do and more adept at; and indeed defers to and takes inspiration from prior studies (Weisz et al., 2021) that show that human generators of code are very likely to engage in partnerships with automatically learned models to produce or maintain code better. For the latter concern, we acknowledge that it is possible to use the output of artefacts from our work in downstream systems that can produce automatic code with little to no oversight. Similar to work on examining the effects of large language models for human natural languages (Bender et al., 2021), much attention is needed where it comes to automatic code generation and translation techniques and models. We look forward to studying some of these issues in partnership with colleagues in the future.\n\n8 R EPRODUCIBILITY S TATEMENT\n\n To allow for the reproducibility of experiments conducted in this work, we provide the source code of the experiments in the supplementary material attached with the submission. The supplementary material is grouped by the three research questions we de\ufb01ne in our work. Each set of \ufb01les within a folder corresponding to a research question contains the source code to the respective experiments. Another critical aspect of our work is the creation of parallel code translation datasets across many languages from the CodeNet dataset. Along with the source code, we also provide the train, valida- tion, and test data sets for a small subset of language pairs in the attached supplementary material. We provide a small subset due to the limitations on the amount of data that can be provided as supplementary material. However, we plan to release the complete noisy dataset we created for our experiments with the \ufb01nal version of the paper.\n\nR EFERENCES\n\n Karan Aggarwal, Mohammad Salameh, and Abram Hindle 10 11 12 13\n\nA R EPRESENTATIVE CODE SAMPLES FROM UTILIZED DATASETS\n\n In this section, we provide representative code samples from the datasets we use in this work. List- ings 1 and 2 are data samples from the Java \u2194 C# dataset. Listings 3 and 4 are data samples from the Java \u2194 Python dataset. Listings 5 and 6 are data samples from the Java \u2194 C++ dataset. Listings 7 and 8 are data samples from the C++ \u2194 Python dataset. Finally, listings 9, 10, 11, 12, 13, 14, 15, 16 provide code samples from one particular problem set from the CodeNet dataset. Listing 1: Java \u2194 C#: Java code sample Listing 2: Java \u2194 C#: C# code sample Listing 3: Java \u2194 Python: Java code sample Listing 4: Java \u2194 Python: Python code sample Listing 5: Java \u2194 C++: Java code sample findS ( int s ) { int sum = 0 ; for ( int n = 1 ; sum < s ; n ++ ) { sum += n * n ; if ( sum == s ) return n ; }return - 1 ; } Listing 6: Java \u2194 C++: C++ code sample int findS ( int s ) { int sum = 0; for ( int n = 1; sum < s; n ++ ) { sum += n * n; if ( sum == s ) return n; }return - 1; } Listing 7: C++ \u2194 Python: C++ code sample void printDistinct ( int arr [ ], int n ) { sort ( arr, arr + n ); for ( int i = 0; i < n; i ++ ) { while ( i < n - 1 && arr [ i ] == arr [ i + (cid:44) \u2192 1 ] ) i ++; cout << arr [ i ] << \" \"; } } Listing 8: C++ \u2194 Python: Python code sample def printDistinct ( arr , n ) : arr.sort ( ) ; for i in range ( n ) : if ( i < n - 1 and arr [ i ] == arr [ i + 1 (cid:44) \u2192 ] ) : while ( i < n - 1 and ( arr [ i ] == arr (cid:44) \u2192 [ i + 1 ] ) ) : i += 1 ; else : print ( arr [ i ] , end = \" \" ) ; Listing 9: CodeNet: C code sample #include<stdio.h>intmain(){ inti,j;for(i=1;i<10;i++){for(j=1;j<10;j++){ printf(\"%dx%d=%d\\n\",i,j,i*j);} }return0;} Listing 10: CodeNet: C# code sample using System; class test { static void Main() {for (int i = 1; i < 10; i++) for (int j = 1; j (cid:44) 14 Listing 11: CodeNet: Go code sample Listing 12: CodeNet: Java code sample Listing 13: CodeNet: JavaScript code sample Listing 14: CodeNet: PHP code sample Listing 15: CodeNet: Ruby code sample Listing 16: CodeNet: Scala code sample B P ARALLEL DATA CREATED FROM C ODE N ET In this section we look at the various properties of the parallel dataset created from the CodeNet dataset. In section B.1, we present most similar data samples identi\ufb01ed by the WMD metric across various language pairs. In section B.2, we present the histograms of similarity scores in the \ufb01nal dataset. Finally, in section B.3, we present the number of data samples in the \ufb01nal dataset created from the CodeNet dataset.\n\nB.1 I DENTIFIED MOST SIMILAR DATA SAMPLES ACROSS LANGUAGE PAIRS\n\n In table 4, we provide the most-similar code samples identi\ufb01ed by the WMD metric across various language pairs along with the computed similarity between the two samples. 15 , py, r) 0 println 16\n\nB.2 S IMILARITY HISTOGRAMS IN THE CREATED DATASET\n\n We show the histograms of the similarity scores in the datasets matched through the WMD metric on the CodeNet dataset in \ufb01gure 4.\n\nB.3 D ATA STATISTICS\n\n In table 5 we provide the number of data samples in the \ufb01nal dataset created from the CodeNet dataset. C \u00d7 2,295 15,534 709 8,900 1,354 837 10,499 5,564 1,247 C# 2,504 \u00d7 3,145 595 2,629 846 635 2,639 2,007 821 C++ 30,977 3,388 \u00d7 1,358 17,809 1,793 1,550 44,707 14,375 2,183 Go 1,732 974 2,496 \u00d7 2,077 642 669 1,755 1,315 729 Java 15,909 5,016 23,821 2,858 \u00d7 2,262 2,202 12,846 7,746 2,034 JS 2,876 1,907 2,954 998 2,653 \u00d7 1,238 2,893 2,307 1,349 PHP 2,802 1,741 2,898 1,180 2,454 1,389 \u00d7 2,996 2,310 1,133 Python 27,947 7,778 76,825 7,235 29,007 6,666 7,641 \u00d7 48,301 7,126 Ruby 28,423 12,595 48,985 8,838 23,515 8,322 9,252 62,298 \u00d7 6,457 Scala 5,506 4,720 6,660 3,343 5,953 3,536 3,249 6,937 5,586 \u00d7 17\n\n.4 C ODE GENERATED FROM FINE - TUNED PLBART MODEL\n\n In table 6, we show examples of code generated from a PLBART model \ufb01ne-tuned on a dataset created using the WMD metric. We show examples from C \u2192 Python, C# \u2192 Java, C++ \u2192 Ruby, Java \u2192 Ruby, JavaScript \u2192 C, PHP \u2192 Python, Python \u2192 C, Ruby \u2192 C, and Scala \u2192 Python language pairs. For a given input source code, we show an example of the correctly generated and incorrectly generated code. 18 19 20 = true; } } if($flg){echo\"Yes\"; }else{echo\"No\";} s = input() flg = False if \"A\" in s or \"B\" in s: flg = True else:if \"A\" in s: flg = True print(\"Yes\" if flg else \"No\") s = input() a = 0 b = 0 for i in range(len(s)): if \"A\" in s[i]: if \"B\" in s[i]: a += 1 else:b += 1 if a == b: print(\"Yes\") else: print(\"No\") Python \u2192 C translation Source Python code Correct C translation Incorrect C translation import sys N = int(input()) a = list(map(int, input().split (cid:44) \u2192 ())) ans = 0 count = 1 if 1 not in a : print(-1) sys.exit() for i in range(N) : if a[i] == count : count += 1 else : ans += 1 print(ans) #include <stdio.h> int main() {int n; scanf(\"%d\", &n); int i; int a[100005]; for (i = 0; i < n; i++) scanf(\"%d\", &a[i]); int ans = 0; int count = 1; for (i = 0; i < n; i++) { if (a[i] == count) count++; elseans++;} printf(\"%d\\n\", ans); return 0; } #include <stdio.h> int main(void){ int N; scanf(\"%d\", &N); int a[N]; for (int i = 0; i < N; i++) { scanf(\"%d\", &a[i]); }int ans = 0; int count = 1; for (int i = 0; i printf 21 } 22", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes using document similarity methods to automatically create noisy parallel datasets for training supervised code translation models.", "topic_class_1": "summary of their contributions", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors explore the effectiveness of various document similarity methods in creating parallel code datasets, study the tolerance of code translation models to varying degrees of noise in the training data, and demonstrate the practical utility of their approach by creating datasets and training models for translating between programming language pairs not explored in prior work.", "topic_class_1": "the experiments covered in this paper", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper addresses an important problem of lack of parallel datasets for code translation, which has limited the application of supervised techniques to this task.", "topic_class_1": "strength on the importance of the research problem", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method provides a way to automatically create such datasets.", "topic_class_1": "strength on methods", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors conduct a thorough comparison of different document similarity methods for creating parallel code datasets and identify Word Movers Distance (WMD) as the most effective.", "topic_class_1": "strength on experiment design and results", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper presents a detailed study of the noise tolerance of code translation models, showing that they can perform reasonably well even with moderate amounts of noise in the training data.", "topic_class_1": "strength on experiment design and results", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This is an important finding for the practicality of the proposed approach.", "topic_class_1": "opinion on the experiment finding", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The authors demonstrate the utility of their method by creating datasets and training models for 10 programming language pairs, expanding the coverage beyond what has been explored in prior work. The results show significant improvements over random baselines.", "topic_class_1": "strength on experiment design and results", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the paper shows promising results, the datasets created are still noisy by design.", "topic_class_1": "concerns on the approach", "topic_class_2": "Concern on noisy data", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The impact of this noise on downstream applications of the code translation models is not fully explored.", "topic_class_1": "concerns on the experiments", "topic_class_2": "Concern on noisy data", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The computational efficiency of using WMD at a large scale is not discussed. This could be a bottleneck for very large datasets.", "topic_class_1": "concerns on the approach", "topic_class_2": "Concern on the efficiency", "reliability": "No", "explanation": "This is a out-of-scope suggestion.", "error_type": "Out-of-scope"}, {"segment_text": "- The paper is lacking human evaluations of the quality of translations produced by the models trained on the noisy datasets. Automatic metrics are used but they may not fully capture translation quality.", "topic_class_1": "concerns on the evaluation metrics", "topic_class_2": "Concern on the lack of human labels", "reliability": "No", "explanation": "This is a convention. You can say every paper lack of human evaluation but its is out-of-scope for the topics using automantic metrics as a convention.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written and easy to follow.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology is clearly described and the results are presented in a logical manner.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The idea of using document similarity for parallel corpus creation is novel in the space of code translation.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments are conducted rigorously and the results demonstrate the effectiveness of the proposed approach.", "topic_class_1": "strength on experiments", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The datasets and code are provided which aids reproducibility.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and effective method for automatically creating parallel code datasets using document similarity techniques.", "topic_class_1": "summary of method", "topic_class_2": "Summary of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Thorough experiments are conducted to validate the approach, compare different similarity methods, and study model performance under different noise levels.", "topic_class_1": "summary of experiments", "topic_class_2": "Summary of evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The results are promising and the authors demonstrate the utility of the method by producing datasets and models for several new language pairs.", "topic_class_1": "summary of experiment results", "topic_class_2": "Summary of experimental results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this is a strong paper that makes a valuable contribution to the code translation field.", "topic_class_1": "opinion on the paper quality", "topic_class_2": "Summary of topics", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some weaknesses exist in evaluating downstream impact and scalability.", "topic_class_1": "opinion on the weakness", "topic_class_2": "Summary of weaknesses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces a novel approach to creating parallel datasets for code translation by using document similarity methods to pair similar code snippets from monolingual corpora.", "topic_class_1": "summary of the method", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This method enables the use of supervised machine learning techniques for code translation without the need for manually curated parallel corpora, thus addressing a significant limitation in the field.", "topic_class_1": "summary of the method", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper demonstrates that models trained on these automatically generated datasets can achieve performance comparable to those trained on manually curated datasets, even in the presence of noise.", "topic_class_1": "summary of the main experiments", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It extends the potential for code translation across a broader range of programming languages and highlights the noise tolerance of models trained on these datasets.", "topic_class_1": "summary of the implications of this paper", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Innovative approach to generating parallel code datasets, addressing a significant gap in resources for code translation tasks.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical validation demonstrates the effectiveness of the method across various programming languages, expanding the applicability of code translation techniques.", "topic_class_1": "strength on the comprehensiveness of validation", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study thoroughly explores the impact of noise on model performance, providing insights into the robustness of machine learning models in this domain.", "topic_class_1": "strength on the experiments", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The reliance on document similarity methods may introduce biases based on the chosen method, potentially affecting the generalizability of the approach.", "topic_class_1": "concerns on the generalization of the methods", "topic_class_2": "Concern on the potential bias", "reliability": "No", "explanation": "Every method has its biases; therefore, it should not be generally stated that the method of this paper has a bias which might  affect its generalization ability.", "error_type": "Out-of-scope"}, {"segment_text": "Limited exploration of the impact of different levels of noise across more diverse programming languages and contexts.", "topic_class_1": "concerns on the coverage of specific experiments", "topic_class_2": "Concern on the scope of evaluation", "reliability": "No", "explanation": "This study has presented sufficient programming languages and contexts to support its claims. This should not be a limitation.", "error_type": "Out-of-scope"}, {"segment_text": "The reproducibility of the results may be challenging due to the complexity of the setup and potential variability in the document similarity measures.", "topic_class_1": "concerns on the reproducibility", "topic_class_2": "Concern on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-structured, clearly presenting its objectives, methodology, experiments, and findings.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, employing rigorous experimental setups and analyses to validate the proposed approach.", "topic_class_1": "strength on the experiment design", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty lies in the method for generating parallel datasets for code translation, a significant contribution to the field.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, reproducibility might require access to specific datasets and clarity on the document similarity measures used.", "topic_class_1": "concerns on the reproducibility of the experiments", "topic_class_2": "Concern on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a significant contribution to the field of code translation by proposing a novel method for generating parallel datasets using document similarity methods.", "topic_class_1": "summary of the paper's contribution", "topic_class_2": "Summary of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This approach addresses a critical bottleneck in the field and opens new avenues for research and application in code translation across a wider range of programming languages.", "topic_class_1": "summary of the paper's contribution", "topic_class_2": "Summary of topics", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The study's thorough empirical validation and exploration of the noise tolerance of models add to its strength.", "topic_class_1": "summary of the paper's strength in experiments", "topic_class_2": "Summary of experimental results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, the potential for bias introduced by document similarity methods and challenges in reproducing the results warrant further investigation.", "topic_class_1": "summary of the weakness", "topic_class_2": "Summary of experimental results", "reliability": "No", "explanation": "Every method has its biases; therefore, it should not be generally stated that the method of this paper has a bias which might  affect its generalization ability.", "error_type": "Vague Critique"}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes using document similarity methods to automatically generate noisy parallel datasets for code translation.", "topic_class_1": "summary of the proposed method and its usage", "topic_class_2": "Summary of method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors argue that this approach can overcome the limitations of existing code translation methods that rely on scarce manually curated parallel data.", "topic_class_1": "summary of the authors' main claim", "topic_class_2": "Summary of claim", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They explore the effectiveness of various document similarity metrics, analyze the noise tolerance of models trained on such datasets, and demonstrate the practical utility by creating parallel datasets for languages not previously explored in code translation research.", "topic_class_1": "summary of the experiments in this paper", "topic_class_2": "Summary of experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Addresses a critical challenge: The paper tackles the major hurdle of limited parallel data in code translation, hindering the application of supervised learning techniques.", "topic_class_1": "strength on research goal", "topic_class_2": "Strength on the significance", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel approach: Applying document similarity methods to automatically generate parallel code datasets is a novel and promising direction.", "topic_class_1": "strength on the novelty of the approach", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical validation: The paper provides extensive experiments to evaluate the effectiveness of different document similarity metrics and the noise tolerance of trained models.", "topic_class_1": "strength on extensive evaluations", "topic_class_2": "Strength on the experiment", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Practical utility: The proposed method enables the creation of parallel datasets for a wider range of programming languages, expanding the scope of code translation research and applications.", "topic_class_1": "strength on the practicality of the method", "topic_class_2": "Strength on the method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Noise concerns: While the authors analyze noise tolerance, the inherent noise in automatically generated datasets could still impact translation quality and modelgeneralizability.", "topic_class_1": "concerns on the noise introduced by the approach", "topic_class_2": "Concern on the noisy data", "reliability": "No", "explanation": "The authors have shown that the noise introduced using their best method is not significant, and the models trained with generated data can perform nearly as good as ground truth data.", "error_type": "Neglect"}, {"segment_text": "Limited analysis of failure cases: The paper lacks a deeper analysis of why the proposed method performs poorly on certain tasks or language pairs.", "topic_class_1": "concerns on the lack of failure case analysis", "topic_class_2": "Concern on the lack of analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Scalability concerns: Identifying critical layers and false prefix-matching heads for each model/task combination might not be scalable for practical applications.", "topic_class_1": "concerns on scalability", "topic_class_2": "Concern on the scalability", "reliability": "No", "explanation": "This scalability issue is not clearly stated here. The authors did not mention anything like prefix-matching in the paper.", "error_type": "Unstated statement"}, {"segment_text": "Missing ablations: The study lacks some key ablations, such as analyzing the impact of partially incorrect demonstrations on model performance.", "topic_class_1": "concerns on missing ablations", "topic_class_2": "Concern on the lack of ablations", "reliability": "No", "explanation": "The \"partially incorrect\" demonstration is not clearly defined.\nThis suggestion is vague and can hardly be implemented in the framework of the paper.", "error_type": "Vague Critique"}, {"segment_text": "The authors should consider exploring techniques to further reduce noise in the generated datasets, potentially by incorporating the similarity score into the model training process.", "topic_class_1": "suggestion on how to reduce noise further ", "topic_class_2": "Concern on the noisy data", "reliability": "No", "explanation": "Incorporating the similarity score into the model training is not going to help reduce noise.", "error_type": "Out-of-scope"}, {"segment_text": "A deeper analysis of failure cases and the reasons behind them would be valuable to understand the limitations of the proposed method.", "topic_class_1": "suggestion on include failure case analysis", "topic_class_2": "Concern on the lack of analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Investigating the scalability of the approach for practical applications and exploring ways to address potential bottlenecks is crucial for future work.", "topic_class_1": "suggestion to investigate scalability", "topic_class_2": "Concern on the scalability", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Conducting additional ablations, such as analyzing the impact of partially incorrect demonstrations, would provide a more comprehensive understanding of the method's robustness.", "topic_class_1": "suggestion to include more ablation studies to understand the method's robustness", "topic_class_2": "Concern on the lack of ablations", "reliability": "No", "explanation": "This suggestion is vague and can hardly be implemented in the framework of the paper.", "error_type": "Vague Critique"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is clearly written and well-organized, making it easy to follow the proposed method and experimental results.", "topic_class_1": "strength on writing", "topic_class_2": "Strength on the writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is well-conducted, with comprehensive experiments and detailed analysis.", "topic_class_1": "strength on experiments and analysis", "topic_class_2": "Strength on the evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The application of document similarity methods for creating parallel code datasets is novel and holds significant potential.", "topic_class_1": "strength on novelty", "topic_class_2": "Strength on the novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors provide the source code for their experiments, facilitating reproducibility and further research.", "topic_class_1": "strength on reproducibility", "topic_class_2": "Strength on the reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "section title", "topic_class_2": "Section title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and promising approach to address the challenge of limited parallel data in code translation.", "topic_class_1": "summary of the paper's research problem", "topic_class_2": "Summary of contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed method of using document similarity metrics to automatically generate parallel datasets shows encouraging results, with models exhibiting reasonable noise tolerance.", "topic_class_1": "summary of the approach ", "topic_class_2": "Summary of experimental results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "However, concerns remain regarding the impact of noise on translation quality and the scalability of the approach for practical applications.", "topic_class_1": "summary of the concerns", "topic_class_2": "Summary of concerns", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further investigation into failure cases and additional ablations are needed to strengthen the research.", "topic_class_1": "summary of potential improvements needed", "topic_class_2": "Summary of concerns", "reliability": "No", "explanation": "This suggestion is vague and can hardly be implemented in the framework of the paper.", "error_type": "Vague Critique"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "accepted", "title": "Out-of-Distribution Detection and Selective Generation for Conditional Language Models", "body_text": "\n\nO UT - OF -D ISTRIBUTION D ETECTION AND S ELECTIVE G ENERATION FOR C ONDITIONAL L ANGUAGE M ODELS\n\n Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Machine learning algorithms typically assume independent and identically dis- tributed samples in training and at test time. Much work has shown that high- performing ML classi\ufb01ers can degrade signi\ufb01cantly and provide overly-con\ufb01dent, wrong classi\ufb01cation predictions, particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be gener- ated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demon- strate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classi\ufb01cation) of high-quality outputs, while automatically abstaining from low-quality ones, en- abling safer deployment of generative language models.\n\n1 I NTRODUCTION\n\n Recent progress in generative language models (Wu et al., 2016a; Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020) has led to quality approaching human-performance on research datasets and has opened up the possibility of their wide deployment beyond the aca- demic setting. In realistic user-facing scenarios such as text summarization and translation, it should be expected that user provided inputs can signi\ufb01cantly deviate from the training data distribution. This violates the independent, identically-distributed (IID) assumption commonly used in evaluating machine learning models. Many have shown that performance of machine learning models can degrade signi\ufb01cantly and in surprising ways on OOD inputs (Nguyen et al., 2014; Goodfellow et al., 2014; Ovadia et al., 2019). For example an image classi\ufb01er may detect cows in images with very high accuracy on its IID test set but con\ufb01dently fails to detect a cow when paired with an unseen background (Murphy, 2023; Nagarajan et al., 2020). This has led to active research on OOD detection for a variety of domains, including vision and text but focused primarily on classi\ufb01cation. Salehi et al. (2021); Bulusu et al. (2020); Ruff et al. (2021) provide comprehensive reviews on this topic. Conditional language models are typically trained given input sequence x = x 1 . . . x L to auto- regressively generate the next token in a sequence y = y 1 . . . y T as a classi\ufb01cation over the token-vocabulary V , p \u03b8 ( y | x ) = (cid:81) Tt =1 p \u03b8 ( y t | y <t , x ) , y t \u2208 V . Consequently, the perils of out-of- distribution are arguably more severe as (a) errors propagate and magnify through auto-regression, and (b) the space of low-quality outputs is greatly increased as arbitrary text sequences can be gen- erated. Common errors from text generation models include dis\ufb02uencies (Holtzman et al., 2020) and factual inaccuracies (Goodrich et al., 2019; Maynez et al., 2020). In this work, we propose OOD detection methods for CLMs using abstractive summarization and translation as case studies. Similar to classi\ufb01cation, we show in Section 2.1 that CLMs have un- trustworthy likelihood estimation on OOD examples, making perplexity a poor choice for OOD detection. In Section 2.2, we propose a highly-accurate, simple, and lightweight OOD score based on the model\u2019s input and output representations (or embeddings) to detect OOD examples, requiring negligible additional compute beyond the model itself. 1 While accurate OOD detection enables the conservative option of abstaining from generation on OOD examples, it may be desirable to generate on known near-domain data, e.g. generate sum- maries for articles from news publishers that differ from our \ufb01ne-tuning set. Thus the ability to selectively generate where the model is more likely to produce higher-quality outputs, enables safer deployment of conditional language models. We call this procedure selective generation , analogous to the commonly used term selective prediction in classi\ufb01cation (Geifman & El-Yaniv, 2017). In Section 4, we show that while model perplexity is a reasonable choice for in-domain examples, combining with our OOD score works much better when the input distribution is shifted. In summary, our contributions are: \u2022 Propose lightweight and accurate scores derived from a CLM\u2019s embeddings for OOD de- tection, signi\ufb01cantly outperforming baselines on abstractive summarization and translation tasks, without the need for a separate detection model. \u2022 Show that model perplexity can be an unreliable signal for quality estimation on OOD ex- amples, but combined with our OOD scores can be used effectively to selectively generate higher-quality outputs while abstaining on lower ones. \u2022 Propose an evaluation framework for OOD detection and selective generation for CLMs, including human quality ratings for summarization.\n\n2 OOD D ETECTION IN C ONDITIONAL L ANGUAGE M ODELS\n\n The maximum softmax probability (MSP), p ( y | x ) , y = arg max k =1 ,...,K p ( k | x ) is a simple, com- monly used OOD score for K -class classi\ufb01cation problem (Hendrycks & Gimpel, 2016; Lakshmi- narayanan et al., 2017). For CLMs, the perplexity, which is monotonically related to the negative log-likelihood of the output sequence averaged over tokens \u2212 1 T (cid:80) Tt =1 log p ( y t | y <t , x ) is a natural OOD score to consider, and analogous to the negative MSP in classi\ufb01cation. We \ufb01rst study how well the perplexity performs for OOD detection tasks.\n\n2.1 P ERPLEXITY IS ILL - SUITED FOR OOD DETECTION\n\n In Figure 1, we compare the distribution of perplexity of (a) a summarization model and (b) a translation model trained on in-domain dataset and evaluated on multiple OOD datasets, respec- tively. For summarization, a model is trained on xsum and evaluated on other news datasets in- cluding cnn dailymail and newsroom as near-OOD datasets, and forum ( forumsum ) and dialogue ( samsum and reddit tifu ) datasets as far-OOD (see Section 3 for details). The perplexity distribu- tions overlap signi\ufb01cantly with each other even though the input documents are signi\ufb01cantly differ- ent. Furthermore, perplexity assigns cnn dailymail even lower scores than the in-domain xsum . For translation, the model is trained on WMT15 dataset and evaluated on other WMT test splits (Bojar et al., 2015), OPUS100 (Aulamo & Tiedemann, 2019), and MTNT (Michel & Neubig, 2018). The in-domain and OOD datasets perplexity densities overlap even more. Overall, these results suggest that perplexity is not well suited for OOD detection. 2\n\n2.2 D ETECTING OOD USING CLM\u2019 S EMBEDDINGS\n\n Given a trained conditional language model, we propose using the input and output representa- tions/embeddings computed as part of the inference/generation process to detect OOD examples. In this work, we use Transformer encoder-decoder models and obtain the input embedding z by averaging the encoder\u2019s \ufb01nal-layer hidden state vectors h i \u2208 R d ( d is the hidden dimension) cor- responding to the input sequence token x i . To obtain the output embedding w we average the decoder\u2019s \ufb01nal-layer hidden state vectors g i \u2208 R d corresponding to the output token y i . Thus 1 where L and T are the input and output sequence lengths respectively. Figure 2 illustrates the idea. Intuitively, if the embedding of a test input or output is far from the embedding distribution of the training data, it is more likely to be OOD. One way of measuring this distance is to \ufb01t a Gaussian, N ( \u00b5 , \u03a3 ) , \u00b5 \u2208 R d , \u03a3 \u2208 R d \u00d7 d , to the training embeddings and use the Mahalanobis distance (MD) : ) This has been used for OOD detection using the representations from classi\ufb01cation models (Lee et al., 2018) and computing the distances to class-conditional Gaussians. Unlike classi\ufb01cation, which has class labels, in conditional language modeling we have paired input and output text sequences. We \ufb01t one Gaussian on the training input embeddings, N ( \u00b5 z , \u03a3 z ) , and a second Gaussian on the embeddings of the training ground-truth outputs, N ( \u00b5 w , \u03a3 w ) . For a test input and output embedding pair ( z test , w test ) , the input MD is computed as The output MD is computed similarly: Mahalanobis distance is equivalent to computing a negative log-likelihood of the Gaussian distribu- tion (up to a constant and a scalar), i.e. \u2212 log p ( z ) = d 2 log(2 \u03c0 ) + 12 log | \u03a3 | + 12 ( z \u2212 \u00b5 ) T \u03a3 \u2212 1 ( z \u2212 \u00b5 ) = const. + 12 MD ( z ) . Ren et al. (2019) showed that normalizing the likelihood with the likeli- hood of a background model works better for OOD detection. In a similar vein, Ren et al. (2021) proposed an analogous Relative Mahalanobis Distance (RMD) for classi\ufb01cation: using the rela- tive distance between the class-conditional Gaussians and a single background Gaussian using data from all classes. That method cannot be directly applied for CLMs because outputs are not just class labels. Thus in this work, we extend the RMD idea to conditional language models, where MD 0 ( z test ) := MD ( z test ; \u00b5 z 0 , \u03a3 z 0 ) is the MD to a background Gaussian N ( \u00b5 z 0 , \u03a3 z 0 ) , \ufb01t using a large, broad dataset to approximately represent all domains. In practice, we use C4 , a large Com- mon Crawl-based English dataset (Raffel et al., 2020) 1 and ParaCrawl \u2019s English -French dataset 1 https://www.tensor\ufb02ow.org/datasets/catalog/c4 3 (Ba\u02dcn\u00b4on et al., 2020) 2 , as the data for \ufb01tting the background distributions for summarization and translation in our experiments, respectively. While we use the ground-truth outputs to \ufb01t N ( \u00b5 w , \u03a3 w ) , we do not have ground-truth in general for the background examples (e.g. C4 ). We decode outputs from the trained CLMs and use those output embeddings to \ufb01t the background output Gaussian, N ( \u00b5 w \u03b4 , \u03a3 w \u03b4 ) . where MD \u03b4 ( w test ) := MD ( w test ; \u00b5 w \u03b4 , \u03a3 w \u03b4 ) is the MD to the decoded output background distribution N ( \u00b5 w \u03b4 , \u03a3 w \u03b4 ) . See Algorithm 1 and 2 for the detailed steps. The RMD score can be regarded as a background contrastive score that indicates how close the test example is to the training domain compared to the background domains. A negative score suggests the example is relatively in-domain, while a positive score suggests the example is OOD. A higher score indicates greater OOD-ness. Binary classi\ufb01er for OOD detection Since we have implicitly de\ufb01ned two classes, in-domain and background/general domain, another option is to train a binary classi\ufb01er to discriminate embeddings from the two classes. We train a logistic regression model and use the un-normalized logit for the background as an OOD score. The Input Binary logits OOD score uses the input embeddings as features, whereas the Output Binary logits OOD score uses the decoded output embeddings as features. A higher score suggests higher likelihood of OOD. The preferred use of the logits over probability was also recommended by previous OOD studies for classi\ufb01cation problems (Hendrycks et al., 2019). Though RMD is a generative-model based approach and the binary classi\ufb01er is a discriminative model, we show that RMD is a generalized version of binary logistic regression and can be reduced to a binary classi\ufb01cation model under certain conditions (see Section A.5 for details).\n\n3 E XPERIMENTS : OOD DETECTION 3.1 E XPERIMENT SETUP\n\n We run our experiments using Transformer (Vaswani et al., 2017) encoder-decoder models trained for abstractive summarization and translation. Below we specify the dataset used for training/\ufb01ne- tuning (i.e. in-domain) and the OOD datasets. In the case of summarization, OOD datasets can be intuitively categorized as near or far OOD based on the nature of the documents. For example, news articles from different publishers may be considered as sourced from different distributions, but are closer than news articles are to dialogue transcripts. In contrast, the translation datasets we use consist of English-French sentence pairs with less variation between datasets due to the shorter length of sentences.\n\nSummarization model\n\n We use a PEGASUS LARGE model (Zhang et al., 2020) \ufb01ne-tuned on the xsum (Narayan et al., 2018) dataset, consisting of BBC News articles with short, abstractive sum- maries.\n\nSummarization datasets\n\n We use 10 , 000 examples from xsum and C4 training split to \ufb01t in- domain/foreground and background Gaussian distributions, respectively. For test datasets, we have cnn dailymail (Hermann et al., 2015; See et al., 2017), news articles and summaries from CNN and DailyMail; newsroom (Grusky et al., 2018), article-summary pairs from 38 major news publi- cations; reddit tifu (Kim et al., 2018), informal stories from sub-reddit TIFU with author written summaries of very diverse styles; samsum (Gliwa et al., 2019) and forumsum (Khalman et al., 2021), high-quality summaries of casual dialogues.\n\nTranslation model\n\n We train a Transformer base model (Vaswani et al., 2017) with embedding size 512 on WMT15 English-French (Bojar et al., 2015). The model is trained with Adafactor optimizer (Shazeer & Stern, 2018) for 2M steps with 0.1 dropout and 1024 batch size. Decoding is done using beam search with 10 beam size and \u03b1 = 0 . 6 length normalization (Wu et al., 2016b). The best checkpoint scores 39.9 BLEU on newstest2014.\n\nTranslation datasets\n\n We use 100 , 000 examples from WMT15 En-Fr and the same number of examples from ParaCrawl En-Fr to \ufb01t the foreground and background Gaussian distributions, 2 https://www.tensor\ufb02ow.org/datasets/catalog/para crawl 4 respectively. For test, we use newstest2014 ( nt14 ), newsdiscussdev2015 ( ndd15 ), and newsdis- cusstest2015 ( ndt15 ) from WMT15 (Bojar et al., 2015) and the law , Koran , medical , IT and subtitles ( sub ) subsets from OPUS100 (Tiedemann, 2012; Aulamo & Tiedemann, 2019). We also use the English-French test set of MTNT (Michel & Neubig, 2018), consisting of noisy comments from Reddit.\n\nEvaluation metric\n\n We use the area under the ROC curve (AUROC) between the in-domain test data as negative and the OOD test data as positive sets to evaluate and compare the OOD detection performance. AUROC 1.0 means a perfect separation, and 0.5 means the two are not distinguishable.\n\nBaseline methods\n\n We compare our proposed OOD scores with various baseline methods, includ- ing (1) the model perplexity score, (2) the embedding-based Mahalanobis distance. In addition, we also compare with (3) Natural Language Inference (NLI) score (Honovich et al., 2022) for sum- marization task, and (4) COMET (Rei et al., 2020) and (5) Prism (Thompson & Post, 2020) for translation task. NLI score measures the factual consistency by treating the input document as a premise and the generated summary as a hypothesis. Both COMET and Prism are quality estima- tion metrics that are designed to measure translation quality without access to a human reference. More speci\ufb01cally, COMET \ufb01netunes the large XLM-R model (Conneau et al., 2020) on human evaluation data, and Prism is the perplexity score from a multilingual NMT model trained on 99.8M sentence pairs in 39 languages.\n\n3.2 R ESULTS\n\n RMD and Binary classi\ufb01er are better at OOD detection than baselines Table 1 shows the AUROCs for OOD detection on the (a) summarization and (b) translation datasets. Overall, our proposed OOD scores RMD and Binary logits outperform the baselines with high AUROCs (above 0.8). The commonly used output metrics, perplexity, NLI, COMET and Prism, have generally low AUROC scores (many have values around 0.5-0.6), suggesting they are not suited for OOD detec- tion. Interestingly, we noticed that the output OOD scores perform better for summarization, while 5 the input OOD scores perform better for translation. One possible reason is that when summariza- tion outputs are low-quality (e.g. producing repeated text or irrelevant summaries) they look very different than reference summaries, making OOD output score more sensitive to the contrast. Though RMD and Binary logits OOD scores both perform well at OOD detection, RMD OOD score is better at distinguishing near-OOD from far-OOD . This can be seen in Figure 3 where near-OOD datasets have scores distributed in between in-domain and far-OOD. In the summariza- tion task, near-OOD (news articles) datasets cnn dailymail and newsroom have their RMD scores distributed in the middle of xsum and reddit tifu , forumsum and samsum . In contrast, under the binary logits score, the near-OOD and far-OOD datasets have largely overlapping score distributions making it hard to distinguish between the two. In practice, RMD OOD score may be better suited for selective generation where domain shifts are expected. We explore this in more detail in Section 4. For the translation task, we additionally note that all methods have small AUROC for law dataset, suggesting that none of the methods are detecting the dataset as OOD. To better understand the special characteristics of the law dataset, we conducted an n -gram overlap analysis between the various test sets including law and the in-domain training data. We observed that law has the highest unigram overlap rate (48.8%) and the second highest overall overlap with the in-domain data (Table A.7). 3 This con\ufb01rms that law is actually not OOD data and explains why no method can detect it.\n\n4 U SING OOD SCORES FOR SELECTIVE GENERATION\n\n The most conservative option for deployment of a conditional language model is to completely abstain from generating on inputs that are detected as out-of-distribution, for which we have shown in Section 3 our OOD scores are fairly accurate. However, it is often desirable to expand the use of models beyond strictly in-distribution examples, if the quality of outputs is suf\ufb01ciently high. In classi\ufb01cation, this has been framed as determining when to trust a classi\ufb01er, or selective prediction (Geifman & El-Yaniv, 2017; Lakshminarayanan et al., 2017; Tran et al., 2022). In this section, we seek to predict the quality of generation given an example, which may be out-of-distribution and abstain if the predicted quality is low. We call this selective generation . In practice, abstaining may correspond to hiding the model\u2019s generated text, or turning off a summarization/translation feature.\n\n4.1 E XPERIMENT SETUP\n\n We use the same models and datasets described in Section 3.1 but instead of simply detecting out- of-distribution examples, our focus now is to predict the quality of generation for examples possibly outside the training distribution.\n\nMeasuring Translation quality\n\n We use BLEURT (Pu et al., 2021) as the main metric to mea- sure translation quality. Previous work has demonstrated that neural metrics such as BLEURT are much better correlated with human evaluation, on both the system level and the sentence level (Fre- itag et al., 2021). BLEURT scores range from 0 to 1, with higher scores indicating better translation quality. 3 We de\ufb01ne overlap rate as the percentage of unique n -grams in the test set that are also present in the in- domain data. The overall overlap is de\ufb01ned as the geometric mean of all the n -gram overlap rates up to n = 4 . All domains/splits including the in-domain data are subsampled to 1K for this analysis. 6\n\nMeasuring Summarization quality\n\n In general, it is unclear how to automatically measure the quality of summaries generated by a model on out-of-distribution examples (in this case, exam- ples from different datasets). The reason is summarization datasets have dataset-speci\ufb01c summary styles that may be dif\ufb01cult to compare. For example, xsum summaries are typically single-sentence whereas cnn dailymail summaries consist of multiple sentences. Thus we report ROUGE-1 score as an automatic measure but primarily use human evaluation to assess the quality. Amazon Me- chanical Turk workers were asked to evaluate summaries generated by the xsum model on a scale of 1-5 (bad-good) using 100 examples from xsum , cnn dailymail , reddit tifu , and samsum . We collected 3 ratings per example and took the median to reduce inter-rater noise. Details about MTurk setup can be found in Section A.3.\n\n4.2 P ERPLEXITY HAS DIMINISHING CAPABILITY IN PREDICTING QUALITY ON OOD DATA\n\n Since the models are trained using negative log-likelihood as the loss, perplexity (which is mono- tonically related) is a good predictor of output quality for in-domain data. In fact, the Kendall rank correlation coef\ufb01cient \u03c4 between perplexity and human judged quality score is 0.256 (See Table 2) for in-domain xsum for summarization. However, when including shifted datasets to test, we found that the perplexity score is worse at predicting quality on OOD data. For example the Kendall\u2019s \u03c4 de- creases to 0.068 for OOD dataset samsum (see Table A.2). We observed similar trend in translation, although less severe, as data shifted from in-domain to OOD, the Kendall\u2019s \u03c4 between perplexity and BLEURT decreases (see Table A.3). Figure 4 further shows the correlation between perplexity and the quality score (ROUGE-1, human rating, and BLEURT, respectively) as a function of OOD score. It is clear to see the correlation decreasing as OOD score increases and the trend is consistent for both summarization\n\n4.3 C OMBINING OOD SCORES AND PERPLEXITY\n\n While model perplexity for quality estimation is worse for OOD examples, we observed that our OOD scores and perplexity are complementary in quality prediction. Figure A.1 shows a 2-D plot between the OOD score and perplexity regarding quality. We can see that neither perplexity nor OOD score can perfectly separate good and bad examples, and the combination of the two can work much better. Our observation echos work in uncertainty estimation in classi\ufb01cation models (Mukhoti et al., 2021): perplexity based on softmax predictive distribution is regarded as an estimation for aleatoric uncertainty (caused by inherent noise or ambiguity in data), and the OOD distance based on representation estimates the epistemic uncertainty (caused by a lack of training data), and combining the two provides a comprehensive estimation of uncertainty. We propose two simple methods to combine perplexity and OOD scores. (1) A simple linear re- gression, trained on a random 10% data split using ROUGE-1 or BLEURT as the quality score, and evaluated on the test split and human evaluation split. (2) the sum of the percentile ranks (PR) of the scores, i.e. PR sum = PR perplexity + PR OOD . We sum PRs instead of their raw values because the two scores are in different ranges, PR ( x ) = R ( x ) N \u00d7 100 , where R is x \u2019s rank in the list of size N . Table 2 shows the Kendall\u2019s \u03c4 correlation coef\ufb01cient between the various single and combined scores and the quality metric with only in-domain and all examples from all datasets. When all datasets are merged, the combined scores signi\ufb01cantly improve the correlation over perplexity by up to 7 12% (absolute) for summarization and 8% for translation, while the gains over the best exter- nal model-based (and much more expensive) baselines are 4% and 3% . The two combination methods perform similarly. See Tables A.2 and A.3 for an expanded table of scores.\n\n4.4 S ELECTIVE GENERATION USING THE COMBINED SCORE\n\n In selective generation, our goal is to generate when the model is more likely to produce high- quality output, and abstain otherwise, enabling safer deployment of generative language models. To evaluate that, we propose using the Quality vs Abstention Curve (QA) , analogous to accuracy versus rejection curve used for selective prediction in the classi\ufb01cation. Similar concepts were proposed also in Malinin & Gales (2020); Xiao et al. (2020). Speci\ufb01cally, at a given abstention rate \u03b1 , the highest \u03b1 -fraction scoring examples are removed and the average quality of remaining examples is computed. We want to maximize the quality of what is selectively generated and a better curve is one that tends to the upper-left which corresponds to removing bad examples earlier than good ones. Figure 5 shows the QA curves for various methods on summarization and translation tasks. Quality is measured by human evaluation for summarization (similar ROUGE-1 plot in Figure A.3), and BLEURT for translation. The combined scores have the highest quality score at almost all ab- stention rates for both summarization and translation , and the linear regression and the PR sum perform similarly. For single scores, the OOD score performs better than the perplexity and NLI scores at almost all abstention rates for summarization. For translation, the OOD score is better than perplexity when abstention rate \u03b1 > 0 . 5 and worse than perplexity when \u03b1 < 0 . 5 . In other words, OOD score is a better at abstaining slightly far-OOD while perplexity is better at abstaining near-OOD examples. Interestingly, we see our simple combined score is even marginally better than COMET, which is a separate neural network model trained on annotated human evaluation data while our combined score does not use any human annotation supervision. Prism is better than sin- gle scores, but much worse than our combined score. Area under the QA curves are shown in Tables A.4 and A.6 for reference. Figures 5 (b, d) are the corresponding survival curves showing how many examples per dataset are selected for generation as a function of abstention rate, based on the PR sum score. For summarization, the samples from far-OOD datasets reddit tifu and samsum are eliminated \ufb01rst with their sample count decreasing rapidly. The near-OOD dataset cnn dailymail and in-domain xsum are kept intact until \u03b1 > 0 . 3 , and in-domain xsum examples survive the longest. Similarly for translation, the out-of-domain and worst-quality (as seen in Table A.1b) Koran , MTNT , and subtitles examples are eliminated \ufb01rst, and the best-performing law and in-domain datasets are abstained last. The order in which datasets are eliminated corresponds to the aggregate quality by dataset , which we report in Table A.1.\n\n5 R ELATED W ORK\n\n OOD detection problem was \ufb01rst proposed and studied in vision classi\ufb01cation problems (Hendrycks & Gimpel, 2016; Liang et al., 2017; Lakshminarayanan et al., 2017; Lee et al., 2018; Hendrycks et al., 2018; 2019), and later in text classi\ufb01cation problems such as sentiment analysis (Hendrycks 8 et al., 2020), natural language inference (Arora et al., 2021), intent prediction (Liu et al., 2020a; Tran et al., 2022), and topic prediction (Rawat et al., 2021). The widely used OOD methods can be characterized roughly into two categories (1) softmax probability or logits-based scores (Hendrycks & Gimpel, 2016; Liang et al., 2017; Hendrycks et al., 2019; Liu et al., 2020b), (2) embedding-based methods that measure the distance to the training distribution in the embedding space (Lee et al., 2018; Ren et al., 2021; Sun et al., 2022). OOD detection problem is relatively less studied in CLMs. A few studies explored the OOD detec- tion in semantic parsing (Lukovnikov et al., 2021; Lin et al., 2022), speech recognition (Malinin & Gales, 2020), and machine translation (Malinin et al., 2021; Xiao et al., 2020), but many of them focus on ensemble-based methods like Monte Carlo dropout or deep ensemble which sample multi- ple output sequences auto-regressively and use the averaged perplexity as the uncertainty score. The ensembling method costs N times of the inference time, which is not feasible in practice. In this work, we focus on developing scores that can be readily derived from the generative model itself, without much increase in computation.\n\n6 C ONCLUSION AND F UTURE WORK\n\n We have proposed lightweight and accurate scores to detect out-of-distribution examples for con- ditional language generation tasks. For real-world deployment, we have also shown how our OOD scores can be combined with language model perplexity to selectively generate high-quality outputs while abstaining from low-quality ones in the setting of input distribution shift. Although our experiments focus on summarization and translation, our methods do not make any assumptions about the task modality, and we believe our method is widely applicable to other tasks where the model output is a sequence, e.g. image captioning. While our analysis was restricted to conditional language modeling with encoder-decoder Transformers, we expect our method to also work with decoder-only (Liu et al., 2018) architectures, used by some large language models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al., 2022). Finally, analyzing why certain examples are OOD could lead to insights in how to make models more robust. Appendix A.9 presents one possible way to attribute OOD scores to sentences along with a visualization. 9\n\nR EFERENCES\n\n 10 3905\u20133920, 11 18653/v1/D18 1206 URL https: aclanthology.org/D18-1206 12 .org 1099 13 14 A\n\nA\n\n PPENDIX\n\nA\n\n .1\n\nT HE OUTPUT QUALITY FOR SUMMARIZATION AND TRANSLATION DATASETS .\n\n\n\nA.2 OOD SCORE AND PERPLEXITY ARE COMPLEMENTARY FOR PREDICTING OUTPUT QUALITY .\n\n 15\n\nA.3 A MAZON M ECHANICAL T URK ASSESSMENT OF SUMMARY QUALITY\n\n A PEGASUS LARGE model \ufb01ne-tuned on xsum was run on a random sample of 100 examples from the test split of four datasets: xsum , cnn dailymail , reddit tifu , samsum . Each example was rated for general summarization quality on a rating of 1-5 by 3 AMT workers using the template shown in Figure A.2. Workers were required to be Masters located in the US with greater than 95% HIT Approval Rate, with at least 1000 HITs approved and were paid $0.80 per rating. The median rating was used to reduce noise. 16\n\nA.4 A LGORITHM FOR RMD OOD SCORES\n\n 1: Input: CLM M with encoder f e and decoder f d trained on in-domain train set D intrain = { ( x , y ) } . A large and background dataset such as C4 or ParaCrawl D bgtrain = { ( x , \u02c6 y ) } , where \u02c6 y = M ( x ) . 2: Generate the input embeddings S intrain = { z | f e ( x ) , x \u2208 D intrain } and S bgtrain = { z | f e ( x ) , x \u2208 D bgtrain } . 3: Fit a Gaussian distribution N ( \u00b5 z , \u03a3 z ) using S intrain , and a background Gaussian N ( \u00b5 z 0 , \u03a3 z 0 ) using S bgtrain . 4: Similarly, generate output embeddings E intrain = { w | f d ( y ) , y \u2208 D intrain } , and E bgtrain = { w | f d (\u02c6 y ) , \u02c6 y \u2208 D bgtrain } . 5: Fit a Gaussian distribution N ( \u00b5 w , \u03a3 w ) using E intrain and a background Gaussian N ( \u00b5 w \u03b4 , \u03a3 w \u03b4 ) using E bgtrain . 1: Input: In-domain test set D intest = { ( x , \u02c6 y ) } . OOD test set D oodtest = { ( x , \u02c6 y ) } , where \u02c6 y = M ( x ) . 2: Generate input embeddings S intest = { z | f e ( x ) , x \u2208 D intest } and S oodtest = { z | f e ( x ) , x \u2208 D oodtest } . 3: Compute input OOD score RMD input ( z ) for z \u2208 S intest and S oodtest , respectively. Compute AUROC based on the input OOD scores. 4: Similarly, generate output embeddings E intest = { w | f d (\u02c6 y ) , \u02c6 y \u2208 D intest } and E oodtest = { w |\n\nA.5 T HE CONNECTION BETWEEN RMD AND B INARY CLASSIFIER\n\n RMD is a generative model based approach which assumes the distributions of the two classes are Gaussian, while the binary classi\ufb01er is a discriminative model which learns the decision boundary between two classes. Though they have different settings, under certain condition, the Gaussian generative model can be reduced to a binary classi\ufb01er. To see the connection, let us assume the label y = 0 if the sample is from in-domain, and y = 1 if the sample is from the general domain. Let us also assume the two classes have balanced sample size without loss of generality p ( y = 1) = p ( y = 0) . Since the log-probability of log p ( y = 1 | z ) can be rewritten using the Bayes rule log p ( y = 1 | z ) = log p ( z | y = 1) + log p ( y = 1) \u2212 log p ( z ) , the logit (log odds) can be written as, logit = log When \u03a3 = \u03a3 y =1 = \u03a3 y =0 , the equation can be further simpli\ufb01ed as Therefore, when assuming the covariance matrices are identical for the two Gaussian distributions, the Gaussian generative model can be reduced to a binary classi\ufb01cation model. However, our RMD does not assume the same covariance matrix in both distributions. We estimate the covariance matrix individually for each class. So our RMD is different from binary classi\ufb01er, and it has higher model capacity than the binary classi\ufb01er. 17\n\nA.6 C ORRELATION BETWEEN DIFFERENT SCORES AND THE QUALITY METRICS\n\n 18\n\nA.7 S ELECTIVE GENERATION AND OUTPUT QUALITY PREDICTION\n\n 19 20 21\n\nA.8 I NVESTIGATION OF THE N - GRAM OVERLAP BETWEEN LAW DATASET AND IN - DOMAIN DATASETS\n\n\n\nA.9 V ISUALIZATION OF OOD SCORE ON SHIFTED DATASET\n\n We explore how individual parts of an input text contribute to the OOD score, which can help us visualize which parts of the text are OOD. We de\ufb01ne the OOD score of each sentence in the text using a leave-one-out strategy: For any given sentence, we compute the OOD score of the article with and without that sentence in it. The negative of the change in the OOD score after removing the sentence denotes the OOD score of that sentence. Intuitively, if removing the sentence decreases the overall OOD score, that sentence is assigned a positive OOD score and vice-versa. Figure A.5 illustrates an example where an article contains noise in the form of tweets with emojis, and the OOD scoring mechanism described above assigns positive OOD scores to those tweets and negative scores to the main text. . 22", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes a lightweight and effective out-of-distribution (OOD) detection method for conditional language models (CLMs) such as those used in abstractive summarization and translation.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors show that model perplexity is not well-suited for OOD detection and instead propose using the input and output embeddings of the CLM itself to compute OOD scores, without requiring a separate detection model.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The OOD score is based on the Mahalanobis distance (MD) to Gaussians fit on the training embeddings, and an extension called Relative Mahalanobis Distance (RMD) which incorporates a background Gaussian.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "A binary classifier using the embeddings as features is also evaluated.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments demonstrate the proposed OOD scores significantly outperform perplexity and other baselines for detecting OOD examples.", "topic_class_1": "Summary of experiments", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Furthermore, the authors show that while perplexity is a reasonable quality predictor for in-distribution examples, combining it with the proposed OOD scores enables much better selective generation when the input distribution is shifted, abstaining on low-quality outputs while generating the rest.", "topic_class_1": "Summary of experiments", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed OOD scores are simple, lightweight and do not require training a separate detection model, making them very practical.", "topic_class_1": "Strength in method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Extensive experiments on both summarization and translation tasks demonstrate the effectiveness of the proposed approach, significantly outperforming baselines.", "topic_class_1": "Strength in experiments", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Analysis showing perplexity has reduced correlation with output quality on OOD data is insightful and highlights the need for better selective generation under distributional shift.", "topic_class_1": "Strength in experiments", "topic_class_2": "Strength / Analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Combining perplexity with OOD scores to enable selective generation on shifted data is a key contribution with high practical value for safer deployment of language models.", "topic_class_1": "Strength in experiments", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The Gaussian fitting and Mahalanobis distance computations could become expensive for very high-dimensional embeddings.", "topic_class_1": "Concerns of the method", "topic_class_2": "Weakness / Method", "reliability": "No", "explanation": NaN, "error_type": "Contradiction"}, {"segment_text": "Computational cost and scalability could be discussed.", "topic_class_1": "Suggestion in analysis", "topic_class_2": "Weakness / Method", "reliability": "No", "explanation": NaN, "error_type": "Out-of-scope"}, {"segment_text": "Analysis of selective generation focuses on abstention rate, but it would be interesting to see the coverage vs quality trade-off, i.e.", "topic_class_1": "Suggestion in analysis", "topic_class_2": "Weakness / Analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "what % of examples can be generated while maintaining a specified quality bar.", "topic_class_1": "Suggestion in analysis", "topic_class_2": "Weakness / Analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments cover a good range of domains but are limited to summarization and translation.", "topic_class_1": "Concerns of experiments", "topic_class_2": NaN, "reliability": "No", "explanation": NaN, "error_type": "Out-of-scope"}, {"segment_text": "Generalization to other language tasks could be discussed.", "topic_class_1": "Suggestion in experiments", "topic_class_2": "Weakness / Experiments", "reliability": "No", "explanation": NaN, "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is clearly written and easy to follow.", "topic_class_1": "Opinion in clarity", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology is explained in detail, including algorithms for fitting the Gaussians and computing OOD scores.", "topic_class_1": "Opinion in clarity", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Assumptions made are stated upfront.", "topic_class_1": "Opinion in clarity", "topic_class_2": "Strength / Paper Writing", "reliability": "No", "explanation": "This paper does not explicitly state the assumptions", "error_type": "Unstated statement"}, {"segment_text": "The experimental setup is comprehensive, covering multiple datasets of varying shift from the training data.", "topic_class_1": "Opinion in quality", "topic_class_2": "Strength / Experiments", "reliability": "No", "explanation": "Contradict to line 23. It is not comprehensive in terms of task types", "error_type": "Contradiction"}, {"segment_text": "Results are extensive and include both automatic and human evaluation.", "topic_class_1": "Opinion in quality", "topic_class_2": "Strength / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Key strengths and weaknesses of the method are analyzed.", "topic_class_1": "Opinion in quality", "topic_class_2": "Strength / Analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach of using the CLM's own embeddings for OOD detection without a separate model is quite novel, as is the extension of selective prediction to language generation under distribution shift.", "topic_class_1": "Opinion in novelty", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors plan to release code which would aid reproducibility.", "topic_class_1": "Opinion in reproducibility", "topic_class_2": "Strength / Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Hyperparameters are provided to reproduce the binary classifiers.", "topic_class_1": "Opinion in reproducibility", "topic_class_2": "Strength / Reproducibility", "reliability": "No", "explanation": "No hyperparameters for the binary classifiers", "error_type": NaN}, {"segment_text": "Details on hardware requirements and runtimes could further improve reproducibility.", "topic_class_1": "Opinion in reproducibility", "topic_class_2": "Strength / Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes valuable contributions to OOD detection and selective generation for conditional language models under distributional shift.", "topic_class_1": "Opinion in topic", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed OOD scores are highly effective while being simple and lightweight.", "topic_class_1": "Opinion in method", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": "Contradiction"}, {"segment_text": "Experiments are extensive and results are promising.", "topic_class_1": "Opinion in experiments", "topic_class_2": "Summary of Strength on Experiments", "reliability": "No", "explanation": "somehow self-contradictory because the experiments is challenged in line 22", "error_type": "Contradiction"}, {"segment_text": "The key weakness is the lack of discussion on computational efficiency.", "topic_class_1": "Opinion in weakness", "topic_class_2": "Summary of Weakness on Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "But overall, this is a strong paper that would be of interest to the NLP community and spur further work in this important direction.", "topic_class_1": "Opinion in topic", "topic_class_2": "Summary of recommendation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents a method for out-of-distribution (OOD) detection and selective generation for conditional language models (CLMs), focusing on improving the safety and reliability of CLMs in practical applications like abstractive summarization and translation.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "By identifying and abstaining from low-quality, OOD examples, the method enables CLMs to produce higher-quality outputs.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach uses embeddings from CLMs to accurately detect OOD examples and combines perplexity scores with a newly introduced OOD score for selective generation.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "No", "explanation": "Duplication of segment 2. ", "error_type": "Duplication"}, {"segment_text": "This combination allows for the adjustment of output quality based on the distributional shift of the input,  demonstrating significant improvement over baseline methods in both summarization and translation tasks.", "topic_class_1": "Summary of experiments", "topic_class_2": "Summary", "reliability": "No", "explanation": "Duplication of segment . 3", "error_type": "Duplication"}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The methodology introduces a practical and effective solution for OOD detection in CLMs without requiring a separate detection model.", "topic_class_1": "Strength on method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper provides a comprehensive evaluation framework that includes human quality ratings, addressing a critical need in the community.", "topic_class_1": "Strength on experiments", "topic_class_2": "Strength / Method", "reliability": "No", "explanation": "This is unclear. The proposed OOD detection is an automatic evaluation method. Human quality rating is involved in quality analysis in selective generation tasks. ", "error_type": "Vague Critique"}, {"segment_text": "The combined use of perplexity and the novel OOD score for selective generation is innovative and demonstrates a significant improvement over existing methods.", "topic_class_1": "Strength on experiments", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The approach's dependency on specific embeddings and the potential for its performance to vary across different models or tasks is not extensively discussed.", "topic_class_1": "Concerns on experiments", "topic_class_2": "Weakness / Experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "While the method shows improvements, the real-world applicability and scalability of the proposed solution could be further explored, particularly in more diverse or adversarial settings.", "topic_class_1": "Concerns on experiments", "topic_class_2": "Weakness / Experiments", "reliability": "No", "explanation": "Duplication of Segment 15.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelity, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written, presenting a clear and logical progression of ideas.", "topic_class_1": "Opinion on clarity", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, leveraging both theoretical insights and empirical evaluations.", "topic_class_1": "Opinion on method", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty of the approach lies in its effective combination of perplexity scores and OOD detection for improving CLM outputs.", "topic_class_1": "Opinion on novelty", "topic_class_2": "Strength / Method", "reliability": "No", "explanation": "The contribution first lies in that introducing an OOD detection framework based on input and output embeddings. ", "error_type": "Misunderstanding"}, {"segment_text": "The provided details and evaluation framework contribute to the paper's reproducibility, although additional information on the implementation and potential limitations would further enhance it.", "topic_class_1": "Opinion on reproducibility", "topic_class_2": "Strength / Reprocibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces an innovative method for enhancing the safety and reliability of conditional language models through OOD detection and selective generation.", "topic_class_1": "Summary of method", "topic_class_2": "Summary of Strength on Method ", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It addresses a critical gap in the field, providing both theoretical contributions and practical improvements.", "topic_class_1": "Summary of method", "topic_class_2": "Summary of Strength on Method ", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite some areas for further exploration, the paper's strengths in novelty, methodological clarity, and potential impact make a compelling case for its acceptance.", "topic_class_1": "Opinion on quality", "topic_class_2": "Summary of Recommendation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": "Section title", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper addresses the issue of out-of-distribution (OOD) detection and selective generation for conditional language models (CLMs).", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "CLMs, trained to predict the next token in a sequence, can suffer significant performance degradation and produce low-quality outputs when faced with OOD inputs.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper proposes lightweight and accurate OOD detection scores based on the model's input and output embeddings.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It further demonstrates how these scores, combined with perplexity, can be used for selective generation, enabling safer deployment of CLMs by abstaining from generating low-quality outputs on OOD examples.", "topic_class_1": "Summary of method", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors evaluate their methods on abstractive summarization and translation tasks, showing significant improvements over baseline approaches.", "topic_class_1": "Summary of experiments", "topic_class_2": "Summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: The proposed OOD detection scores based on input and output embeddings are novel and effective for CLMs.", "topic_class_1": "Strength on method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Lightweight and Accurate: The methods require minimal additional computation and achieve high accuracy in OOD detection.", "topic_class_1": "Strength on method", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Selective Generation: Combining OOD scores with perplexity enables selective generation, allowing models to abstain from generating low-quality outputs on OOD examples.", "topic_class_1": "Strength on experiments", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Evaluation Framework: The paper proposes a comprehensive evaluation framework for OOD detection and selective generation in CLMs, including human evaluation for summarization quality.", "topic_class_1": "Strength on experiments", "topic_class_2": "Strength / Method", "reliability": "No", "explanation": "The proposed selective generation in CLMs is not an evaluation framework. The work combines OOD score and perplexity to improve selective generation. Both automatic evaluation (BLEURT) and human evaluation are adopted to evaluate the translation and summarization quality respectively.", "error_type": "Misunderstanding"}, {"segment_text": "Weaknesses:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited Task Modalities: While the methods are potentially applicable to other sequence-to-sequence tasks, the experiments are limited to summarization and translation. Model Architectures: The analysis focuses on encoder-decoder Transformers, and it is unclear how well the methods generalize to decoder-only architectures used by some large language models.", "topic_class_1": "Concerns on method", "topic_class_2": "Weakness / Experiments Weakness / Method", "reliability": "No", "explanation": "Out-of-scope", "error_type": "Out-of-scope"}, {"segment_text": "Clarity: Some aspects of the methodology, particularly the feature-based and deep ULF methods, could benefit from further elaboration and details.", "topic_class_1": "Concerns on writing", "topic_class_2": "Weakness / Paper Writing", "reliability": "No", "explanation": "It is unclear what feature-based and deep ULF methods mean. The review seems to be generated from LLMs.", "error_type": "Unstated statement"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is generally well-written and easy to follow.", "topic_class_1": "Opinion on clarity", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, with thorough experiments and analysis.", "topic_class_1": "Opinion on quality", "topic_class_2": "Strength / Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed OOD detection scores and selective generation framework are novel and significant contributions.", "topic_class_1": "Opinion on novelty", "topic_class_2": "Strength / Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors mention that code will be released, which will enhance reproducibility.", "topic_class_1": "Opinion on reproducibility", "topic_class_2": "Strength / Reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": "Section title", "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents novel and effective methods for OOD detection and selective generation in CLMs.", "topic_class_1": "Summary of topic", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed OOD scores are lightweight, accurate, and enable safer deployment of CLMs by selectively generating high-quality outputs.", "topic_class_1": "Summary of method", "topic_class_2": "Summary of Strength on Method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is well-conducted and the paper is well-written.", "topic_class_1": "Summary of clarity", "topic_class_2": "Summary of Paper Writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "I recommend accepting this paper for publication.", "topic_class_1": "Summary of paper", "topic_class_2": "Summary of Recommendation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}}
{"decision": "rejected", "title": "Interactively Generating Explanations for Transformer Language Models", "body_text": " I NTERACTIVELY\n\nG ENERATING E XPLANATIONS FOR\n\n T RANSFORMER L ANGUAGE M ODELS Anonymous authors Paper under double-blind review A\n\nBSTRACT\n\n Transformer language models are state-of-the-art in a multitude of NLP tasks. Despite these successes, their opaqueness remains problematic. Recent methods aiming to provide interpretability and explainability to black-box models primar- ily focus on post-hoc explanations of (sometimes spurious) input-output correla- tions. Instead, we emphasize using prototype networks directly incorporated into the model architecture and hence explain the reasoning process behind the net- work\u2019s decisions. Moreover, while our architecture performs on par with several language models, it enables one to learn from user interactions. This not only offers a better understanding of language models but uses human capabilities to incorporate knowledge outside of the rigid range of purely data-driven approaches.\n\n1 I NTRODUCTION\n\n Transformer language models (LMs) are ubiquitous in NLP today but also notoriously opaque. Therefore, it is not surprising that a growing body of work aims to interpret them: Recent evalu- ations of approaches of saliency methods (Ding & Koehn, 2021) and instance attribution methods Pezeshkpour et al. (2021) find that, while intriguing, for the same outputs, different methods assign importance to different inputs. Furthermore, they are usually employed post-hoc, thus possibly en- couraging reporting bias (Gordon & Van Durme, 2013). The black-box character of LMs becomes especially problematic, as the data to train them might be unfiltered and contain (human) bias. As a result, ethical concerns about these models arise, which can have a substantial negative impact on society as they get increasingly integrated into our lives (Bender et al., 2021). Here, we focus on generating case-based reasoning explanations during the inference process, di- rectly outputting the LM\u2019s predictions. In doing so, we avoid the problems mentioned above of post- hoc explanations and help to reduce the issue of (human) bias. To increase the interpretability of the model, we enhance the transformer architecture with a prototype layer and propose Prototypical- Transformer Explanation (Proto-Trex) Networks. Proto-Trex networks provide an explanation as a prototypical example for a specific model prediction, which is similar to (training-)samples with the corresponding label. Our experimental results demonstrate that Proto-Trex networks perform on par with non- interpretable baselines, e.g. , BERT (Devlin et al., 2019) and GPT (Radford et al., 2019). In terms of explanations, we show promising results with learned prototypes providing helpful explanations for the user to understand better the LMs decision-making, which, in turn, increases trust and reliabil- ity. To further enhance the prototypical network with human supervision, we propose an interactive learning setting (iProto-Trex) that allows users of any knowledge to give feedback and improve the model (Ribeiro et al., 2016), moving beyond a purely data-driven approach. To summarize, our contributions are as follows: We (i) introduce prototype networks for transformer LMs that generate explanations and (ii) show that they are on par with non-interpretable baselines on classification tasks on different architectures. Furthermore, to improve prototype networks, we (iii) provide a novel interactive prototype learning setup accounting for user feedback certainty. We proceed as follows. We start by briefly reviewing related work of interpretability in NLP. Then we introduce Proto-Trex networks, including our novel interactive learning setup combining ex- planatory interactive learning with prototype networks. Before concluding, we discuss faithful ex- planations and touch upon the results of our experimental evaluation. 1 Staff available to assist but limited to the knowledge and understanding of the individual. Staff available to assist, but limited to the knowledge and understanding of the individual. Proto-Trex They literally treat you like you are bothering them. No customer service skills. iProto-Trex They offer a bad service.\n\n2 T OWARDS THE E XPLAINABILITY OF T RANSFORMER L ANGUAGE M ODELS\n\n To open the black-box of transformers, we use i/Proto-Trex networks built upon post-hoc interpre- tation methods, case-based reasoning approaches, and explanatory interactive learning. Post-hoc interpretability. Various (post-hoc) interpretability methods focus on different parts of the transformer architecture. Atanasova et al. (2020) and Belinkov & Glass (2019) provide overviews of this fast developing field. Generally, there are methods that analyze word representations (Voita et al., 2019a), the attention distribution throughout the model (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019), and the (attention and classification) model heads (Voita et al., 2019b; Geva et al., 2021). Other approaches such as Geva et al. (2020) focus on the feed-forward layers. Gradient- based approaches, such as Sundararajan et al. (2017) and Smilkov et al. (2017) can generally be used to trace gradients, while influence functions (Koh & Liang, 2017) trace model parameter changes throughout a LM (Han et al., 2020). While backtracking all the model weights might be possible, such explanations can only tell us which part of the input they are looking at (Chen et al., 2019) and humans are ill-equipped to interpret them (Stammer et al., 2021). So instead, we aim for more intuitive and sparse explanations: well-descriptive but short sequences as prototypes. These are not only able to reveal the parts of the input it is looking at but also show prototypical cases similar to those parts. Case-based Reasoning in Deep Neural Networks. Our work relates most closely to previous work demonstrating the benefits of prototype networks in Computer Vision (Li et al., 2018; Chen et al., 2019) as well as for sequential data by combining them with RNNs (Hase et al., 2019; Ming et al., 2019). More precisely, in contrast to post-hoc interpretation methods, prototype networks employ explanations in the, e.g. , classification process, i.e. , classify a sample by comparing several parts \u2013which could be words of a sentence\u2013 to (learned) prototypical parts from other samples for a given class. If things look similar, they are classified similarly. Humans behave very similarly, which is called case-based reasoning (Clifton & Frohnsdorff, 2001). The prototypical parts provided by the network help the user to understand the classification. Tab. 1 illustrates the benefit of prototypical explanations. One can observe that the post-hoc explanation can leave the user clueless, while the prototypical explanations can help better understand the network\u2019s decision. Accordingly, proto- types are an additional method in the interpretability toolbox, extending current post-hoc methods. As described, the reasoning process of prototypical networks is qualitatively similar to that of hu- mans (Clifton & Frohnsdorff, 2001; Chen et al., 2019). In previous approaches, however, the training of such networks was entirely data-driven. Therefore, like Chen et al. (2019), in our work, we do not focus on quantifying unit interpretability of prototypical networks, but extend learning the reasoning process of our network by Explanatory Interactive Learning (Schramowski et al., 2020). Explanatory Interactive Learning (XIL) incorporates explanatory supervision in the learning pro- cess by involving the human user. More precisely, human users can ask the model to explain a pre- diction. They can then respond by correcting the model if necessary, providing a slightly improved \u2013but not necessarily optimal\u2013 feedback on the explanations (Stammer et al., 2021). Interactive learn- ing and, in particular, explanatory interactive learning showed not only to increase the explanation quality of deep black-box models but also the trust of the user (Ross et al., 2017; Selvaraju et al., 2 2019; Teso & Kersting, 2019; Schramowski et al., 2020). In contrast to previous (post-hoc) XIL methods, we avoid tracing gradients. That makes our approach particularly efficient, as we only change prototypes ( cf. Fig 4).\n\n3 P ROTO -T REX : P ROTOTYPE L EARNING FOR T RANSFORMER LM S\n\n The prototype network (Proto-Trex) architecture builds upon large-scale transformer LMs, summa- rized in Fig. 2. In the following, we describe the general idea of prototypical networks and our transformer-based prototype architecture and its modules in more detail\n\n. 3.1 P ROTO -T REX N ETWORKS\n\n General Idea. Proto-Trex is a prototype network, where prototypes are representative observations in the training set. The classification is based on the neighboring prototypes, i.e. an input query is classified as positive when the network thinks it looks similar to a certain (near) positive proto- type within the training set. In more detail, the reasoning and classification processes use the same module, i.e. , they use the similarity between query and prototypes. As both reasoning and classifi- cation use the same module (in parallel), we consequently have an interpretable model. In particular, (interpretable) prototypical explanations help verify Right for the Right Reasons (Ross et al., 2017). Architecture. Specifically, the model, in our case the pre-trained transformer f , creates a context embedding e from the input sequence x . This embedding contains useful features for the predic- tion and is then passed on to the prototype layer. This layer computes the similarity between the embedding e and each of the trainable prototypes p j from the set of prototypes P . The prototypes are learned during the training process and represent prototypical (sub-)sequences in the training data. We use transformer models both on the sentence-level (a) and the word-level (b), resulting in a single input representation for the sentence-level transformer and l for word-level. To compare the word-level embeddings e i with the prototypes, we have to patch them ( Z = { patch ( e i ) } ) into subsequences according to the desired prototype length k . The resulting similarity values are passed on to the final linear layer g with weight matrix w g and no bias, mapping from m (the number of prototypes) to the number of classes. Finally, the output values are passed on to a softmax producing class probabilities and predictions t .\n\n3.2 P ROTO -T REX L OSS\n\n Optimization of prototype networks aims at maximizing both performance and interpretability. To this end, our Proto-Trex loss L combines performance and interpretability with prior knowledge of the prototype network\u2019s structure. Let us illustrate this for the word-level case (the sentence-level case results from e = z ): L := min P , w g 1 3 where \u03bb i weights the influence of the different terms and n is the number of training examples. The first term is the cross-entropy (CE) loss optimizing the predictive power of the network. The second term (Clst) clusters the prototypes w.r.t. the training examples of the same class, maximizing simi- larity to them (we rewrite maximization as minimization terms), and the separation loss minimizes the similarity to other-class instances: Together, Clst and Sep push each prototype to focus more on training examples from the same class and less on training examples from other classes. Both are motivated by ProtoPNet (Chen et al., 2019). To get prototypes that are distributed well in the embedding space, we introduce two additional losses, a distribution loss (Li et al., 2018), assuring that a prototype is nearby each training example, and a diversity loss (Ming et al., 2019): In contrast to the other terms, the diversity loss does not compute similarities between embeddings and prototypes but between prototypes themselves. It is another way of distributing prototypes in the embedding space as it maximizes the distance between prototypes, preventing them from staying at the same \u2013not necessarily optimal\u2013 location. This is especially helpful in the case of multiple prototypes for a single class, encouraging them to represent different facets of the class. If they are otherwise too similar, no information is gained and resulting in redundant prototypes \u2013 together with the class-specific loss encouraged by the cluster loss (Clst) and the separation loss (Sep), this helps to compute prototypes that focus solely on their class. Otherwise, we can get ambiguous prototypes leading to negative reasoning. Also, we clamp the weights of the classification layer with min( w g , 0) to avoid negative reasoning (Chen et al., 2019). Finally, the last term of the Proto- Trex loss (Eq. 1) is an L1-regularization term of the last layer ( g ), which prevents the network from overfitting or relying too much on a single prototype.\n\n3.3 S IMILARITY C OMPUTATION\n\n Computing similarities is an essential aspect of Proto-Trex. For prototypes to represent certain aspects or features of the input distribution in the embedding space, we compute the similarity sim ( e , p ) between an embedded training example and a prototype. A distance minimization can replace each similarity maximization max sim ( e , p ) = min dist ( e , p ) . We here follow common practice and explore both the L2-norm or the cosine similarity: where the index j denotes a specific prototype. For each training example, there is an embedding e which is compared to all m prototypes p , i.e. we get m similarity values for each embedding. The L2-norm assumes a Gaussian prior, which can be a wrong assumption, which is why we also investigate cosine similarity. While L2-norm computes the distance between two vectors, cosine similarity measures the angle between them. Both, but especially cosine similarity, are natural choices for NLP tasks (Manning et al., 2008). However, cosine similarity is more robust here than L2-norm as the distance\u2019s magnitude between vectors has no influence due to normalization.\n\n3.4 S ELECTION FOR W ORD -L EVEL P ROTOTYPES\n\n Since learning prototypes for LMs pre-trained on word-level representations is more involved than for the sentence-level, let us focus on them here; the sentence-level case naturally follows from the discussion. Word-level prototypes are generally sensible as explanations should consist of sparse sequences, at best focusing only on subsequences of the input sentence. As sequences can also be ambiguous and contain little information, we combine different word embeddings ( patch () ) and enforce the prototype to be similar to the relevant subsequences, containing the most information. But how do we select the most informative words? 4 A naive approach would be to simply compute all possible patches (word combinations) of the input sequence and compare them to the prototypes to find the best patch for classification \u2013 the most important subsequence should then be similar to a certain prototype. Unfortunately, for long input sequences with length l it becomes hard to compute all possible word patches of length k , in this case |Z| = (cid:0) lk (cid:1) . To solve this problem without losing too many valuable patches, the following two approaches are sensible ideas. (a) Sliding windows naturally reduce the number of patches. A sliding window is a convolution that selects a certain part of the input according to the window size and then sliding to the next part. The main disadvantage of sliding windows is the relatively rigid structure of a window. This is especially problematic in the context of NLP tasks that often have long-range dependencies. We introduce dilation to loosen this. Dilation facilitates looking at direct word neighborhoods but also at more distant ones to capture long-range dependencies. This \u201cconvolutional\u201d approach is illus- trated in Tab. 3(a). We note that applied to the contextualized word embeddings of transformer LMs, the convolutional approach without dilation should contain not only local information but also some global information. Unfortunately, this information is stored in the embeddings and cannot be visualized easily. As an alternative, the (b) Self-Attention approach adds a self-attention layer after the transformer f and before the prototype layer p to filter out irrelevant words (with low attention-scores), cf. Fig. 3(b). The purpose of this self-attention layer differs from the one in the transformer LM itself. The here used attention mechanism selects the most important words, and the embedding representation re- mains untouched. Hence, the number of heads is chosen to be 1 . To still provide as much information and variety as possible, the number of selected words n w of the attention layer which are passed into the distance computation is a hyperparameter, and we chose it to be twice as much as the length of a single prototype however clamped by the threshold k lim : n w = min ( max ( k lim , k ) , 2 k ) . The thresh- old can be set w.r.t. computational efficiency. This means having a prototype of length k = 4 and k lim = 10 , the attention layer selects n w = 8 words yielding in this example |Z| = (cid:0) 84 (cid:1) = 70 patches for the distance computation.\n\n3.5 D ECODING VIA N EAREST N EIGHBOR P ROJECTION\n\n Proto-Trex networks encode prototypes in the embedding space. Consequently, they cannot simply be decoded from the transformer embedding space, as this space and textual data are categorical and not continuous. To overcome this, we assign, i.e. project, each prototype to its nearest neigh- bor in the training data (where the textual representation is available) in an intermediate training step. Thereby, we ensure that the prototype really represents what it actually looks like, which also increases the interpretability of the Proto-Trex network. The final training step (after the nearest neighbor projection) then fine-tunes the classification head to adapt it to the projected prototypes. Doing so has the advantage of being decoded precisely on the spot and providing more certainty for the explanatory power. The disadvantage, however, is that a prototype may be located sub-optimally, which could lead to performance losses. We faced this issue especially for word-level prototypes and found the projection ineligible for these highly contextualized word representations. Instead, for word-level, we only use the nearest neighbor for approximating the prototype to generate the explanation. 5\n\n4 I NTERACTIVE P ROTOTYPE L EARNING\n\n Recent case-based reasoning approaches in Computer Vision and NLP usually assume a large vol- ume of data for training (Li et al., 2018; Chen et al., 2019; Hase et al., 2019), with little or no user feedback during the model building process. However, user knowledge and interaction, in par- ticular via explanations, can be valuable already in the model building and understanding phase, significantly reducing the amount of data required, avoiding Clever-Hans moments early on, and increasing the explanation quality of the model and, in turn, the trust of the user, see e.g. Teso & Kersting (2019 Schramowski et al. ( . Interactive Proto-Trex. To address this, we propose an explanatory interactive learning approach for Proto-Trex networks, called iProto-Trex, as illustrated in Fig. 4. Specifically, carried out during the training progress by freezing and evaluating the current state or post-hoc training, the interaction takes the following form. At each step, the Proto-Trex networks provide prototypes as explana- tions of a classification. The user responds by correcting the learner, if necessary, in our case, the prototypes. For this, iProto-Trex provides a range of opportunities for interacting with prototypes. Specifically, iProto-Trex distinguishes between weak-knowledge and strong-knowledge interac- tions. In strong-knowledge interactions, users are certain about their feedback (which could require high-level expertise). In this case, iProto-Trex provides options to remove and add prototypes. Re- moving a prototype is also helpful if the network has learned redundant prototypical sequences or prototypes that cover all critical aspects of the task at hand. If users are content with all the present prototypical sequences, they can also add new prototypes ( Replacing a prototype is the same as removing plus adding a prototype). After these interactions, the subsequent classification layer is retrained to integrate and balance the new prototype setup in its decision-making. In weak-knowledge interactions, users only state content or discontent with a prototype based on their intuition. They do not need to know what a replacement should exactly look like, which is a good trade-off between user knowledge and loss optimization of the network. To this end, iProto-Trex uses re-initialization and fine-tuning ; instead of providing an explicit replacement, users freeze prototypes they like while relearning those they are dissatisfied with. The difference between these approaches is that users can express how well the current prototype represents a particular task aspect. Another form of weak-knowledge interaction, which iProto-Trex provides only for sentence- level prototypes, is pruning the sequence length of prototypical explanations, i.e. , compressing it to the essentials. In order to limit meaning changes of a sequence, a threshold is provided for the cosine similarity of the pruned and the original version. We initially set this threshold to 0 . 8 , but users can set the threshold as they like, resulting in a strong-knowledge interaction. Soft User Feedback. iProto-Trex\u2019s interaction methods, especially the strong-knowledge ones, require users to be quite confident about their feedback \u2013they have to be experts. To elevate this burden, we propose a soft feedback mechanism, using an interaction loss based on the similarity between prototypes as with c \u2208 [0 , 1] . Instead of directly replacing the selected prototype p old , this soft interaction loss pushes the prototypes to be similar to the desired prototype p new as suggested by a user. Moreover, users can control how strong iProto-Trex should incorporate their interventions by setting a certainty value c between 0 (low certainty) and 1 (high certainty). 6\n\n5 F AITHFULNESS OF P ROTOTYPICAL E XPLANATIONS\n\n A question that naturally arises when dealing with explainable AI is how to evaluate the quality of an explanation. One of the key metrics in NLP and Computer Vision is the faithfulness of explanations (Jacovi & Goldberg, 2021; DeYoung et al., 2020), i.e. whether a given explanation represents the true reasoning process of the black-box model. We apply and adjust this metric to text-based prototype networks. First, we evaluate the faithfulness of Proto-Trex as a whole (DeYoung et al., 2020) by showing that the class probabilities change significantly if we perturb the input. We follow their approach of computing comprehensiveness and sufficiency. To calculate comprehensiveness, we remove the rationale in each input sequence and evaluate the changes in the class probabilities for the prediction. Second, we perturb the prototype layer by removing the top explanation for a given test sample and compute the loss in accuracy. This helps to identify to what extent the decision was actually based on the explanation given. Since our prototypical explanations are already short sequences, removing the rationale in the explanation is similar to removing the entire prototype.\n\n6 E XPERIMENTAL E VALUATION\n\n Our intention here is to investigate how good prototypes help to understand transformer LMs. To this end, we evaluated i/Proto-Trex explanations on three benchmark datasets: MovieReview (Pang et al., 2002), Open Yelp 1 and Jigsaw Toxicity 2 . We compared five pre-trained LMs (GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), DistilBERT, (Sanh et al., 2020), SBERT (Reimers & Gurevych, 2019) and the text-encoder of CLIP (Radford et al., 2021)) to investigate three questions: (Q1) How much does adding a prototype layer affect the performance of (non-interpretable) LMs, i.e. , a classification head defined by two fully-connected non-linear layers? (Q2) How does the performance change after and during interaction with the model explanations? In particular, we investigated the different modules of Proto-Trex networks on sentence- and word-level and the interaction between users and the prototypical explanations. (Q3) How faithful are the given explanations, i.e. how well does the given explanation represent the true reasoning of the black-box model? We present qualitative as well as quantitative results and refer to the Appendix for additional details on the experiments and our implementation as well as additional qualitative results. If not stated otherwise, the Proto-Trex architecture includes sentence- and word-level embeddings, the convolu- tion module without dilation to select word-tokens, and cosine similarity to compute the similarity between prototypical explanations and input query. We optimized the prototype and classification module with the proposed loss (Eq. 1). In each experiment, the number of prototypes is m =10 , and the largest variant of the corresponding pre-trained LM is evaluated. (Q1,2) Trade off accuracy and interpretability. Tab. 1 summarizes the experimental result of Proto-Trex network based on different sentence- and word-level LMs. As one can see and expected from the literature, interpretability comes along with a trade-off in accuracy. The trade-off is gen- erally higher on sentence-level LMs, partially due to the nearest neighbor projection ( cf. Appendix for direct comparison). However, the difference between traditional LMs and Proto-Trex LMs is often marginal and task-dependent ( e.g. DistilBERT on Yelp and Movie). Surprisingly, in the case of BERT and GPT-2 (on Yelp and Toxicity), the Proto-Trex network is outperforming the baseline LMs. Most interestingly, one can see that the user may boost the performance of the corresponding Proto-Trex network interactively (iProto-Trex). Overall, i/Proto-Trex is competitive with state-of- the-art LMs while being much more transparent. (Q1) Ablation study. Next, we investigate different Proto-Trex module choices for performance, cf. Tab. 1b), and explanation generation impact. While input- and prototype similarity computation does not explicitly correlate with the prototypes learned \u2013yet we assumed it to be the better choice\u2013, the word-selection module also impacts the explanation outcome. Furthermore, cosine similarity is not only more accurate but also converges faster. In terms of accuracy, the convolutional word selection outperforms the attention module. More importantly, we also observe an advantage for the 1 https://www.yelp.com/dataset 2 https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge 7 generated explanations; namely, the attention module tends to select punctuation and stop words. According to Ethayarajh (2019), this is because punctuation and stop-words are among the most context-specific word representations\u2013they are not polysemous themselves but can have an infinite number of contexts. In contrast, the convolution module is easier to interpret as words are coherent. (Q1,2) Generated explanations. To analyze the generated explanations, we consider the SBERT based Proto-Trex networks, cf. Tab. 1, trained on the Open Yelp dataset. Tab. 2 shows how Proto- Trex provides users with generated explanations. These explanations are given as prototypical se- quences that correspond to a query. In addition, Proto-Trex provides corresponding importance scores, indicating the significance of an explanation for the classification. We show four sentence- level prototypes for the query that users can quickly extract the important aspects that help under- stand the classification. However, one can observe that the sentence-level explanations are some- times lacking sparsity and are difficult to interpret about the query, which also demonstrates the demand for (further) interaction ( cf. colored boxes with prototypical explanations P6 and P2 in Tab. 2). The pruned Proto-Trex provides less ambiguous and easier to interpret sequences. 8 (Q2) Interactive prototype learning. In the previous experiment, pruning has already shown the benefits of adapting explanations to users\u2019 preferences. In order to further evaluate our interactive learning setup, we incorporated certainty ( c ) and evaluated its effectiveness. Since sentence-level prototypes allow for better coherence, this is where we will focus the interactive learning. Tab. 3 shows an influential, i.e. high importance value, sentence-level explanation (yet no interaction) for negative restaurant reviews on the Open Yelp dataset. Assuming a user is dissatisfied with an expla- nation yet uncertain about what a good explanation would entail: Applying re-initialization (most uncertain interaction technique) confirms the user\u2019s intuition of a \u201cweak\u201d component, as the slight increase in accuracy indicates. More importantly, we can already observe that users can influence the network\u2019s decision process based on their preferences without performance loss. However, the revised explanation is still not sufficiently interpretable. Therefore, we considered incorporating explicit user feedback here. In this case, the phrase \u201cThey offer a bad service\u201d served as soft re- placement for the model\u2019s generated explanation, and the user applied it with different levels of certainty. First, a low certainty value ( c =0 . 5 ) results in the same explanation as before. This is be- cause the similarity of any two prototypes with clearly negative sentiment is higher than a certainty threshold of 0 . 5 . As the user gets more certain, he gradually increases the threshold. Finally ( c =1 ), the user simply replaced the prototype to obtain his desired solution without a trade-off in accu- racy. In summary, our results demonstrate that interactive learning is a solid method to counter the network\u2019s incapability in consistently providing interpretable prototypes along with high accuracy. (Q3) Faithful Prototypes. To analyze faithfulness, we first follow DeYoung et al. (2020) and use the movie review dataset as they provide human-annotated rationales for this dataset. We com- pare the classification probabilities of the samples (DeYoung et al., 2020) with the ones where the rationale is removed (comprehensiveness) and with the ones with only the rationale (sufficiency). Tab. 1(c) shows that i/Proto-Trex scores high for comprehensiveness, indicating that the network is focusing on the input rationale for the classification, while low sufficiency scores indicate that the surrounding context has little to even poor impact on the prediction. This shows that i/Proto-Trex are generally faithful classifiers, i.e. the human-annotated rationales agree with the internal rationales in the model. Furthermore, we remove the top (prototype) explanation for the classification and exam- ine the change in accuracy after the removal to show that it is the prototype that faithfully captures the rationale. Tab. 1(c) shows a drastic decrease in accuracy for both models after the explanation removal, confirming the faithful contribution of prototypical explanations to the classification.\n\n7 C ONCLUSION\n\n Large-scale transformer LMs, like other black-box models, lack interpretability. We presented meth- ods (prototype networks) to incorporate case-based reasoning to explain the LM\u2019s decisions. Despite the explanatory power of prototypical explanations, challenges regarding the quality of their inter- pretability still exist. Previous applications lack human supervision, although case-based reasoning is a human-inspired approach. Therefore, we propose an interactive prototype learning setup to over- come these challenges and improve the network\u2019s capabilities by incorporating human knowledge with the consideration of knowledge certainty. Future work covers improving methods for patching the input for word-level prototypes to enable projection for higher interpretability. 9\n\n8 E THICS AND R EPRODUCIBILITY S TATEMENT\n\n The article presented here approaches the explainability of transformer language models through case-based reasoning in the form of prototypical explanations. The interpretable prototype module that we learn is built on a pre-trained transformer language model. As these models are trained on data that is not publicly available, it remains an open question whether any kind of bias is inherent to the model as a whole (Bender et al., 2021). In addition, concerns about privacy violations and other potential misuse emerge as these models are trained with weak to no supervision The code to reproduce the results can be found in our publicly available repository. Furthermore, we provide the trained Proto-Trex models to generate explanations\n\nR EFERENCES\n\n 10 11\n\nA A PPENDIX A.1 D ATASETS\n\n The benchmark datasets mentioned above provide a fixed test set except the Yelp Open dataset. For the Yelp Open dataset we randomly select 200 000 training examples and split them into train ( 70% ), validation ( 15% ) and test ( 15% ) set. We split the Jigsaw Toxicity train set into train ( 20% ) and validation ( 80% ) set. Before the split, we filter out long sequences ( num tokens> 40 ) for each dataset. This is required because transformer-based language models can only handle sequences of 12 limited length, especially CLIP, and long sequences also cause the other sequences of a set to be padded to the same length, which, in turn, produces many padding tokens. For the tokenization we use the GPT2-tokenizer 3 . We apply grid search for hyper-parameters optimization. We report the cross-validated results. For the faithfulness computation we use the eraserbenchmark repository 4 and data 5 .\n\nA.2 T RAINING\n\n The Proto-Trex networks are trained with and evaluated on different datasets. We use PyTorch 6 for the implementation. We optimize our model with Adam optimizer with hyperparameters \u03b2 = (0 . 9 , 0 . 999) and \u03f5 = 10 \u2212 8 . We use a base learning rate of lr base = 0 . 001 and apply learning rate warm-up and scheduling that is here a linear decay. The learning rate is then given as where step i is the current step, e the total number of epochs and e wup the number of warm-up epochs. The warm-up takes place for e wup = min (cid:0) 10 , e 20 (cid:1) epochs. Warm-up reduces the dependency of early optimization steps that may cause difficulties in the longer run. Also, we weigh the cross-entropy loss with the class occurrences to re-balance the influence of an unbalanced dataset. The same is done in the accuracy computation where a balanced class accuracy is computed. For regularization, we apply an L1-regularization on the weights of the last linear layer. During the training process, a network may tend to overfit. To counteract this, we evaluate the model every 10 th epoch on the validation set and keep the model that yielded the best validation result. Furthermore, we set up a class mask to assign each class to the same number of prototypes. This is used and enforced by the class-specific losses (Clst and Sep). Changing the balance of the class assignment can be sensible to correct for imbalance in the dataset or focus on a specific class if its sentiment is more polysemous or generally more relevant.\n\nA.3 S ENTENCE - VS . W ORD - LEVEL L OSS\n\n For clarification, we present the loss terms for sentence-level i/Proto-Trex networks in more detail: L := min g (5) where 6 7 (8) and Divers ( p min (9) 3 https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer 4 https://github.com/jayded/eraserbenchmark 5 https://www.eraserbenchmark.com / 6 https://pytorch.org / 13\n\nA.4 P ROJECTION\n\n We show the impact of projecting the prototypes onto their nearest neighbor in Tab. 4. The pro- jection is evaluated for sentence-level Proto-Trex networks. One can see the trade-off between interpretability and accuracy introduced by projection.\n\nA.5 G ENERATED E XPLANATIONS\n\n Here we show the explanations that Proto-Trex generates. For each network, we use 10 prototypes, and for word-level networks, we use a prototype length of 4 tokens. First we extend the results of Tab. 2 for the word-level case, shown in Tab. 5. Then we show prototype lists for the models on different datasets. In Tab. 6 we show all prototypes for Proto-Trex based on the GPT-2 transformer and in Tab. 7 the prototypes based on the SBERT transformer. Both tables show the prototypes for the Yelp Open and MovieReview dataset. The results for the Jigsaw Toxicity dataset can be found in an external document in the codebase. (cid:34) CONTENT WARNING: the content in this document can be disturbing due to highly toxic texts! These results do not represent the authors\u2019 opinion and provide prototypical explanations generated solely by Proto-Trex. Tab. 8 shows the pruned prototypes of iProto-Trex from Tab. 7(a). Pruning cuts off the words at the end of a sequence that go beyond two sentences or 15 tokens in total. We showcase all experiments of the user interaction in Tab. 9 which is an extension of Tab. 6. The extension includes (1) retraining the classification layer for the same number of epochs to provide a fairer comparison with the interaction methods and to exclude changes in accuracy simply due to training for more epochs, (2) pruning the original, (3) removing the original, (4) adding a new proto- type without changing the original, (5) replacing the original with the user-chosen alternative \u201cThey offer a bad service.\u201d without using the interaction loss and (6) fine-tuning the original prototype. In Fig. 10 we shows the full explanations with importance scores for the exemplary query in the motivation ( cf. Fig. 1). It highlights the advantage of using prototype networks and the benefit of interactive learning. 14 This places not authentic and the pho is not that good. Very small uncomfortable dining area and the service was horrible. Great ambiance with a menu that is short and sweet . The food was delicious and I loved the ginger beer! Excellent service. Definitely recommend this place! Waited over hour for food. Only one person making sushi. Left without food, they still charged for the one beer we had. Bad server + no food = horrible exp. Food was great...service was excellent! Will be eating there regularly. Called it. This place didn\u2019t stand a chance with terrible management, gross food and slow service. Too bad for the workers. Excellent baby back ribs. Creamed corn was terrific too . Came in the afternoon and was easy to get in and have a relaxing meal. Excellent service too!! Don\u2019t waste your money at this restaurant. Took my daughter to celebrate her birthday. They didn\u2019t even sing Happy Birthday. The food was horrible. Great food, great service. Chicken tikka was awesome with fresh peppers. Iced tea had a surprising and refreshingly different flavor. Worst customer service . No one even said hello after going in 3 separate times. I am an avid spender and have spent lots of money there in the past. Will not shop there again! A great experience each time I come in . The employees are friendly and the food is awesome . There\u2019s too much falseness to the second half, and what began as an intriguing look at youth fizzles into a dull, ridiculous attempt at heart-tugging. Parts of the film feel a bit too much like an infomercial for ram dass\u2019s latest book aimed at the boomer demographic. But mostly it\u2019s a work that, with humor, warmth, and intelligence, captures a life interestingly lived . Godard\u2019s ode to tackling life\u2019s wonderment is a rambling and incoherent manifesto about the vagueness of topical excess... In praise of love remains a ponderous and pretentious endeavor that\u2019s unfocused and tediously exasperating. It\u2019s a lovely film with lovely performances by buy and accorsi. It throws quirky characters, odd situations, and off-kilter dialogue at us, all as if to say, \u201dLook at this! This is an interesting movie!\u201d but the film itself is ultimately quite unengaging . Like a Tarantino movie with heart, alias Betty is richly detailed, deftly executed and utterly absorb- ing. A bland, obnoxious 88-minute infomercial for universal studios and its ancillary products. The film fearlessly gets under the skin of the people involved... this makes it not only a detailed historical document, but an engaging and moving portrait of a subculture. A sad and rote exercise in milking a played-out idea \u2013 a straight guy has to dress up in drag \u2013 that shockingly manages to be even worse than its title would imply. an inventive, absorbing movie that\u2019s as hard to classify as it is hard to resist 15 16 17", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces Prototypical-Transformer Explanation (Proto-Trex) Networks, which enhance transformer language models with a prototype layer to generate case-based reasoning explanations for model predictions.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "An interactive learning setup (iProto-Trex) is also proposed to incorporate human feedback and improve the model's explanations.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments on text classification tasks demonstrate that Proto-Trex performs on par with non-interpretable baselines while providing prototypical explanations, and interactive learning further improves explanation quality.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper experiments", "reliability": "No", "explanation": "The statement should be - \"the paper evaluates these methods on explanability on sentiment classification tasks..\"", "error_type": "Inaccurate Summary"}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Proto-Trex provides interpretable explanations for transformer language models while maintaining competitive performance.", "topic_class_1": "Strengths", "topic_class_2": "Strength of novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The interactive learning setup allows incorporating human feedback to improve explanation quality.", "topic_class_1": "Strengths", "topic_class_2": "Strength of interactive setup", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Experiments are conducted on multiple datasets and architectures to validate the approach.", "topic_class_1": "Strengths", "topic_class_2": "Strength of evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The trade-off between interpretability and accuracy could be further analyzed and improved. - More details on the user study for interactive learning would strengthen the paper's claims. - Comparison to other interpretability methods for language models is lacking.", "topic_class_1": "Weakness", "topic_class_2": "Weakness of possibale trade-off Weakness of lack of details Weakness of lack of comparisons", "reliability": "No", "explanation": "Very poorly detailed limitations, needs more content to be actionable.", "error_type": "Vague Critique"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is generally well-written and easy to follow.", "topic_class_1": "Clarity", "topic_class_2": "Strength on writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed Proto-Trex architecture is novel in generating prototypical explanations for transformer language models.", "topic_class_1": "Quality", "topic_class_2": "Strength on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The interactive learning component is also an interesting extension.", "topic_class_1": "Quality", "topic_class_2": "Strength on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments seem comprehensive, covering different datasets, architectures, and ablations.", "topic_class_1": "Quality", "topic_class_2": "Strength on experiments", "reliability": "No", "explanation": "The reviewer used very vauge and unsure wording \"seem\", making this review not reliable", "error_type": "Inexpert Statement"}, {"segment_text": "However, some details on reproducibility, such as hyperparameters and model selection, are missing.", "topic_class_1": "Reproducibility", "topic_class_2": "Concerns over reproducibility", "reliability": "No", "explanation": "The hyperprameters are reported in detail", "error_type": "Neglect"}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes a solid contribution in enhancing transformer language models with prototype networks to generate interpretable explanations. The interactive learning extension is also valuable. While there is some room for improvement in the analysis and comparisons, overall this work seems significant and worthy of publication with minor revisions.", "topic_class_1": "Summary", "topic_class_2": "Summary of paper Comments on paper contribution Summary of overall assessment", "reliability": "No", "explanation": "These reviews are way too generic and lack depth/ reviewer style", "error_type": "Superficial Review"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces Prototypical-Transformer Explanation (Proto-Trex) Networks, a novel approach to enhancing the interpretability of transformer language models (LMs) through prototype networks incorporated directly into the model architecture.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Proto-Trex Networks provide case-based reasoning explanations by outputting prototypical examples for specific model predictions, aiming to increase the model's transparency and reduce bias.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also presents an interactive prototype learning setup, iProto-Trex, allowing user feedback to refine and improve the model's explanations and decisions.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experimental results show that Proto-Trex networks perform comparably to non-interpretable baselines while offering significant improvements in interpretability and user trust.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Introduces an innovative approach to interpretability in transformer LMs, enhancing transparency without sacrificing performance.", "topic_class_1": "Strengths", "topic_class_2": "Strength of novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The interactive learning setup (iProto-Trex) leverages user feedback to refine explanations and model accuracy, demonstrating a novel way to bridge human insights and AI.", "topic_class_1": "Strengths", "topic_class_2": "Strength of novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Provides comprehensive experimental evaluation, demonstrating the effectiveness of Proto-Trex networks across multiple datasets and against various baselines.", "topic_class_1": "Strengths", "topic_class_2": "Strength of evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The complexity of the Proto-Trex architecture might pose challenges for widespread adoption, especially in resource-constrained environments.", "topic_class_1": "Weakness", "topic_class_2": "Comments on method complexity", "reliability": "No", "explanation": "proto-types are obtained using SBERT model, which is a family of models. The resource constraints are no greater than the ones involved with any other Transformers.", "error_type": "Misunderstanding"}, {"segment_text": "While promising, the evaluation of user interactions and their impact on model performance and interpretability could benefit from further in-depth studies involving a broader range of participants.", "topic_class_1": "Weakness", "topic_class_2": "Comments on impact of different users", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper is well-written, presenting a clear and concise explanation of the Proto-Trex and iProto-Trex frameworks.", "topic_class_1": "Clarity", "topic_class_2": "Strength on writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The research is of high quality, contributing novel insights into interpretability in NLP.", "topic_class_1": "Quality", "topic_class_2": "Strength on quality", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed methods are novel, offering a practical approach to improving the transparency of transformer LMs.", "topic_class_1": "Novelty", "topic_class_2": "Strength on novelty", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The experiments are well-designed, and the paper provides sufficient detail for reproducibility, though the complexity of the models might pose some challenges.", "topic_class_1": "Reproducibility", "topic_class_2": "Strength on reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper presents an innovative approach to enhancing the interpretability of transformer language models through prototype networks and interactive learning.", "topic_class_1": "Summary", "topic_class_2": "Summary of paper", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It demonstrates the effectiveness of Proto-Trex networks in providing interpretable and accurate model predictions, supported by comprehensive experimental evaluations. While the complexity of the proposed methods might challenge their adoption, the paper significantly contributes to the field of NLP by offering a novel solution to the interpretability problem in deep learning models.", "topic_class_1": "Summary", "topic_class_2": "Summary of paper Summary of future work", "reliability": "No", "explanation": "These reviews are way too generic and lack depth/ reviewer style", "error_type": "Superficial Review"}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper introduces Proto-Trex and iProto-Trex, novel prototype-based approaches for generating explanations for Transformer language models (LMs).", "topic_class_1": "Summary", "topic_class_2": "Summay of paper topic", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Proto-Trex incorporates a prototype layer into the LM architecture, providing explanations in the form of prototypical examples similar to training samples with the corresponding label.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "iProto-Trex extends this by allowing users to interactively refine the prototypes, incorporating human knowledge and improving explanation quality.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper method", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper evaluates these methods on sentiment classification tasks, demonstrating competitive performance with non-interpretable baselines and highlighting the benefits of interactive learning for explanation faithfulness and user trust.", "topic_class_1": "Summary", "topic_class_2": "Summay of paper contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel approach: Proto-Trex offers a novel way to explain Transformer LMs by integrating prototype networks into the model architecture, moving beyond post-hoc explanations.", "topic_class_1": "Strengths", "topic_class_2": "Strength of novelty", "reliability": "No", "explanation": "Is novelty always a strength?", "error_type": "Misunderstanding"}, {"segment_text": "Interpretability: The use of prototypes provides human-understandable explanations, potentially increasing trust and reliability in LMs.", "topic_class_1": "Strengths", "topic_class_2": "Strength of interpretability", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Interactive learning: iProto-Trex allows users to refine prototypes, incorporating human knowledge and potentially improving model performance and explanation quality.", "topic_class_1": "Strengths", "topic_class_2": "Strength of user interaction", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Empirical evaluation: The paper provides a thorough evaluation on multiple datasets and compares Proto-Trex with various baseline models.", "topic_class_1": "Strengths", "topic_class_2": "Strength of evaluation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Trade-off with accuracy: While generally competitive, Proto-Trex sometimes shows a slight decrease in accuracy compared to non-interpretable baselines.", "topic_class_1": "Weakness", "topic_class_2": "Weakness of possibale trade-off", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited scope: The evaluation focuses on sentiment classification tasks.", "topic_class_1": "Weakness", "topic_class_2": "Weakness of limited task", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further research is needed to assess thegeneralizability of the approach to other NLP tasks.", "topic_class_1": "Weakness", "topic_class_2": "Weakness of lack of generalizability analysis", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Word-level explanations: While the paper explores word-level prototypes, the explanations generated seem less coherent and interpretable than sentence-level ones.", "topic_class_1": "Weakness", "topic_class_2": "Weakness of sentence-level results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Additional Comments:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "It would be interesting to see how Proto-Trex performs on other NLP tasks beyond sentiment classification.", "topic_class_1": "Weakness", "topic_class_2": "Comments on performance on other tasks", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Further research could explore alternative methods for selecting and representing word-level prototypes to improve their interpretability.", "topic_class_1": "Weakness", "topic_class_2": "Comments on future works", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Investigating the impact of different user types and expertise levels on iProto-Trex could be valuable.", "topic_class_1": "Weakness", "topic_class_2": "Comments on impact of different users", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Overall, this paper makes a valuable contribution to the field of interpretable NLP and offers a promising direction for future research.", "topic_class_1": "Summary", "topic_class_2": "Comments on paper contributions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is generally well-written and easy to follow, although some technical details might require further clarification. Quality: The research is well-conducted, with a solid methodology and comprehensive experiments. Novelty: The use of prototype networks for explaining Transformer LMs is novel and offers a promising direction for interpretable NLP.", "topic_class_1": "Clarity Quality Novelty", "topic_class_2": "Strength on writing Strength on quality Strength on novelty", "reliability": "Yes", "explanation": null, "error_type": NaN}, {"segment_text": "Reproducibility: The paper provides access to code and trained models, facilitating reproducibility and further research.", "topic_class_1": "Reproducibility", "topic_class_2": "Strength on reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": "Section Title", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper presents a novel and promising approach for generating explanations for Transformer LMs. Proto-Trex and iProto-Trex offer interpretable and potentially more trustworthy models, while remaining competitive with non-interpretable baselines. Further research is needed to explore thegeneralizability of the approach and improve word-level explanations.", "topic_class_1": "Summary", "topic_class_2": "Summary of paper Summary of future work", "reliability": "No", "explanation": "Repeat of all comments, once again, these reviews are way too generic and lack depth/ reviewer style", "error_type": "Superficial Review"}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
{"decision": "rejected", "title": "SaMoE: Parameter Efficient MoE Language Models via Self-Adaptive Expert Combination", "body_text": " S A M O E: P ARAMETER E FFICIENT M O E L ANGUAGE M ODELS VIA S ELF -\n\nA\n\n DAPTIVE E XPERT C OMBINATION Anonymous authors Paper under double-blind review\n\nA BSTRACT\n\n Recently, Mixture-of-Experts (MoE) has demonstrated success in scaling models to have large amounts of parameters without significant increases in computational cost. However, MoEs have been also reported to be parameter inefficient such that larger models do not always lead to better performance. In this work, we study how to build parameter-efficient MoE models. Our analysis identifies that MoE layers exhibit poor gradient flow as the number of experts increases, leading to insufficient training of experts. To overcome this issue, we propose a new MoE architecture design (SaMoE), which improves the parameter-efficiency of MoE models by learning a soft combination of a global set of expert layers for each MoE layer. Such a scheme enables substantial parameter savings on MoE while achieving comparable or better accuracy than the standard MoE training baseline. Extensive experiments on billion-scale GPT-3 style autoregressive MoE language models demonstrate that SaMoE significantly improves the parameter efficiency of MoE models by reducing up to 5.2 \u00d7 total parameters while obtaining superior pre-training and zero-shot generalization results as compared to baseline.\n\n1 I NTRODUCTION\n\n Over the past few years, there has been an explosion in research revolving around large language models, primarily motivated by the impressive performance of Transformer-based language mod- els (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019; Brown et al., 2020). One of the most impactful findings of this research is that the performance of these models continues to scale as the number of parameters increases (Kaplan et al., 2020; Clark et al., 2022). However, sustaining model parameters growth is getting more challenging due to the increasing compute requirements. As such, there has been substantial interest in exploring more efficient model designs and training methodologies. Among them, sparsely activated models, such as architectures based on Mixture-of- Experts (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021), have demonstrated promising results for training massive language models. MoE allows each input to interact with only a subset of the network parameters \u2013 chosen independently for each input. As such, the number of parameters is nearly disentangled from the computation cost of processing an input. Recently, several works explored whether MoE models can be comparatively accurate to dense mod- els but with much lower computational costs. As a result, they have successfully trained MoE-based language models and demonstrated that MoE models could perform on par with their dense equiva- lent counterparts but with up to 4-7 \u00d7 reduction in computation cost (Artetxe et al., 2021; Du et al., 2022; Rajbhandari et al., 2022). Despite promising results, MoE architecture appears to be parame- ter inefficient, considering the yielded model quality improvement vs. the involved parameters. For example, prior works report that to achieve the same quality as the dense model, the MoE model has roughly an order of magnitude more parameters than its corresponding dense model (Rajbhandari et al., 2022; Du et al., 2022; Artetxe et al., 2021). This parameter inefficiency adds a high cost of using additional memory and devices during model training and inference. Therefore, a natural question to ask is: \u201dAre all these expert parameters necessary to increase the model quality?\u201d or equivalently, \u201d Given a bound on the number of trainable parameters of a model, how can we arrive at an MoE model with higher quality? \u201d In this work, we investigate parameter-efficient architectures for MoE. In particular, our analysis shows that MoE models face challenges of poor gradient flow at MoE layers, leading to insuffi- 1 cient training of those layers compared to the dense layers. Based on this analysis, we conjecture that sharing parameters across experts would allow experts to receive more sufficient training and become useful. As such, we study several expert sharing strategies for MoE models. Our studies show that due to the smaller number of parameters, the performance of MoE models with aggres- sive tied-experts suffers when training on large-scale GPT pretraining datasets. On the other hand, relaxing the expert sharing constraints helps improve the model quality, but it requires manually designing the sharing strategy and the manually determined strategy may still achieve sub-optimal model quality. Our contributions in this work are: SaMoE . We improve the parameter efficiency of MoE models by developing a novel parameter- efficient MoE architecture, referred to as SaMoE. SaMoE learns an expert pool that consists of a global set of shared MoE layers and expresses each MoE layer as a soft combination of global MoE layers. Such a scheme decouples the number of experts from MoE model depth, drastically reducing MoE parameters while achieving better accuracy than baseline approaches (Section 4). Analysis . We identify poor gradient flow in MoE layers as the main cause of the poor parameter efficiency of MoE models. Our preliminary analysis shows that expert-sharing helps overcome the poor gradient flow issue and encourages MoE layers to learn more sufficiently (Section 3). Evaluation . We conduct experiments on billion-scale autoregressive MoE language models with open-sourced massive datasets and demonstrate that (i) SaMoE significantly improves the parame- ter efficiency of MoE models, reducing the model size by up to 5.2 \u00d7 while achieving superior model quality than prior works such as PR-MoE in zero-shot generalization accuracy (Section 5); (ii) Ab- lation study of the effectiveness of the proposed design elements in SaMoE (Section 5.4); (iii) A detailed evaluation of the scaling properties of SaMoE that reveals the strong scalability of SaMoE (Section 5.5); and (iv) Comparison results between SaMoE and alternative heuristic strategies (Sec- tion 5.6). We will also open-source the training and evaluation code at anonymous_link .\n\n2 R ELATED W ORK\n\n Mixture-of-Experts architecture converts multiple layers of a deep neural network to sparsely ac- tivated counterparts and jointly trained with the rest of the network (Jacobs et al., 1991; Shazeer et al., 2017). It falls into the paradigm of conditional computation (Bengio et al., 2013), which was proposed to activate only a small fraction of the model\u2019s parameters and computation on-demand on a per-example basis (Bengio et al., 2013; Davis & Arel, 2014; Cho & Bengio, 2014; Bengio et al., 2015). As such, they provide a promising path to build neural networks of much higher capacity without significantly increasing the computation required. Recent work has shown that MoE models can be extended with Transformer architecture for scaling language models(Lepikhin et al., 2020; Fedus et al., 2021). Despite the promising aspects of sparsely activating parameters, MoE models are difficult to train. In particular, prior works attribute the training difficulty of MoE to the unbal- anced load of experts and conjecture that encouraging or enforcing all experts to process balanced compute loads can help improve the learning of the gating function. For example, Lepikhin et al. (2020) and Fedus et al. (2021) propose to add load-balancing loss term into the training objective. Lewis et al. (2021) guarantee load balancing across experts by post-processing the routing output to re-assign expert selections to ensure that all experts are selected evenly. Roller et al. (2021) propose to use a fixed hash as the gating function, and Nie et al. (2021) propose to adaptively choose K in top-K selection during training. In addition, another challenge is that the gating function is highly non-differentiable. To address it, Clark et al. (2022) propose to use reinforcement learning for rout- ing, and Hazimeh et al. (2021) learns expert selection through a differentiable loss. Different from those works, our analysis shows that there is parameter redundancy in MoE layers, and we focus on developing parameter-efficient MoE architectures that reduce parameter redundancy. Parameter efficient architectures have always been an interesting question in machine learning com- munity (Mnih & Hinton, 2008; Mikolov et al., 2010; Press & Wolf, 2017; Inan et al., 2017; Savarese & Maire, 2019; Lan et al., 2020). Lan et al. (2020) discovered that sharing weights across layers improves the parameter efficiency of transformer models (Lan et al., 2020). However, ALBERT focus on encoder-based masked language model pre-training with dense Transformer blocks. More recently, Xue et al. (2022) proposed to share the weights of all MoE layers. However, we find that directly sharing all MoE layers leads to severe accuracy degradation for decoder-based MoE mod- els, especially on large-scale autoregressive GPT-3 style pretraining tasks (e.g., 10\u2013100 \u00d7 larger in 2 model and data size), which motivates the design of SaMoE. Please see our analysis in Section 3 and extensive evaluation in Section 5.\n\n3 A NALYSIS OF A UTOREGRESSIVE M O E L ANGUAGE M ODELS\n\n This section first introduces some preliminaries about MoE and notations needed to describe our approach and then analyzes the behavior of MoE models for language model pretraining. Preliminaries. A Transformer-based MoE model is built on top of a list of M MoE layers extended from a base dense model with N -layer ( M \u2264 N ). Each MoE layer contains a self-attention sub- layer and a sparsely activated feed-forward connection (FFC) sub-layer f ( m ) \u03b8 that consists E versions of FFCs weights in parallel, where \u03b8 ( m ) = { \u03b8 ( m ) 1 , \u03b8 ( m ) 2 , ..., \u03b8 ( m ) E } and \u03b8 ( m ) e (i.e., expert) represents the e -th version of the parameter ( f me . = f \u03b8 ( m ) e ) at the m -th MoE layer. The number of input tokens to an MoE layer is partitioned E -way across experts, and a gating function g ( \u00b7 ) : R H \u2192 [1, E] K associated with the layer (e.g., a small network) decides which expert(s) an input should be routed to. For example, given an input vector x in \u2208 R H , where H is the hidden dimension, the gate value of routing x in to experts is: p e ( x in ) = [ softmax ( g ( x in ))] e . Given the gate values { p e ( x in ) } Ee =1 , we select the top-K experts to form an activated set of experts E \u2032 \u2282 { 1 ...E } , where | E \u2032 | =K. The output x out of the MoE layer is given as the weighted sum of the outputs of each expert: x out = (cid:80) e \u2208 E \u2032 p e ( x in ) E e ( x in ) . The gate value p e ( x in ) permits differentiability to the gating function. In practice, we default K =1 because recent studies demonstrated that it leads to a simplified routing strategy while still preserving model quality and reducing communication overhead (Fedus et al., 2021; Clark et al., 2022). Since often E >> K (e.g., K=64), the number of FLOPS required for one input is significantly smaller than the number of parameters in MoE layers. We follow Fedus et al. (2021); Rajbhandari et al. (2022) by placing MoE layers at every other layer. Finally, existing studies identified that balancing workloads across experts is crucial, so we follow prior works to add a load balancing regularization term to make inputs to be more uniformly partitioned across experts (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021). In our experiment, we find that this strategy is sufficient to let each expert take about the same time to finish their assigned load. We note that other load balancing mechanisms could also be adopted (Roller et al., 2021; Lewis et al., 2021; Zuo et al., 2022), which we leave as future work. Analysis. We present several preliminary studies that reveal the challenges of MoE models, which motivate and guide the design in Section 4. We focus on the autoregressive decoder-based MoE: an architecture chosen due to its state-of-the-art performance (Brown et al., 2020; Du et al., 2022; Rajbhandari et al., 2022). Some of the largest models are also decoder-based, making their parameter efficiency important. More details about the experimental setups are presented in Section 5. Challenge I: MoEs require significantly more parameters than the base model being extended but exhibit diminishing returns in model quality as we increase the number of experts. We first investigate the scaling property of autoregressive MoE language models, referred to as AR-MoE, by varying the number of experts per MoE layer from 1 to 64. In all cases, the number of activated parameters per input barely increases due to the sparsity of MoE. Figure 1(a) shows that for training the same number of data samples, adding more experts generally leads to lower validation loss. Meanwhile, while scaling the number of experts brings diminishing returns, the total parameters increase almost exponentially. With E=64, the model size increases from 0.125B to 1.9B, a 15 \u00d7 increase. This observation is consistent with prior work (Fedus et al., 2021; Clark et al., 2022). Challenge II: Expert layers have poor gradient flow, receiving insufficient training and failing to contribute to the generalization. We also study MoE training using gradient flow, which is the first-order approximation of the decrease in the loss expected after a gradient step, to understand how a signal propagates within MoE models. Figure 1(b) shows that the gradients with respect to the weights (measured at 10k steps) are noticeably smaller for MoE layers (layers with even ids). In contrast, we do not observe large unbalanced gradients for non-MoE layers (odd layer ids). During MoE training, the number of input tokens to an MoE layer is partitioned across all experts, so the number of tokens per expert is reduced proportionally to the number of experts compared to the layers where no such partition is done. As such, each expert may receive insufficient training and 3 become under-fitted. As prior studies suggested that an efficient sparse training regime requires preserving the gradient flow through the network (Wang et al., 2020), we hypothesize that by letting MoE layers receive more gradient flow, the overall parameter efficiency of MoE models can be improved. This observation also indicates that dense layers tend to have a stronger capability to preserve the gradient flow, which may help explain why prior works only extend MoE layers at every other layer (Fedus et al., 2021; Artetxe et al., 2021; Rajbhandari et al., 2022). Challenge III: Sharing the parameter of MoE layers leads to a significant accuracy drop. Ty- ing weights improves the gradient flow of a layer, because if a weight is used multiple times during training, that weight will receive gradients from multiple places, and those gradients will add up. We experiment with weight sharing on autoregressive MoE models. Unlike the observation in prior works (Dehghani et al., 2019; Lan et al., 2020; Xue et al., 2022), where there is often no accuracy drop when all transformer layers share one set of weights ( share-all ), we find that by tying one MoE layer\u2019s weights across all MoE layers, the validation loss suffers significantly, as shown in Figure 1(c). In the evaluation section, we show that the share-all strategy leads to a serious down- stream accuracy drop as well, indicating that expert-sharing may not be sufficient for large-scale autoregressive language models (e.g., pre-training GPT-3 style models (Brown et al., 2020)).\n\n4 B UILDING P ARAMETER -E FFICIENT M O E S VIA S A M O E\n\n This section introduces the techniques used in SaMoE to improve the MoE parameter efficiency. Self-Adaptive Expert Combinations. In Transformer-based MoE (Lepikhin et al., 2020) and its variations such as SwitchTransformer (Fedus et al., 2021) and DeepSpeed-MoE (Rajbhandari et al., 2022), each expert layer contains a set of E experts, with no explicit relation between expert pa- rameters of different layers. Conversely, the tied-MoE-layer strategy described in Section 3 has a single set of MoE layers shared among all MoE layers. This strategy leads to accuracy degradation, presumably because the reduced expert capacity hurts the model quality. While using the same parameters for all MoE layers limits the capability of MoE models, one may wonder if we could relax the technique by dividing MoE layers into groups. As such, instead of preparing parameters for only one MoE layer, we can prepare parameters for C MoE layers to construct a model with M MoE layers, where 1 \u2264 C \u2264 M . For the group\u2019s assignment to each layer, we experiment with several strategies and compare them empirically in Section 5.6. Overall, we find that manual determination of an effective group assignment strategy is non-trivial and requires expert knowledge of MoE layer characteristics. It is also not clear that using expert knowledge, even when it is applicable, will lead to superior efficiency. In contrast, we are interested in using a pure learning approach applicable in situations where expert knowledge is unavailable. Rather than manually determining an expert sharing scheme \u2013 we propose to learn an expert pool ( EP ), which consists of a fixed number of global MoE layers EP = [ EP (1) , ..., EP ( C ) ] , where C is the size of the expert pool (chosen freely as a hyperparameter). Figure 2 illustrates the overview of SaMoE. We then express each MoE layer f ( m ) \u03b8 as a linear combination of global MoE layers in the expert pool: f ( m ) \u03b8 := (cid:80) Cc =1 \u03b1 ( m ) c EP ( c ) , where \u03b1 ( m ) c , a C dimensional vector, is the coefficient of the m -th MoE layer. This scheme allows for coefficient and global MoE layers to be jointly optimized with gradient-based methods. The global MoE layers in the expert pool can be seen as global representation extractors, and coefficient \u03b1 ( m ) decides which representations learned by a global layer are relevant for the m -th MoE layer computation of a network. 4 FFC 1 ... P=0.6 FFC2 FFC 3 FFC E \u03b2 FFC 1 ... FFN 1 FFC2 FFN 2 FFC 3 FFN 3 FFC E FFN E ... Expert pool EP (1) EP (2) \u03b1 1(l) \u03b1 2(l) Router x in Router x in x out ... We partition the expert pool across multiple workers, where we place different EP m to different work- ers and execute them in parallel in one training step. For an input se- quence of T features, we first use g ( \u00b7 ) to compute the score for routing each token representation to an EP m . Each worker then sends T/M tokens to each other worker, with an all2all communication operation, similar to standard MoE training. Following prior work (Lan et al., 2020), we make LayerNorm non-shared across MoE layers be- cause (1) they are not parameter and computation intensive, and (2) private LayerNorm encourages more diversified input representations of different MoE layers. We note that having C as a free parameter decouples the MoE layers in MoE models from the MoE layer depth. Typically, M MoE layers with E experts in each layer and hidden dimension size H have O( M \u00b7 E \u00b7 H 2 ) parameters. With the expert pool, the number of parameters for all MoE layers becomes O( C \u00b7 M + C \u00b7 E \u00b7 H 2 ) = O( C \u00b7 E \u00b7 H 2 ), where C \u00b7 M refers to the parameters for coefficient \u03b1 . In practice, we find that increasing the expert pool size leads to better model quality but often a small expert pool size C = [2 , 3] already provides similar or better model quality than the baselines. We also note that in the special case, where the expert pool\u2019s cardinality | C | = 1 , then the proposed scheme turns out to be the tied-MoE-layer strategy described in Section 3. Also, if we constraint coefficient \u03b1 ( m ) to be one-hot vectors, SaMoE would make hard sharing assignment similar to manually determined sharing strategy, at the cost of non-differentiability. Finally, we remark that the idea of learning a combination of experts is inspired by prior studies on dictionary learning of sparse coding (Kreutz-Delgado et al., 2003; Mairal et al., 2008; 2009), where a data representation is learned via a linear combination (represented as a sparse vector) of a small set of basic elements (called dictionary). Instead of finding a sparse approximation, we learn expert combinations that improve the expressiveness of global MoE layers. Self-Adaptive Shortcut Connections. While identity mapping has been widely used in dense Transformers for propagating the loss signal across different Transformer layers, Figure 1(b) shows that experts in MoE layers exhibit vanishing gradient phenomenon with identity mapping. There- fore, we conjecture that as we switch to the expert pool, the shortcut connections at those global MoE layers need to be more adaptive as well such that they can expand to cover a sufficiently large number of MoE layers. As such, we apply two additional changes to SaMoE: (1) adding a scaling factor to re-balance the MoE shortcut connection: x out = \u03b2x in + f ( m ) \u03b8 ( LayerNorm ( x in )) , where \u03b2 is a learnable parameter and acts as the scaling factor of the shortcut branch, and (2) make each MoE layer has its individual shortcut connection. We empirically find that the self-adaptive shortcut connections help further improve the performance of SaMoE (Section 5.4).\n\n5 E XPERIMENTS AND R ESULTS\n\n In this section, we first compare SaMoE training to several MoE baselines, to carefully measure the parameter efficiency gains. We then conduct an ablation analysis of SaMoE and evaluate the scaling properties of SaMoE. Finally, we compare SaMoE with several heuristic-based sharing strategies.\n\n5.1 E XPERIMENTAL S ETUP\n\n Data. We follow recent practice (Black et al., 2022; Rae et al., 2021) and use the public available Pile dataset (Gao et al., 2020) to train the autoregressive language models. Pile is a massive curated dataset specifically designed for training large language models. It contains 825GiB English text cor- pus composed of 22 diverse web-domain datasets and has been demonstrated to achieve comparable performance to similarly sized GPT-3 models. Following Brown et al. (2020), we train all configu- rations for 300 billion tokens for fair comparison unless otherwise indicated. For downstream task evaluation, we follow prior work Rajbhandari et al. (2022) and primarily focus on zero-shot gener- alization. As described in Brown et al. (2020), zero-shot evaluation is one of the most challenging settings, because the model is only given a natural language instruction describing the task and has 5 been used by prior work for monitoring scientific progress (Rajbhandari et al., 2022). We use the Eleuther AI Language Model Evaluation Harness (Gao et al., 2021), an open-sourced codebase for downstream task zero-shot evaluation, covering a wide range of NLP tasks. Models. We focus our evaluation on autoregressive MoE models whose base architecture mainly follows that of GPT-3 (Brown et al., 2020). As the main goal of the evaluation is to show that our proposed method can improve the parameter efficiency of MoE models instead of achieving SoTA performance, we choose a model that can finish the training in a reasonable time frame (e.g. in roughly 120 wall-clock hours on 32 A100 GPUs for one run). As such, we choose a 1.9 billion- parameter MoE model using a 125M dense model (12 layers, 768 hidden sizes, 12 attention heads) the same as the GPT-3 small setting in Brown et al. (2020), as the base model. Then we add E=64 MoE layers on every other FFC layer. We choose E=64 by following Clark et al. (2022); Rajbhandari et al. (2022), which suggest targeting E \u2208{ 64, 128 } . Hyperparameters. We largely follow the methodology from Brown et al. (2020) and Rajbhandari et al. (2022) to train models. For detailed hyperparameter settings, please refer to Appendix A. Framework. We implement and train our models in PyTorch (Paszke et al., 2019) using DeepSpeed library (Rajbhandari et al., 2022). We provide more implementation details in Appendix B. Compared Models. We compare the proposed strategy with the following schemes: \u2022 AR-MoE : The first baseline is an autoregressive MoE model, which is a decoder-based MoE model, as described in Rajbhandari et al. (2022). \u2022 PR-MoE : We follow the training procedure in Rajbhandari et al. (2022) to train a PR-MoE by using half experts for all MoE layers except the last two MoE layers. \u2022 Grouped-MoE : We create a strong baseline by partitioning MoE layers into C groups (e.g., 1g , 2g , 3g ), and let every \u230a M/C \u230b consecutive MoE layers share their parameters. \u2022 SaMoE : The proposed method described in Section 4.\n\n5.2 P RE - TRAINING WITH S A M O E\n\n To monitor the impact of SaMoE on the pre-training, we report the validation loss during pre-training because it shows how SaMoE af- fects the convergence of the pre-training of the autoregressive MoE model in a straightforward way. Figure 3 shows that despite hav- ing a smaller model size, SaMoE outperforms AR-MoE, PR-MoE, and Grouped-MoE by a wide margin. PR-MoE achieves a simi- lar validation loss as AR-MoE but is worse than SaMoE. The pri- mary difference between PR-MoE and SaMoE is that PR-MoE ex- ploits layer sensitivity and uses half experts for lower MoE layers, which reduces the number of experts by less than 2 \u00d7 . In contrast, SaMoE decouples the number of experts from the model depth and allows MoE models to scale to deeper models without significantly increasing the number of parameters. SaMoE further improves the parameter efficiency of the remaining parameters via self-adaptive expert combinations. A clear goal for future work includes com- bining SaMoE and heterogeneous experts to achieve even greater parameter efficiency of MoE models. Grouped-MoE (3 groups) struggles to achieve comparable validation loss as AR-MoE despite already having a 1.4 \u00d7 larger model size than SaMoE, indicat- ing that sharing weights via grouped MoE layers is insufficient to achieve high MoE model quality. Further increasing the group size helps mitigate the accuracy gap, at the cost of significantly more trainable parameters (as analyzed in Section 5.6). Finally, since AR-MoE, PR-MoE, and SaMoE all have converged, it would be hard for them to outperform SaMoE on validation loss even if we spent longer training time. Therefore, this figure indicates that our proposed method is more parameter- efficient than baselines for pre-training tasks.\n\n5.3 D OWNSTREAM E VALUATION\n\n Table 1 showcases the most important advantage of SaMoE\u2019s design choices via zero-shot eval- uation. We provide full results of 21 tasks in Appendix C. The first row is an MoE model with a 6 comparable model size from related work trained with a different dataset (Du et al., 2022). We make several observations: (1) with only around 37% of AR-MoE\u2019s parameters (0.7B vs. 1.9B), SaMoE achieves comparable accuracy as the baseline AR-MoE, e.g., 40 vs. 39.9 on average across 21 tasks. (2) By increasing the depth of the MoE layer, we obtain a SaMoE model that has 0.85B parameters (44% of AR-MoE) but achieves significantly zero-shot evaluation improvements over AR-MoE, as measured by the difference of a wide range of downstream tasks: Lambada (+3 points), Piqa (+1.5 points), Copa (+2 points), RTE (+2.5 points), Wikitext (+1.3 points) and on average 0.9 points across 21 tasks. SaMoE achieves much better parameter efficiency than AR-MoE in that instead of relying on parameter-intensive MoE layers at every other Transformer layer, SaMoE learns an expert pool of global MoE layers and letting each MoE layer learns a linear combination of the global MoE layers in the pool. More importantly, the number of parameters does not increase as we scale the model depth, improving the parameter efficiency while drastically reducing the model size. Compared with PR-MoE, SaMoE achieves 2.3 points higher zero-shot generalization accuracy on average with only 69% of its parameters. We note that PR-MoE incurs a slightly larger accuracy loss over AR-MoE because the original PR-MoE used a different learning rate from AR-MoE. In contrast, in our experiments, all configurations use the same learning rate for a fair comparison. Fur- thermore, PR-MoE achieves more comparable accuracy to AR-MoE when the base model becomes larger, as shown in the next part. It is possible that by slightly tuning the hyperparameters of PR- MoE, its zero-shot evaluation results can be improved. SaMoE achieves higher parameter efficiency than PR-MoE because SaMoE decouples the expert parameters from the MoE model depth, leading to a higher parameter reduction ratio than PR-MoE. SaMoE achieves +1.4 points higher average accuracy than Grouped-MoE (41.1 vs. 39.7) and +2.3 points better perplexity on WikiText despite having 89% of parameters (0.89B vs. 1B). As expected, the number of parameters of Grouped-MoE decreases as we decrease the number of groups in Grouped-MoE. However, we observe that there are also drastic zero-shot evaluation accuracy decreases as we do so. As an example, the LAMBADA accuracy drops 4.1 points when switching from Grouped-MoE (3g) to Grouped-MoE (1g). There- fore, Grouped-MoE is not sufficient to achieve parameter efficient MoE for autoregressive language models. These results demonstrate the parameter efficiency benefits of SaMoE.\n\n5.4 A BLATION AND A NALYSIS R ESULTS\n\n In this part, we study the importance of each component of SaMoE: self-adaptive expert combination (SEC), and self-adaptive shortcut connection (SSC). We use 125M-64E MoE models in this study. As such, SaMoE- SSC represents disabling self-adaptive shortcut connection. SaMoE- SEC - SSC further disables adaptive expert combination so the model uses hard sharing strategies for MoE layers. Furthermore, we also include SaMoE with only one global MoE layer, which removes group sharing (GS) completely ( SaMoE- GS - SEC - SSC ) as an additional baseline. Figure 6 shows the validation perplexity of different configurations. It is expected that the removal of either component in SaMoE results in a performance drop.\n\n5.5 E VALUATION OF THE S CALING P ROPERTY OF S A M O E\n\n In this part, we investigate the scaling properties of SaMoE. As scaling billion-scale MoE models often require significant training budgets, we choose the number of updates so that each training can still be complete in roughly 72 wall-clock hours. 7 Scaling in Expert Pool Size and Width. Expressing an MoE layer as a linear combination of global MoE layers improves the parameter efficiency. Of immediate interest is C : the expert pool size. A larger C increases the number of parameters in the model but also enables a more flexible combination of global experts. To verify if a larger expert pool size helps improve the MoE model quality, we compare SaMoE with C = 2 and C = 3 . We also study SaMoE with different expert widths (e.g., H=768, H=1024). Figure 4(a) shows the results. We make several observations: (1) In all cases, SaMoE consistently outperforms the baseline despite a smaller model size, indicating that scaling expert pool size and width brings additional benefits in model convergence while still allow- ing SaMoE to be more parameter efficient than the baseline. (2) Increasing C increases the model size by 14\u201317%, depending on the hidden dimension. However, by doing so, the validation perplex- ity also gains noticeable improvements (e.g., \u223c 0.25 points). (3) Scaling either C or H improves the convergence, though scaling H together with a smaller C =[2,3] appears more competitive. Scaling in Model Depth and Experts. Given that SaMoE achieves higher accuracy than alternative methods with fewer parameters, one interesting question is how would SaMoE scale as we further increase the depth of the model and increase the number of experts? To study this, we investigate three configurations: (1) scaling M : varying the depth of the model from 12L to 24L (L24, E=64), (2) scaling E : varying the number of experts from 64 to 128 (L12, E=128), and (3) scaling both M and E : increasing both the depth and the number of experts (L24, E=128). In all three cases, we keep the expert pool size C to the same. Figure 4(b) shows the scaling results. We observe that increasing both model depth and experts leads to further improvement of SaMoE model equality, and compound benefits can be achieved by scaling along both dimensions . However, while scaling E leads to an increased number of parameters and model sizes, scaling along the depth dimension M leads to improvements with only a mild increase (+18%) in model size. Therefore, scaling the depth dimension of SaMoE appears to be more effective from a parameter efficiency perspective a li d a t i n P e r p l e x i t y Larger MoE models. We evaluate SaMoE on two larger MoE models, including (1) 13B/350M+128E: a 13 billion-parameter MoE model that uses 350M as the base model and with 12 128-expert MoE layers, and (2) 30B/760M+128E: a 30 billion-parameter MoE model with 760M as the base model and 12 128-expert MoE layers. For (1), we scale SaMoE along the width dimen- sion (1024 \u2192 1536), which produces a 5.3B model; and for (2), we scale SaMoE along the depth dimension (24 \u2192 32), which leads to a 5.8B parameter model. Due to resource limitations, we train 100k and 50k steps (out of 572k total steps) for the 13B and 30B models, respectively. Figure 5(a) 8 and Figure 5(b) show the validation loss of these two models. SaMoE outperforms the AR-MoE baseline by a large margin with 2.5 \u00d7 fewer parameters in comparison to the 13B baseline model. For the 30B model, SaMoE achieves slightly better validation loss as the baseline with 5.2 \u00d7 model size reduction. Table 2 also confirms that the same trend is observed on zero-shot evaluation. In Appendix C, we also provide the complete zero-shot evaluation of these two models.\n\n5.6 E VALUATION OF A LTERNATIVE S TRATEGIES\n\n As described in Section 3, there are other strategies for sharing MoE layer parameters. As such, we have also explored and quantified alternative cross-sharing strategies for building MoE. Share-bottom/-top vs. SaMoE . Share-bottom shares the parameter of MoE layer with the first half MoE layers and leaves the second half of MoE layers unshared, and Share-top switches the shared MoE layer to the second half of the layers. Although both strategies have the same model size, Figure 7(a) shows that share-bottom leads to lower validation loss than Share-top . This is somewhat expected. From Figure 1(b), we see that higher MoE layers have larger gradient norms, which implies that higher MoE layers tend to have more freedom than lower layers for their expressiveness. In other words, lower MoE layers probably have more redundant parameters compared to the higher MoE layers. That said, share-bottom still has the other half of MoE layers non-shared. Therefore, its model size is still quite large compared to SaMoE (1.3B vs.0.89B). Exploiting MoE layer heterogeneity . Rajbhandari et al. (2022) reported heterogeneity in MoE layers, e.g., the last two MoE layers are more important than the remaining ones and should contain more experts. To investigate how layer heterogeneity affects expert sharing, we investigate another strategy, referred to as Share-pyramid , which lets the last several MoE layers (e.g., 2) have their own weights while the other MoE layers share parameters. Figure 7(a) also shows that while Share- pyramid achieves similar convergence as Share-top with a smaller model size. However, compared with SaMoE, layer heterogeneity has a very minimal effect on model convergence. MoE Layer sharing with varying groups . Figure 7(b) shows that by increasing the groups in Grouped-MoE (1 g \u2192 2 g \u2192 3 g ) , the validation loss decreases. However, even with 3 groups, the Grouped-MoE cannot obtain the same validation loss as the baseline model and SaMoE despite having a larger model size. What if we train an MoE with a smaller number of experts? Another method to obtain an MoE model with a smaller number of parameters is to reduce the number of experts. Figure 7(c) shows that although reducing the number of experts per MoE layer (e.g., 64 \u2192 32 \u2192 16) reduces the model size, there is a significant drop in model quality. As such, our proposed method is much more parameter efficient than models with reduced E .\n\n6 C ONCLUSION AND F UTURE D IRECTIONS\n\n This work investigates parameter-efficient architecture design for Mixture-of-Expert models. Based on the analysis of the challenges of MoE, we introduce a novel MoE method called SaMoE that learns self-adaptive expert combinations for each MoE layer. Our extensive experiments demon- strate that our SaMoE is more parameter-efficient than prior arts and can significantly reduce the model size while achieving high accuracy. While this work mainly focuses on the autoregressive MoE models, future work might investigate the performance of our methods on other tasks and MoE architectures. Another exciting topic is studying the best trade-off between scaling in expert pool size, depth, and experts. 9\n\nR EFERENCES\n\n 10 11 12 13 In this part, we present results that are not included in the main text due to the space limit.\n\nA H YPERPARAMETERS\n\n In this part, we include detailed hyperparameters used for training the models in this work. We largely follow prior works (Brown et al., 2020; Rajbhandari et al., 2022) to set the hyperparameters. The batch size and learning rates are set according to the model size following Brown et al. (2020) (e.g., batch size 256, learning rates 6e-4). We linearly warm-up the learning rate from a min learning rate (6e-5) over the first 375M tokens and use a cosine learning rate decay back to min lr over the remaining tokens. For all experiments, we use a batch size of 4 for each GPU, with gradient accumulation of 2 and train with FP16. Table 3 provides the detailed hyperparameters used for training models in Section 5.2 and Section 5.3. Table 4 provides the hyperparameters used for training 13B MoE and 30B MoE, respectively, in Section 5.5. 14\n\nB I MPLEMENTATION D ETAILS\n\n We implement and train our models in PyTorch (Paszke et al., 2019) using DeepSpeed library (Ra- jbhandari et al., 2022). To implement SaMoE, we partition the global expert layers and their com- putation using expert parallelism, where we place different experts on different devices and execute them in parallel in one mini-batch training step. We use the All-to-All collective to handle the com- munication among experts. When the number of experts in the base layer is smaller than the number of available GPUs, we use expert parallelism in combination with data parallelism as described in (Rajbhandari et al., 2022). We use data parallelism for non-MoE layers. We use a top-K gating function to activate a subset of K experts in the MoE layer for each token. Specially, we follow Fedus et al. (2021) to choose top-1 expert selection. As such, the MoE model has roughly the same number of parameters to be activated for each token as its dense counterpart part. During training and inference, each expert has a capacity of min ( min capacity, \u2308 Q \u00b7 BE \u2309 ) to- kens, which determine how many tokens a single expert can handle. Q is a capacity factor which we set to 1.0, B is the total number of tokens in a batch, and we set min capacity to 4. We handle tokens that exceed the capacity of an expert the same way as Fedus et al. (2021), by considering additional tokens as \u201doverflowed\u201d and passing their representations to the next layer via the shortcut connection. Hardware. We conduct the pre-training and evaluation with 32 NVIDIA Ampere A100-40GB GPUs.\n\nC Z ERO - SHOT E VALUATION R ESULTS\n\n This part includes the complete zero-shot evaluation results on 21 downstream tasks. 9B 1.3B 0.42B 0.72B 1B 0.72B 0.85B lambada 48.0 33.8 44.1 46.2 48.2 48.1 51.3 triviaqa 3.6 1.8 2.4 2.6 3.0 2.9 3.8 webqs 2.0 1.3 1.4 1.4 1.6 1.4 0.9 piqa 67.7 63.2 64.0 66.9 67.7 66.5 68.6 race 30.3 28.2 30.5 30.1 29.4 31.9 31.4 boolq 55.0 55.4 58.2 59.2 54.2 59.4 55.7 copa 67.0 68.0 65.0 67.0 67.0 67.0 69.0 winogrande 51.9 52.1 50.3 50.0 52.2 50.9 51.6 arc challenge 25.0 23.5 23.6 21.6 24.5 25.1 26.3 arc easy 45.7 41.3 42.0 44.1 44.6 45.3 47.3 openbookqa 30.2 28.8 28.4 17.4 29.4 30.6 30.8 cb 33.9 33.9 23.2 37.5 25.0 28.6 28.6 rte 54.2 52.0 53.1 54.9 57.8 52.7 56.7 wic 50.0 49.4 50.0 50.0 49.7 50.0 50.0 wsc 36.5 42.3 36.5 36.5 36.5 36.5 37.5 multirc 0.8 1.9 0.8 0.8 0.9 0.9 0.8 record-f1 73.7 63.1 71.9 72.8 73.8 73.8 76.5 record-em 73.1 62.4 71.2 72.0 73.1 73.1 75.8 anli r1 31.9 34.2 34.9 33.1 31.2 32.9 33.5 anli r2 31.6 33.1 32.9 34.4 31.0 33.2 34.0 anli r3 32.1 33.8 33.3 33.6 32.8 33.7 33.0 AVG Acc. 40.2 38.3 38.9 39.6 39.7 40.2 41.1 15 0.8 multirc 1.0 1.3 1.7 74.1 74.7 75.3 record 75 0 75.4 77.9 73.4 74.0 74.5 31.8 34.2 32.3 31.8 33.5 34.5 anli 33.3 33.1 33.3 32.1 32.9 35.3 34.8 34.9 34.4 36.5 33.8 35.8 AVG 40 7 41.1 41.6 41.2 41.4 41.7 16", "claude_opus": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes SaMoE, a new parameter-efficient Mixture-of-Experts (MoE) architecture that improves MoE models by learning a soft combination of a global set of shared expert layers for each MoE layer.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors identify that poor gradient flow in MoE layers leads to insufficient training and parameter redundancy.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "SaMoE overcomes this by introducing a fixed-size \"expert pool\" and expressing each MoE layer as a weighted combination of the experts in this pool.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This enables substantial parameter savings while achieving comparable or better accuracy than standard MoE training.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "SaMoE is evaluated on billion-scale autoregressive language models and demonstrates significant improvements in parameter efficiency, reducing model size by up to 5.2x while obtaining superior pretraining and zero-shot generalization results.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Clearly identifies a key issue with standard MoE architectures (poor gradient flow) and provides an effective solution via the self-adaptive expert combination scheme.", "topic_class_1": "Strengths", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- SaMoE significantly improves parameter efficiency of large MoE language models, enabling up to 5.2x parameter reduction with no loss or even gains in accuracy.", "topic_class_1": "Strengths", "topic_class_2": "contribution", "reliability": "No", "explanation": "More summary like", "error_type": "Superficial Review"}, {"segment_text": "This is an important result.", "topic_class_1": NaN, "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Extensive experiments on large-scale models and datasets provide convincing empirical support for the effectiveness of the method.", "topic_class_1": "Strengths", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Ablations and comparisons to reasonable baselines are included.", "topic_class_1": "Strengths", "topic_class_2": "experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Well-written paper that clearly describes the limitations of prior MoE approaches and the key aspects of the SaMoE architecture.", "topic_class_1": "Clarity", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- While the rationale for the self-adaptive combination is clear, it would be useful to provide some additional intuition or analysis on why this particular formulation is effective.", "topic_class_1": "Limitations", "topic_class_2": "weakness: lack analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- Evaluation focuses on autoregressive language models. It would be interesting to see if the benefits extend to other architectures like encoder-only models as well.", "topic_class_1": "Limitations", "topic_class_2": "weakness: studied architectures are limited suggestions", "reliability": "No", "explanation": "Out of scope and the authors list it as future work.", "error_type": "Out-of-scope"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The paper is clearly written and easy to follow. Key concepts are well-explained and placed in context of prior work.", "topic_class_1": "Clarity", "topic_class_2": "writing", "reliability": "No", "explanation": "Generic", "error_type": "Superficial Review"}, {"segment_text": "- The work is high quality, with strong empirical results supporting the claimed contributions.", "topic_class_1": "Quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Ablations and comparisons provide additional evidence for the importance of the proposed components.", "topic_class_1": "Quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "- The self-adaptive combination scheme is a novel contribution that hasn't been explored for MoE models before to my knowledge.", "topic_class_1": "Novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Decoupling expert parameters from model depth is a key insight.", "topic_class_1": "Results", "topic_class_2": "contribution", "reliability": "No", "explanation": "Wrong category", "error_type": "Misunderstanding"}, {"segment_text": "- Sufficient implementation details (e.g.", "topic_class_1": "Reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "appendix, code release) are provided to enable reproducibility.", "topic_class_1": "Reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper makes an important contribution by identifying a key limitation in standard MoE architectures and proposing an effective solution via self-adaptive expert combinations in SaMoE.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed approach leads to substantial improvements in parameter efficiency while maintaining or even improving model quality.", "topic_class_1": "Summary", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The claims are well-supported by extensive experiments and ablations.", "topic_class_1": "Summary", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some additional analysis and generalization to other architectures could strengthen the work further, but overall this is a high-quality paper that represents a meaningful advance in the important problem of scaling MoE models efficiently.", "topic_class_1": "Summary", "topic_class_2": "suggestions", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gpt4": {"review": [{"segment_text": "Summary of the Paper", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The authors address the parameter inefficiency problem in Mixture-of-Experts (MoE) models, identifying poor gradient flow in MoE layers as the main issue.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They propose SaMoE, a novel architecture that utilizes a soft combination of a global set of expert layers to improve parameter efficiency significantly.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Their experiments on billion-scale GPT-3 style models demonstrate that SaMoE reduces total parameters by up to 5.2\u00d7 while achieving superior accuracy compared to baseline MoE models.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper contributes an in-depth analysis of MoE inefficiencies and presents a scalable solution that maintains high model quality with fewer parameters.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths and Weaknesses", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity and Methodological Rigor: The paper is well-structured, presenting a clear problem statement, detailed methodology, and extensive experimental results.", "topic_class_1": "Clarity", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The proposed solution is innovative, addressing a significant issue in the scaling of MoE models.", "topic_class_1": "Novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Significant Parameter Efficiency: SaMoE achieves remarkable reductions in parameter count while maintaining or even improving model performance on various tasks.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This advancement could make large-scale language models more accessible and cost-effective.", "topic_class_1": "Result", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Comprehensive Evaluation: The authors conduct thorough experiments, including ablation studies and comparisons with several baselines and MoE configurations.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "They demonstrate SaMoE's superiority across multiple benchmarks, providing a solid foundation for the claims.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited Discussion on Limitations and Broader Impact: While the paper extensively discusses the technical aspects and benefits of SaMoE, it lacks a detailed discussion on potential limitations, broader impacts, and ethical considerations of deploying such models.", "topic_class_1": "Limitations", "topic_class_2": "weakness: lack analyses", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility Concerns: Though the paper states that the code will be open-sourced, the lack of detailed hyperparameter settings in the main text could hinder reproducibility. A dedicated section for reproducibility, including data preprocessing and model initialization details, would enhance the paper's value.", "topic_class_1": "Reproducibility", "topic_class_2": "reproducibility", "reliability": "No", "explanation": "This orthogonal to other parts of the review", "error_type": "Contradiction"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity: The paper is well-written, with a logical flow that facilitates understanding of complex concepts.", "topic_class_1": "Clarity", "topic_class_2": "writing", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The use of figures and tables effectively illustrates the methodology and results.", "topic_class_1": "Clarity", "topic_class_2": "presentation", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Quality: The research is of high quality, demonstrating a deep understanding of MoE models and presenting a novel approach to improving their parameter efficiency.", "topic_class_1": "Quality", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novelty: SaMoE introduces a unique method for combining expert layers, distinguishing it from previous works on MoE models.", "topic_class_1": "Novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The self-adaptive expert combination mechanism is a novel contribution to the field.", "topic_class_1": "Novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: Given the promise to open-source the code, the paper meets the reproducibility criterion.", "topic_class_1": "Reproducibility", "topic_class_2": "reproducibility", "reliability": "No", "explanation": "This orthogonal to the weakness of the review", "error_type": "Contradiction"}, {"segment_text": "However, detailed hyperparameter settings and training procedures in the supplementary material or the main text would further strengthen this aspect.", "topic_class_1": "Limitation", "topic_class_2": "reproducibility", "reliability": "No", "explanation": "Expectation of more experiments in the supplementary material is unfair to the authors", "error_type": "Out-of-scope"}, {"segment_text": "Summary of the Review", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper introduces SaMoE, a novel MoE architecture that significantly improves parameter efficiency by adopting a soft combination of global expert layers.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This work stands out for its methodological rigor, extensive experimentation, and the significant reductions in parameter counts while maintaining high model performance.", "topic_class_1": "Summary", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The novelty of the self-adaptive expert combination and the thorough evaluation solidify the paper's contributions to the field of NLP and large-scale language models.", "topic_class_1": "Summary", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Despite minor weaknesses in discussing broader impacts and limitations, the paper presents a compelling advancement in making large-scale MoE models more efficient and accessible.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 4", "Empirical Novelty and Significance": " 4", "Flag for Ethics Review": " NO", "Recommendation": " 8", "Confidence": " 4"}}, "gemini_pro_1.5": {"review": [{"segment_text": "Summary of the Paper:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This paper proposes SaMoE, a novel architecture for Mixture-of-Experts (MoE) language models that aims to improve parameter efficiency.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "SaMoE introduces a global expert pool and expresses each MoE layer as a soft combination of these global layers.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "This design decouples the number of experts from the model depth, allowing for significant parameter reduction while maintaining or even improving accuracy compared to standard MoE training.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "The paper also analyzes the challenges of MoE models, identifying poor gradient flow in MoE layers as a key factor contributing to parameter inefficiency.", "topic_class_1": "Summary", "topic_class_2": "paper summary", "reliability": "No", "explanation": "This challenge of poor gradient flow is identified as a by-product of other analysis and is not a direct analysis.", "error_type": "Inaccurate Summary"}, {"segment_text": "Strengths and Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Strengths:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Improved parameter efficiency: SaMoE significantly reduces the number of parameters required for MoE models while achieving comparable or better accuracy than baselines.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Experiments demonstrate up to 5.2x parameter reduction with superior pre-training and zero-shot generalization results.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Novel architecture: The self-adaptive expert combination approach is a novel and effective way to share parameters across MoE layers, addressing the issue of poor gradient flow.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Extensive evaluation: The paper provides comprehensive experiments on billion-scale autoregressive MoE language models, demonstrating the effectiveness of SaMoE across different model sizes and configurations.", "topic_class_1": "Strength", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Open-source code: The authors commit to open-sourcing the training and evaluation code, facilitating reproducibility and further research.", "topic_class_1": "Strength", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Weaknesses:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited exploration of proxy tasks: The paper relies on a proxy task to measure the closeness to a \"ground-truth prompt\" during prompt search, but the definition and selection of this proxy task are not thoroughly investigated.", "topic_class_1": "Limitations", "topic_class_2": "weakness: lack experiments", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Some unconvincing results: Certain results, such as the reported human-written performance on some datasets, seem lower than expected and require further clarification.", "topic_class_1": "Limitations", "topic_class_2": "weakness: unexpected results", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Limited comparison baselines: While the paper compares SaMoE with standard MoE and PR-MoE, including more recent baselines would provide a stronger evaluation of its performance.", "topic_class_1": "Limitations", "topic_class_2": "weakness: lack experiments", "reliability": "No", "explanation": "Generic feedback, the feedback needs to be specific. For example, what specific baselines the reviewer thinks missing?", "error_type": "Superficial Review"}, {"segment_text": "Clarity, Quality, Novelty, and Reproducibility:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Clarity and Quality: The paper is well-written and easy to follow, with a clear explanation of the proposed method and analysis of the challenges addressed.", "topic_class_1": "Clarity", "topic_class_2": "writing", "reliability": "No", "explanation": "The quality aspect has no supporting statements.", "error_type": "Unstated statement"}, {"segment_text": "Novelty: While the general idea of expert sharing is not entirely new, the specific design of SaMoE's self-adaptive expert combination is novel and shows significant improvements.", "topic_class_1": "Novelty", "topic_class_2": "contribution", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Reproducibility: The authors' commitment to open-sourcing the code strengthens the potential for reproducibility and further research on SaMoE.", "topic_class_1": "Reproducibility", "topic_class_2": "reproducibility", "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "Summary of the Review:", "topic_class_1": NaN, "topic_class_2": NaN, "reliability": "Yes", "explanation": NaN, "error_type": NaN}, {"segment_text": "SaMoE presents a promising approach to improve the parameter efficiency of MoE language models. The proposed architecture addresses the issue of poor gradient flow in MoE layers and achieves significant parameter reduction while maintaining or improving accuracy. However, some aspects of the evaluation require further clarification and comparison with additional baselines would strengthen the assessment of its performance. Overall, SaMoE is a valuable contribution to the field of MoE language models and warrants further investigation.", "topic_class_1": "Summary", "topic_class_2": "paper summary contribution weakness: lack experiments contribution", "reliability": "Yes", "explanation": null, "error_type": NaN}], "score": {"Technical Novelty and Significance": " 3", "Empirical Novelty and Significance": " 3", "Flag for Ethics Review": " NO", "Recommendation": " 6", "Confidence": " 4"}}}
